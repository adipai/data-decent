{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adipai/data-decent/blob/main/src/automation/automation_Eclipse_PDE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pmlb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YAJ8lflEvuaA",
        "outputId": "4eee8be1-0ff2-441c-80fc-ea2ad835cc61"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pmlb in /usr/local/lib/python3.10/dist-packages (1.0.1.post3)\n",
            "Requirement already satisfied: pandas>=1.0.5 in /usr/local/lib/python3.10/dist-packages (from pmlb) (2.1.4)\n",
            "Requirement already satisfied: requests>=2.24.0 in /usr/local/lib/python3.10/dist-packages (from pmlb) (2.32.3)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from pmlb) (6.0.2)\n",
            "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.5->pmlb) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.5->pmlb) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.5->pmlb) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.5->pmlb) (2024.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.24.0->pmlb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.24.0->pmlb) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.24.0->pmlb) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.24.0->pmlb) (2024.7.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0.5->pmlb) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sdv"
      ],
      "metadata": {
        "id": "COAFe5iG-02V",
        "outputId": "fa978d4e-1887-4562-b79f-0e1278ab721e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sdv\n",
            "  Downloading sdv-1.16.1-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting boto3<2.0.0,>=1.28 (from sdv)\n",
            "  Downloading boto3-1.35.10-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting botocore<2.0.0,>=1.31 (from sdv)\n",
            "  Downloading botocore-1.35.10-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: cloudpickle>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from sdv) (2.2.1)\n",
            "Requirement already satisfied: graphviz>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from sdv) (0.20.3)\n",
            "Requirement already satisfied: tqdm>=4.29 in /usr/local/lib/python3.10/dist-packages (from sdv) (4.66.5)\n",
            "Collecting copulas>=0.11.0 (from sdv)\n",
            "  Downloading copulas-0.11.1-py3-none-any.whl.metadata (9.1 kB)\n",
            "Collecting ctgan>=0.10.0 (from sdv)\n",
            "  Downloading ctgan-0.10.1-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting deepecho>=0.6.0 (from sdv)\n",
            "  Downloading deepecho-0.6.0-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting rdt>=1.12.3 (from sdv)\n",
            "  Downloading rdt-1.12.3-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting sdmetrics>=0.14.0 (from sdv)\n",
            "  Downloading sdmetrics-0.15.1-py3-none-any.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: platformdirs>=4.0 in /usr/local/lib/python3.10/dist-packages (from sdv) (4.2.2)\n",
            "Requirement already satisfied: pyyaml>=6.0.1 in /usr/local/lib/python3.10/dist-packages (from sdv) (6.0.2)\n",
            "Requirement already satisfied: pandas>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from sdv) (2.1.4)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.3 in /usr/local/lib/python3.10/dist-packages (from sdv) (1.26.4)\n",
            "Collecting jmespath<2.0.0,>=0.7.1 (from boto3<2.0.0,>=1.28->sdv)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
            "Collecting s3transfer<0.11.0,>=0.10.0 (from boto3<2.0.0,>=1.28->sdv)\n",
            "  Downloading s3transfer-0.10.2-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from botocore<2.0.0,>=1.31->sdv) (2.8.2)\n",
            "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /usr/local/lib/python3.10/dist-packages (from botocore<2.0.0,>=1.31->sdv) (2.0.7)\n",
            "Requirement already satisfied: plotly>=5.10.0 in /usr/local/lib/python3.10/dist-packages (from copulas>=0.11.0->sdv) (5.15.0)\n",
            "Requirement already satisfied: scipy>=1.9.2 in /usr/local/lib/python3.10/dist-packages (from copulas>=0.11.0->sdv) (1.13.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from ctgan>=0.10.0->sdv) (2.4.0+cu121)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.4.0->sdv) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.4.0->sdv) (2024.1)\n",
            "Collecting Faker>=17 (from rdt>=1.12.3->sdv)\n",
            "  Downloading Faker-28.1.0-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: scikit-learn>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from rdt>=1.12.3->sdv) (1.3.2)\n",
            "Collecting plotly>=5.10.0 (from copulas>=0.11.0->sdv)\n",
            "  Downloading plotly-5.24.0-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly>=5.10.0->copulas>=0.11.0->sdv) (9.0.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from plotly>=5.10.0->copulas>=0.11.0->sdv) (24.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<2.0.0,>=1.31->sdv) (1.16.0)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.1.0->rdt>=1.12.3->sdv) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.1.0->rdt>=1.12.3->sdv) (3.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->ctgan>=0.10.0->sdv) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->ctgan>=0.10.0->sdv) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->ctgan>=0.10.0->sdv) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->ctgan>=0.10.0->sdv) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->ctgan>=0.10.0->sdv) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->ctgan>=0.10.0->sdv) (2024.6.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->ctgan>=0.10.0->sdv) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->ctgan>=0.10.0->sdv) (1.3.0)\n",
            "Downloading sdv-1.16.1-py3-none-any.whl (148 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m148.8/148.8 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading boto3-1.35.10-py3-none-any.whl (139 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.2/139.2 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading botocore-1.35.10-py3-none-any.whl (12.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.5/12.5 MB\u001b[0m \u001b[31m50.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading copulas-0.11.1-py3-none-any.whl (51 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.6/51.6 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ctgan-0.10.1-py3-none-any.whl (24 kB)\n",
            "Downloading deepecho-0.6.0-py3-none-any.whl (27 kB)\n",
            "Downloading rdt-1.12.3-py3-none-any.whl (65 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.2/65.2 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sdmetrics-0.15.1-py3-none-any.whl (170 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m170.7/170.7 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Faker-28.1.0-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m42.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Downloading plotly-5.24.0-py3-none-any.whl (19.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.0/19.0 MB\u001b[0m \u001b[31m39.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading s3transfer-0.10.2-py3-none-any.whl (82 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.7/82.7 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: plotly, jmespath, Faker, botocore, s3transfer, rdt, deepecho, copulas, sdmetrics, ctgan, boto3, sdv\n",
            "  Attempting uninstall: plotly\n",
            "    Found existing installation: plotly 5.15.0\n",
            "    Uninstalling plotly-5.15.0:\n",
            "      Successfully uninstalled plotly-5.15.0\n",
            "Successfully installed Faker-28.1.0 boto3-1.35.10 botocore-1.35.10 copulas-0.11.1 ctgan-0.10.1 deepecho-0.6.0 jmespath-1.0.1 plotly-5.24.0 rdt-1.12.3 s3transfer-0.10.2 sdmetrics-0.15.1 sdv-1.16.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -o data.zip https://raw.githubusercontent.com/adipai/data-decent/main/data.zip\n",
        "!unzip data.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kdI76_FnE4Xu",
        "outputId": "a0716562-07be-4183-819f-74ecacaa0a5d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 40.9M  100 40.9M    0     0   9.8M      0  0:00:04  0:00:04 --:--:--  9.8M\n",
            "Archive:  data.zip\n",
            "   creating: data/\n",
            "   creating: data/imbalance_defects_prediction/\n",
            "   creating: data/project_health/\n",
            "  inflating: data/README.md          \n",
            "   creating: data/JavaScript_Vulnerability/\n",
            "   creating: data/Bug_Reports/\n",
            "   creating: data/Vulnerable_Files/\n",
            "   creating: data/defects_prediction/\n",
            "   creating: data/imbalance_defects_prediction/7_CK_NET_PROC/\n",
            "   creating: data/imbalance_defects_prediction/2_NET/\n",
            "   creating: data/imbalance_defects_prediction/4_CK_NET/\n",
            "   creating: data/imbalance_defects_prediction/3_PROC/\n",
            "   creating: data/imbalance_defects_prediction/6_NET_PROC/\n",
            "   creating: data/imbalance_defects_prediction/1_CK/\n",
            "   creating: data/imbalance_defects_prediction/5_CK_PROC/\n",
            "   creating: data/project_health/monthly_closed_PRs_2mo/\n",
            "   creating: data/project_health/monthly_commits_2mo/\n",
            "   creating: data/project_health/monthly_open_PRs_2mo/\n",
            "   creating: data/project_health/monthly_closed_issues_2mo/\n",
            "   creating: data/project_health/monthly_commits_12mo/\n",
            "   creating: data/project_health/monthly_open_issues_2mo/\n",
            "   creating: data/project_health/monthly_closed_issues_12mo/\n",
            "   creating: data/project_health/monthly_contributors_2mo/\n",
            "   creating: data/project_health/monthly_closed_PRs_12mo/\n",
            "  inflating: data/JavaScript_Vulnerability/JSVulnerabilityDataSet-1.0.csv  \n",
            "  inflating: data/Bug_Reports/ambari-train.csv  \n",
            "  inflating: data/Bug_Reports/ambari-test.csv  \n",
            "  inflating: data/Vulnerable_Files/moodle-2_0_0-metrics.arff  \n",
            "  inflating: data/Vulnerable_Files/moodle-2_0_0-tokens.arff  \n",
            "  inflating: data/defects_prediction/Droppy.csv  \n",
            "  inflating: data/defects_prediction/android-transcoder.csv  \n",
            "  inflating: data/defects_prediction/android-flowlayout.csv  \n",
            "   creating: data/imbalance_defects_prediction/7_CK_NET_PROC/input/\n",
            "  inflating: data/imbalance_defects_prediction/7_CK_NET_PROC/data_sets.txt  \n",
            "   creating: data/imbalance_defects_prediction/2_NET/input/\n",
            "  inflating: data/imbalance_defects_prediction/2_NET/data_sets.txt  \n",
            "  inflating: data/imbalance_defects_prediction/4_CK_NET/reImbPUB0.jar  \n",
            "   creating: data/imbalance_defects_prediction/4_CK_NET/input/\n",
            "  inflating: data/imbalance_defects_prediction/4_CK_NET/data_sets.txt  \n",
            "   creating: data/imbalance_defects_prediction/3_PROC/input/\n",
            "  inflating: data/imbalance_defects_prediction/3_PROC/data_sets.txt  \n",
            "   creating: data/imbalance_defects_prediction/6_NET_PROC/input/\n",
            "  inflating: data/imbalance_defects_prediction/6_NET_PROC/data_sets.txt  \n",
            "   creating: data/imbalance_defects_prediction/1_CK/input/\n",
            "  inflating: data/imbalance_defects_prediction/1_CK/data_sets.txt  \n",
            "  inflating: data/imbalance_defects_prediction/5_CK_PROC/reImbPUB0.jar  \n",
            "   creating: data/imbalance_defects_prediction/5_CK_PROC/input/\n",
            "  inflating: data/imbalance_defects_prediction/5_CK_PROC/data_sets.txt  \n",
            "  inflating: data/project_health/monthly_closed_PRs_2mo/health_project0008.csv  \n",
            "  inflating: data/project_health/monthly_closed_PRs_2mo/health_project0009.csv  \n",
            "  inflating: data/project_health/monthly_closed_PRs_2mo/health_project0007.csv  \n",
            "  inflating: data/project_health/monthly_closed_PRs_2mo/health_project0006.csv  \n",
            "  inflating: data/project_health/monthly_closed_PRs_2mo/health_project0004.csv  \n",
            "  inflating: data/project_health/monthly_closed_PRs_2mo/health_project0010.csv  \n",
            "  inflating: data/project_health/monthly_closed_PRs_2mo/health_project0011.csv  \n",
            "  inflating: data/project_health/monthly_closed_PRs_2mo/health_project0005.csv  \n",
            "  inflating: data/project_health/monthly_closed_PRs_2mo/health_project0001.csv  \n",
            "  inflating: data/project_health/monthly_closed_PRs_2mo/health_project0000.csv  \n",
            "  inflating: data/project_health/monthly_closed_PRs_2mo/health_project0002.csv  \n",
            "  inflating: data/project_health/monthly_closed_PRs_2mo/health_project0003.csv  \n",
            "  inflating: data/project_health/monthly_commits_2mo/health_project0008.csv  \n",
            "  inflating: data/project_health/monthly_commits_2mo/health_project0009.csv  \n",
            "  inflating: data/project_health/monthly_commits_2mo/health_project0007.csv  \n",
            "  inflating: data/project_health/monthly_commits_2mo/health_project0006.csv  \n",
            "  inflating: data/project_health/monthly_commits_2mo/health_project0004.csv  \n",
            "  inflating: data/project_health/monthly_commits_2mo/health_project0010.csv  \n",
            "  inflating: data/project_health/monthly_commits_2mo/health_project0011.csv  \n",
            "  inflating: data/project_health/monthly_commits_2mo/health_project0005.csv  \n",
            "  inflating: data/project_health/monthly_commits_2mo/health_project0001.csv  \n",
            "  inflating: data/project_health/monthly_commits_2mo/health_project0000.csv  \n",
            "  inflating: data/project_health/monthly_commits_2mo/health_project0002.csv  \n",
            "  inflating: data/project_health/monthly_commits_2mo/health_project0003.csv  \n",
            "  inflating: data/project_health/monthly_open_PRs_2mo/health_project0008.csv  \n",
            "  inflating: data/project_health/monthly_open_PRs_2mo/health_project0009.csv  \n",
            "  inflating: data/project_health/monthly_open_PRs_2mo/health_project0007.csv  \n",
            "  inflating: data/project_health/monthly_open_PRs_2mo/health_project0006.csv  \n",
            "  inflating: data/project_health/monthly_open_PRs_2mo/health_project0004.csv  \n",
            "  inflating: data/project_health/monthly_open_PRs_2mo/health_project0010.csv  \n",
            "  inflating: data/project_health/monthly_open_PRs_2mo/health_project0011.csv  \n",
            "  inflating: data/project_health/monthly_open_PRs_2mo/health_project0005.csv  \n",
            "  inflating: data/project_health/monthly_open_PRs_2mo/health_project0001.csv  \n",
            "  inflating: data/project_health/monthly_open_PRs_2mo/health_project0000.csv  \n",
            "  inflating: data/project_health/monthly_open_PRs_2mo/health_project0002.csv  \n",
            "  inflating: data/project_health/monthly_open_PRs_2mo/health_project0003.csv  \n",
            "  inflating: data/project_health/monthly_closed_issues_2mo/health_project0008.csv  \n",
            "  inflating: data/project_health/monthly_closed_issues_2mo/health_project0009.csv  \n",
            "  inflating: data/project_health/monthly_closed_issues_2mo/health_project0007.csv  \n",
            "  inflating: data/project_health/monthly_closed_issues_2mo/health_project0006.csv  \n",
            "  inflating: data/project_health/monthly_closed_issues_2mo/health_project0004.csv  \n",
            "  inflating: data/project_health/monthly_closed_issues_2mo/health_project0010.csv  \n",
            "  inflating: data/project_health/monthly_closed_issues_2mo/health_project0011.csv  \n",
            "  inflating: data/project_health/monthly_closed_issues_2mo/health_project0005.csv  \n",
            "  inflating: data/project_health/monthly_closed_issues_2mo/health_project0001.csv  \n",
            "  inflating: data/project_health/monthly_closed_issues_2mo/health_project0000.csv  \n",
            "  inflating: data/project_health/monthly_closed_issues_2mo/health_project0002.csv  \n",
            "  inflating: data/project_health/monthly_closed_issues_2mo/health_project0003.csv  \n",
            "  inflating: data/project_health/monthly_commits_12mo/health_project0008.csv  \n",
            "  inflating: data/project_health/monthly_commits_12mo/health_project0009.csv  \n",
            "  inflating: data/project_health/monthly_commits_12mo/health_project0007.csv  \n",
            "  inflating: data/project_health/monthly_commits_12mo/health_project0006.csv  \n",
            "  inflating: data/project_health/monthly_commits_12mo/health_project0004.csv  \n",
            "  inflating: data/project_health/monthly_commits_12mo/health_project0010.csv  \n",
            "  inflating: data/project_health/monthly_commits_12mo/health_project0011.csv  \n",
            "  inflating: data/project_health/monthly_commits_12mo/health_project0005.csv  \n",
            "  inflating: data/project_health/monthly_commits_12mo/health_project0001.csv  \n",
            "  inflating: data/project_health/monthly_commits_12mo/health_project0000.csv  \n",
            "  inflating: data/project_health/monthly_commits_12mo/health_project0002.csv  \n",
            "  inflating: data/project_health/monthly_commits_12mo/health_project0003.csv  \n",
            "  inflating: data/project_health/monthly_open_issues_2mo/health_project0008.csv  \n",
            "  inflating: data/project_health/monthly_open_issues_2mo/health_project0009.csv  \n",
            "  inflating: data/project_health/monthly_open_issues_2mo/health_project0007.csv  \n",
            "  inflating: data/project_health/monthly_open_issues_2mo/health_project0006.csv  \n",
            "  inflating: data/project_health/monthly_open_issues_2mo/health_project0004.csv  \n",
            "  inflating: data/project_health/monthly_open_issues_2mo/health_project0010.csv  \n",
            "  inflating: data/project_health/monthly_open_issues_2mo/health_project0011.csv  \n",
            "  inflating: data/project_health/monthly_open_issues_2mo/health_project0005.csv  \n",
            "  inflating: data/project_health/monthly_open_issues_2mo/health_project0001.csv  \n",
            "  inflating: data/project_health/monthly_open_issues_2mo/health_project0000.csv  \n",
            "  inflating: data/project_health/monthly_open_issues_2mo/health_project0002.csv  \n",
            "  inflating: data/project_health/monthly_open_issues_2mo/health_project0003.csv  \n",
            "  inflating: data/project_health/monthly_closed_issues_12mo/health_project0008.csv  \n",
            "  inflating: data/project_health/monthly_closed_issues_12mo/health_project0009.csv  \n",
            "  inflating: data/project_health/monthly_closed_issues_12mo/health_project0007.csv  \n",
            "  inflating: data/project_health/monthly_closed_issues_12mo/health_project0006.csv  \n",
            "  inflating: data/project_health/monthly_closed_issues_12mo/health_project0004.csv  \n",
            "  inflating: data/project_health/monthly_closed_issues_12mo/health_project0010.csv  \n",
            "  inflating: data/project_health/monthly_closed_issues_12mo/health_project0011.csv  \n",
            "  inflating: data/project_health/monthly_closed_issues_12mo/health_project0005.csv  \n",
            "  inflating: data/project_health/monthly_closed_issues_12mo/health_project0001.csv  \n",
            "  inflating: data/project_health/monthly_closed_issues_12mo/health_project0000.csv  \n",
            "  inflating: data/project_health/monthly_closed_issues_12mo/health_project0002.csv  \n",
            "  inflating: data/project_health/monthly_closed_issues_12mo/health_project0003.csv  \n",
            "  inflating: data/project_health/monthly_contributors_2mo/health_project0008.csv  \n",
            "  inflating: data/project_health/monthly_contributors_2mo/health_project0009.csv  \n",
            "  inflating: data/project_health/monthly_contributors_2mo/health_project0007.csv  \n",
            "  inflating: data/project_health/monthly_contributors_2mo/health_project0006.csv  \n",
            "  inflating: data/project_health/monthly_contributors_2mo/health_project0004.csv  \n",
            "  inflating: data/project_health/monthly_contributors_2mo/health_project0010.csv  \n",
            "  inflating: data/project_health/monthly_contributors_2mo/health_project0011.csv  \n",
            "  inflating: data/project_health/monthly_contributors_2mo/health_project0005.csv  \n",
            "  inflating: data/project_health/monthly_contributors_2mo/health_project0001.csv  \n",
            "  inflating: data/project_health/monthly_contributors_2mo/health_project0000.csv  \n",
            "  inflating: data/project_health/monthly_contributors_2mo/health_project0002.csv  \n",
            "  inflating: data/project_health/monthly_contributors_2mo/health_project0003.csv  \n",
            "  inflating: data/project_health/monthly_closed_PRs_12mo/health_project0008.csv  \n",
            "  inflating: data/project_health/monthly_closed_PRs_12mo/health_project0009.csv  \n",
            "  inflating: data/project_health/monthly_closed_PRs_12mo/health_project0007.csv  \n",
            "  inflating: data/project_health/monthly_closed_PRs_12mo/health_project0006.csv  \n",
            "  inflating: data/project_health/monthly_closed_PRs_12mo/health_project0004.csv  \n",
            "  inflating: data/project_health/monthly_closed_PRs_12mo/health_project0010.csv  \n",
            "  inflating: data/project_health/monthly_closed_PRs_12mo/health_project0011.csv  \n",
            "  inflating: data/project_health/monthly_closed_PRs_12mo/health_project0005.csv  \n",
            "  inflating: data/project_health/monthly_closed_PRs_12mo/health_project0001.csv  \n",
            "  inflating: data/project_health/monthly_closed_PRs_12mo/health_project0000.csv  \n",
            "  inflating: data/project_health/monthly_closed_PRs_12mo/health_project0002.csv  \n",
            "  inflating: data/project_health/monthly_closed_PRs_12mo/health_project0003.csv  \n",
            "  inflating: data/imbalance_defects_prediction/7_CK_NET_PROC/input/jedit-4.0--CK_NET_PROC.arff  \n",
            "  inflating: data/imbalance_defects_prediction/7_CK_NET_PROC/input/Lucene--CK_NET_PROC.arff  \n",
            "  inflating: data/imbalance_defects_prediction/7_CK_NET_PROC/input/synapse-1.2--CK_NET_PROC.arff  \n",
            "  inflating: data/imbalance_defects_prediction/7_CK_NET_PROC/input/jedit-4.1--CK_NET_PROC.arff  \n",
            "  inflating: data/imbalance_defects_prediction/7_CK_NET_PROC/input/synapse-1.1--CK_NET_PROC.arff  \n",
            "  inflating: data/imbalance_defects_prediction/7_CK_NET_PROC/input/jedit-4.2--CK_NET_PROC.arff  \n",
            "  inflating: data/imbalance_defects_prediction/7_CK_NET_PROC/input/Equinox_Framework--CK_NET_PROC.arff  \n",
            "  inflating: data/imbalance_defects_prediction/7_CK_NET_PROC/input/xerces-1.2--CK_NET_PROC.arff  \n",
            "  inflating: data/imbalance_defects_prediction/7_CK_NET_PROC/input/velocity-1.6--CK_NET_PROC.arff  \n",
            "  inflating: data/imbalance_defects_prediction/7_CK_NET_PROC/input/synapse-1.0--CK_NET_PROC.arff  \n",
            "  inflating: data/imbalance_defects_prediction/7_CK_NET_PROC/input/jedit-4.3--CK_NET_PROC.arff  \n",
            "  inflating: data/imbalance_defects_prediction/7_CK_NET_PROC/input/xerces-1.3--CK_NET_PROC.arff  \n",
            "  inflating: data/imbalance_defects_prediction/7_CK_NET_PROC/input/jedit-3.2--CK_NET_PROC.arff  \n",
            "  inflating: data/imbalance_defects_prediction/7_CK_NET_PROC/input/ant-1.4--CK_NET_PROC.arff  \n",
            "  inflating: data/imbalance_defects_prediction/7_CK_NET_PROC/input/poi-2.0--CK_NET_PROC.arff  \n",
            "  inflating: data/imbalance_defects_prediction/7_CK_NET_PROC/input/camel-1.2--CK_NET_PROC.arff  \n",
            "  inflating: data/imbalance_defects_prediction/7_CK_NET_PROC/input/ant-1.5--CK_NET_PROC.arff  \n",
            "  inflating: data/imbalance_defects_prediction/7_CK_NET_PROC/input/camel-1.6--CK_NET_PROC.arff  \n",
            "  inflating: data/imbalance_defects_prediction/7_CK_NET_PROC/input/Eclipse_PDE_UI--CK_NET_PROC.arff  \n",
            "  inflating: data/imbalance_defects_prediction/7_CK_NET_PROC/input/ant-1.6--CK_NET_PROC.arff  \n",
            "  inflating: data/imbalance_defects_prediction/7_CK_NET_PROC/input/ant-1.3--CK_NET_PROC.arff  \n",
            "  inflating: data/imbalance_defects_prediction/7_CK_NET_PROC/input/Eclipse_JDT_Core--CK_NET_PROC.arff  \n",
            "  inflating: data/imbalance_defects_prediction/7_CK_NET_PROC/input/camel-1.0--CK_NET_PROC.arff  \n",
            "  inflating: data/imbalance_defects_prediction/7_CK_NET_PROC/input/Mylyn--CK_NET_PROC.arff  \n",
            "  inflating: data/imbalance_defects_prediction/7_CK_NET_PROC/input/camel-1.4--CK_NET_PROC.arff  \n",
            "  inflating: data/imbalance_defects_prediction/7_CK_NET_PROC/input/log4j-1.0--CK_NET_PROC.arff  \n",
            "  inflating: data/imbalance_defects_prediction/7_CK_NET_PROC/input/ivy-2.0--CK_NET_PROC.arff  \n",
            "  inflating: data/imbalance_defects_prediction/2_NET/input/jedit-4.0--NET.arff  \n",
            "  inflating: data/imbalance_defects_prediction/2_NET/input/camel-1.4--NET.arff  \n",
            "  inflating: data/imbalance_defects_prediction/2_NET/input/Eclipse_PDE_UI--NET.arff  \n",
            "  inflating: data/imbalance_defects_prediction/2_NET/input/ant-1.6--NET.arff  \n",
            "  inflating: data/imbalance_defects_prediction/2_NET/input/camel-1.2--NET.arff  \n",
            "  inflating: data/imbalance_defects_prediction/2_NET/input/Equinox_Framework--NET.arff  \n",
            "  inflating: data/imbalance_defects_prediction/2_NET/input/Eclipse_JDT_Core--NET.arff  \n",
            "  inflating: data/imbalance_defects_prediction/2_NET/input/jedit-4.1--NET.arff  \n",
            "  inflating: data/imbalance_defects_prediction/2_NET/input/synapse-1.2--NET.arff  \n",
            "  inflating: data/imbalance_defects_prediction/2_NET/input/Mylyn--NET.arff  \n",
            "  inflating: data/imbalance_defects_prediction/2_NET/input/Lucene--NET.arff  \n",
            "  inflating: data/imbalance_defects_prediction/2_NET/input/jedit-3.2--NET.arff  \n",
            "  inflating: data/imbalance_defects_prediction/2_NET/input/poi-2.0--NET.arff  \n",
            "  inflating: data/imbalance_defects_prediction/2_NET/input/synapse-1.1--NET.arff  \n",
            "  inflating: data/imbalance_defects_prediction/2_NET/input/jedit-4.2--NET.arff  \n",
            "  inflating: data/imbalance_defects_prediction/2_NET/input/camel-1.6--NET.arff  \n",
            "  inflating: data/imbalance_defects_prediction/2_NET/input/velocity-1.6--NET.arff  \n",
            "  inflating: data/imbalance_defects_prediction/2_NET/input/xerces-1.3--NET.arff  \n",
            "  inflating: data/imbalance_defects_prediction/2_NET/input/ivy-2.0--NET.arff  \n",
            "  inflating: data/imbalance_defects_prediction/2_NET/input/ant-1.4--NET.arff  \n",
            "  inflating: data/imbalance_defects_prediction/2_NET/input/camel-1.0--NET.arff  \n",
            "  inflating: data/imbalance_defects_prediction/2_NET/input/jedit-4.3--NET.arff  \n",
            "  inflating: data/imbalance_defects_prediction/2_NET/input/synapse-1.0--NET.arff  \n",
            "  inflating: data/imbalance_defects_prediction/2_NET/input/ant-1.3--NET.arff  \n",
            "  inflating: data/imbalance_defects_prediction/2_NET/input/xerces-1.2--NET.arff  \n",
            "  inflating: data/imbalance_defects_prediction/2_NET/input/ant-1.5--NET.arff  \n",
            "  inflating: data/imbalance_defects_prediction/2_NET/input/log4j-1.0--NET.arff  \n",
            "  inflating: data/imbalance_defects_prediction/4_CK_NET/input/synapse-1.2--CK_NET.arff  \n",
            "  inflating: data/imbalance_defects_prediction/4_CK_NET/input/jedit-3.2--CK_NET.arff  \n",
            "  inflating: data/imbalance_defects_prediction/4_CK_NET/input/camel-1.2--CK_NET.arff  \n",
            "  inflating: data/imbalance_defects_prediction/4_CK_NET/input/ant-1.6--CK_NET.arff  \n",
            "  inflating: data/imbalance_defects_prediction/4_CK_NET/input/velocity-1.6--CK_NET.arff  \n",
            "  inflating: data/imbalance_defects_prediction/4_CK_NET/input/jedit-4.2--CK_NET.arff  \n",
            "  inflating: data/imbalance_defects_prediction/4_CK_NET/input/xerces-1.2--CK_NET.arff  \n",
            "  inflating: data/imbalance_defects_prediction/4_CK_NET/input/camel-1.4--CK_NET.arff  \n",
            "  inflating: data/imbalance_defects_prediction/4_CK_NET/input/Lucene--CK_NET.arff  \n",
            "  inflating: data/imbalance_defects_prediction/4_CK_NET/input/xerces-1.3--CK_NET.arff  \n",
            "  inflating: data/imbalance_defects_prediction/4_CK_NET/input/jedit-4.3--CK_NET.arff  \n",
            "  inflating: data/imbalance_defects_prediction/4_CK_NET/input/Mylyn--CK_NET.arff  \n",
            "  inflating: data/imbalance_defects_prediction/4_CK_NET/input/log4j-1.0--CK_NET.arff  \n",
            "  inflating: data/imbalance_defects_prediction/4_CK_NET/input/Eclipse_JDT_Core--CK_NET.arff  \n",
            "  inflating: data/imbalance_defects_prediction/4_CK_NET/input/Equinox_Framework--CK_NET.arff  \n",
            "  inflating: data/imbalance_defects_prediction/4_CK_NET/input/poi-2.0--CK_NET.arff  \n",
            "  inflating: data/imbalance_defects_prediction/4_CK_NET/input/Eclipse_PDE_UI--CK_NET.arff  \n",
            "  inflating: data/imbalance_defects_prediction/4_CK_NET/input/ant-1.5--CK_NET.arff  \n",
            "  inflating: data/imbalance_defects_prediction/4_CK_NET/input/camel-1.6--CK_NET.arff  \n",
            "  inflating: data/imbalance_defects_prediction/4_CK_NET/input/synapse-1.1--CK_NET.arff  \n",
            "  inflating: data/imbalance_defects_prediction/4_CK_NET/input/jedit-4.1--CK_NET.arff  \n",
            "  inflating: data/imbalance_defects_prediction/4_CK_NET/input/synapse-1.0--CK_NET.arff  \n",
            "  inflating: data/imbalance_defects_prediction/4_CK_NET/input/ant-1.3--CK_NET.arff  \n",
            "  inflating: data/imbalance_defects_prediction/4_CK_NET/input/ivy-2.0--CK_NET.arff  \n",
            "  inflating: data/imbalance_defects_prediction/4_CK_NET/input/ant-1.4--CK_NET.arff  \n",
            "  inflating: data/imbalance_defects_prediction/4_CK_NET/input/camel-1.0--CK_NET.arff  \n",
            "  inflating: data/imbalance_defects_prediction/4_CK_NET/input/jedit-4.0--CK_NET.arff  \n",
            "  inflating: data/imbalance_defects_prediction/3_PROC/input/xerces-1.2--PROC.arff  \n",
            "  inflating: data/imbalance_defects_prediction/3_PROC/input/poi-2.0--PROC.arff  \n",
            "  inflating: data/imbalance_defects_prediction/3_PROC/input/camel-1.4--PROC.arff  \n",
            "  inflating: data/imbalance_defects_prediction/3_PROC/input/log4j-1.0--PROC.arff  \n",
            "  inflating: data/imbalance_defects_prediction/3_PROC/input/jedit-4.2--PROC.arff  \n",
            "  inflating: data/imbalance_defects_prediction/3_PROC/input/jedit-3.2--PROC.arff  \n",
            "  inflating: data/imbalance_defects_prediction/3_PROC/input/ant-1.5--PROC.arff  \n",
            "  inflating: data/imbalance_defects_prediction/3_PROC/input/camel-1.6--PROC.arff  \n",
            "  inflating: data/imbalance_defects_prediction/3_PROC/input/jedit-4.0--PROC.arff  \n",
            "  inflating: data/imbalance_defects_prediction/3_PROC/input/synapse-1.1--PROC.arff  \n",
            "  inflating: data/imbalance_defects_prediction/3_PROC/input/camel-1.0--PROC.arff  \n",
            "  inflating: data/imbalance_defects_prediction/3_PROC/input/jedit-4.3--PROC.arff  \n",
            "  inflating: data/imbalance_defects_prediction/3_PROC/input/ant-1.4--PROC.arff  \n",
            "  inflating: data/imbalance_defects_prediction/3_PROC/input/synapse-1.2--PROC.arff  \n",
            "  inflating: data/imbalance_defects_prediction/3_PROC/input/xerces-1.3--PROC.arff  \n",
            "  inflating: data/imbalance_defects_prediction/3_PROC/input/ivy-2.0--PROC.arff  \n",
            "  inflating: data/imbalance_defects_prediction/3_PROC/input/camel-1.2--PROC.arff  \n",
            "  inflating: data/imbalance_defects_prediction/3_PROC/input/jedit-4.1--PROC.arff  \n",
            "  inflating: data/imbalance_defects_prediction/3_PROC/input/Eclipse_PDE_UI--PROC.arff  \n",
            "  inflating: data/imbalance_defects_prediction/3_PROC/input/velocity-1.6--PROC.arff  \n",
            "  inflating: data/imbalance_defects_prediction/3_PROC/input/Mylyn--PROC.arff  \n",
            "  inflating: data/imbalance_defects_prediction/3_PROC/input/ant-1.6--PROC.arff  \n",
            "  inflating: data/imbalance_defects_prediction/3_PROC/input/Equinox_Framework--PROC.arff  \n",
            "  inflating: data/imbalance_defects_prediction/3_PROC/input/synapse-1.0--PROC.arff  \n",
            "  inflating: data/imbalance_defects_prediction/3_PROC/input/Eclipse_JDT_Core--PROC.arff  \n",
            "  inflating: data/imbalance_defects_prediction/3_PROC/input/Lucene--PROC.arff  \n",
            "  inflating: data/imbalance_defects_prediction/3_PROC/input/ant-1.3--PROC.arff  \n",
            "  inflating: data/imbalance_defects_prediction/6_NET_PROC/input/log4j-1.0--NET_PROC.arff  \n",
            "  inflating: data/imbalance_defects_prediction/6_NET_PROC/input/Eclipse_PDE_UI--NET_PROC.arff  \n",
            "  inflating: data/imbalance_defects_prediction/6_NET_PROC/input/synapse-1.1--NET_PROC.arff  \n",
            "  inflating: data/imbalance_defects_prediction/6_NET_PROC/input/Equinox_Framework--NET_PROC.arff  \n",
            "  inflating: data/imbalance_defects_prediction/6_NET_PROC/input/synapse-1.0--NET_PROC.arff  \n",
            "  inflating: data/imbalance_defects_prediction/6_NET_PROC/input/synapse-1.2--NET_PROC.arff  \n",
            "  inflating: data/imbalance_defects_prediction/6_NET_PROC/input/jedit-3.2--NET_PROC.arff  \n",
            "  inflating: data/imbalance_defects_prediction/6_NET_PROC/input/xerces-1.2--NET_PROC.arff  \n",
            "  inflating: data/imbalance_defects_prediction/6_NET_PROC/input/jedit-4.3--NET_PROC.arff  \n",
            "  inflating: data/imbalance_defects_prediction/6_NET_PROC/input/Eclipse_JDT_Core--NET_PROC.arff  \n",
            "  inflating: data/imbalance_defects_prediction/6_NET_PROC/input/Mylyn--NET_PROC.arff  \n",
            "  inflating: data/imbalance_defects_prediction/6_NET_PROC/input/camel-1.0--NET_PROC.arff  \n",
            "  inflating: data/imbalance_defects_prediction/6_NET_PROC/input/ant-1.6--NET_PROC.arff  \n",
            "  inflating: data/imbalance_defects_prediction/6_NET_PROC/input/ivy-2.0--NET_PROC.arff  \n",
            "  inflating: data/imbalance_defects_prediction/6_NET_PROC/input/camel-1.6--NET_PROC.arff  \n",
            "  inflating: data/imbalance_defects_prediction/6_NET_PROC/input/jedit-4.2--NET_PROC.arff  \n",
            "  inflating: data/imbalance_defects_prediction/6_NET_PROC/input/xerces-1.3--NET_PROC.arff  \n",
            "  inflating: data/imbalance_defects_prediction/6_NET_PROC/input/ant-1.3--NET_PROC.arff  \n",
            "  inflating: data/imbalance_defects_prediction/6_NET_PROC/input/jedit-4.0--NET_PROC.arff  \n",
            "  inflating: data/imbalance_defects_prediction/6_NET_PROC/input/camel-1.4--NET_PROC.arff  \n",
            "  inflating: data/imbalance_defects_prediction/6_NET_PROC/input/ant-1.4--NET_PROC.arff  \n",
            "  inflating: data/imbalance_defects_prediction/6_NET_PROC/input/poi-2.0--NET_PROC.arff  \n",
            "  inflating: data/imbalance_defects_prediction/6_NET_PROC/input/ant-1.5--NET_PROC.arff  \n",
            "  inflating: data/imbalance_defects_prediction/6_NET_PROC/input/velocity-1.6--NET_PROC.arff  \n",
            "  inflating: data/imbalance_defects_prediction/6_NET_PROC/input/camel-1.2--NET_PROC.arff  \n",
            "  inflating: data/imbalance_defects_prediction/6_NET_PROC/input/jedit-4.1--NET_PROC.arff  \n",
            "  inflating: data/imbalance_defects_prediction/6_NET_PROC/input/Lucene--NET_PROC.arff  \n",
            "  inflating: data/imbalance_defects_prediction/1_CK/input/synapse-1.0--CK.arff  \n",
            "  inflating: data/imbalance_defects_prediction/1_CK/input/synapse-1.1--CK.arff  \n",
            "  inflating: data/imbalance_defects_prediction/1_CK/input/ant-1.3--CK.arff  \n",
            "  inflating: data/imbalance_defects_prediction/1_CK/input/Equinox_Framework--CK.arff  \n",
            "  inflating: data/imbalance_defects_prediction/1_CK/input/log4j-1.0--CK.arff  \n",
            "  inflating: data/imbalance_defects_prediction/1_CK/input/velocity-1.6--CK.arff  \n",
            "  inflating: data/imbalance_defects_prediction/1_CK/input/jedit-3.2--CK.arff  \n",
            "  inflating: data/imbalance_defects_prediction/1_CK/input/camel-1.6--CK.arff  \n",
            "  inflating: data/imbalance_defects_prediction/1_CK/input/camel-1.0--CK.arff  \n",
            "  inflating: data/imbalance_defects_prediction/1_CK/input/jedit-4.2--CK.arff  \n",
            "  inflating: data/imbalance_defects_prediction/1_CK/input/jedit-4.3--CK.arff  \n",
            "  inflating: data/imbalance_defects_prediction/1_CK/input/Eclipse_JDT_Core--CK.arff  \n",
            "  inflating: data/imbalance_defects_prediction/1_CK/input/ivy-2.0--CK.arff  \n",
            "  inflating: data/imbalance_defects_prediction/1_CK/input/ant-1.5--CK.arff  \n",
            "  inflating: data/imbalance_defects_prediction/1_CK/input/ant-1.4--CK.arff  \n",
            "  inflating: data/imbalance_defects_prediction/1_CK/input/xerces-1.2--CK.arff  \n",
            "  inflating: data/imbalance_defects_prediction/1_CK/input/xerces-1.3--CK.arff  \n",
            "  inflating: data/imbalance_defects_prediction/1_CK/input/camel-1.4--CK.arff  \n",
            "  inflating: data/imbalance_defects_prediction/1_CK/input/synapse-1.2--CK.arff  \n",
            "  inflating: data/imbalance_defects_prediction/1_CK/input/ant-1.6--CK.arff  \n",
            "  inflating: data/imbalance_defects_prediction/1_CK/input/poi-2.0--CK.arff  \n",
            "  inflating: data/imbalance_defects_prediction/1_CK/input/Eclipse_PDE_UI--CK.arff  \n",
            "  inflating: data/imbalance_defects_prediction/1_CK/input/jedit-4.1--CK.arff  \n",
            "  inflating: data/imbalance_defects_prediction/1_CK/input/jedit-4.0--CK.arff  \n",
            "  inflating: data/imbalance_defects_prediction/1_CK/input/Mylyn--CK.arff  \n",
            "  inflating: data/imbalance_defects_prediction/1_CK/input/Lucene--CK.arff  \n",
            "  inflating: data/imbalance_defects_prediction/1_CK/input/camel-1.2--CK.arff  \n",
            "  inflating: data/imbalance_defects_prediction/5_CK_PROC/input/synapse-1.1--CK_PROC.arff  \n",
            "  inflating: data/imbalance_defects_prediction/5_CK_PROC/input/velocity-1.6--CK_PROC.arff  \n",
            "  inflating: data/imbalance_defects_prediction/5_CK_PROC/input/ant-1.6--CK_PROC.arff  \n",
            "  inflating: data/imbalance_defects_prediction/5_CK_PROC/input/Mylyn--CK_PROC.arff  \n",
            "  inflating: data/imbalance_defects_prediction/5_CK_PROC/input/ant-1.5--CK_PROC.arff  \n",
            "  inflating: data/imbalance_defects_prediction/5_CK_PROC/input/ivy-2.0--CK_PROC.arff  \n",
            "  inflating: data/imbalance_defects_prediction/5_CK_PROC/input/synapse-1.2--CK_PROC.arff  \n",
            "  inflating: data/imbalance_defects_prediction/5_CK_PROC/input/Lucene--CK_PROC.arff  \n",
            "  inflating: data/imbalance_defects_prediction/5_CK_PROC/input/Eclipse_JDT_Core--CK_PROC.arff  \n",
            "  inflating: data/imbalance_defects_prediction/5_CK_PROC/input/Equinox_Framework--CK_PROC.arff  \n",
            "  inflating: data/imbalance_defects_prediction/5_CK_PROC/input/xerces-1.2--CK_PROC.arff  \n",
            "  inflating: data/imbalance_defects_prediction/5_CK_PROC/input/jedit-4.2--CK_PROC.arff  \n",
            "  inflating: data/imbalance_defects_prediction/5_CK_PROC/input/jedit-4.1--CK_PROC.arff  \n",
            "  inflating: data/imbalance_defects_prediction/5_CK_PROC/input/camel-1.0--CK_PROC.arff  \n",
            "  inflating: data/imbalance_defects_prediction/5_CK_PROC/input/jedit-3.2--CK_PROC.arff  \n",
            "  inflating: data/imbalance_defects_prediction/5_CK_PROC/input/camel-1.4--CK_PROC.arff  \n",
            "  inflating: data/imbalance_defects_prediction/5_CK_PROC/input/jedit-4.0--CK_PROC.arff  \n",
            "  inflating: data/imbalance_defects_prediction/5_CK_PROC/input/log4j-1.0--CK_PROC.arff  \n",
            "  inflating: data/imbalance_defects_prediction/5_CK_PROC/input/xerces-1.3--CK_PROC.arff  \n",
            "  inflating: data/imbalance_defects_prediction/5_CK_PROC/input/camel-1.6--CK_PROC.arff  \n",
            "  inflating: data/imbalance_defects_prediction/5_CK_PROC/input/jedit-4.3--CK_PROC.arff  \n",
            "  inflating: data/imbalance_defects_prediction/5_CK_PROC/input/camel-1.2--CK_PROC.arff  \n",
            "  inflating: data/imbalance_defects_prediction/5_CK_PROC/input/ant-1.4--CK_PROC.arff  \n",
            "  inflating: data/imbalance_defects_prediction/5_CK_PROC/input/Eclipse_PDE_UI--CK_PROC.arff  \n",
            "  inflating: data/imbalance_defects_prediction/5_CK_PROC/input/poi-2.0--CK_PROC.arff  \n",
            "  inflating: data/imbalance_defects_prediction/5_CK_PROC/input/synapse-1.0--CK_PROC.arff  \n",
            "  inflating: data/imbalance_defects_prediction/5_CK_PROC/input/ant-1.3--CK_PROC.arff  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# All imports here\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from pmlb import fetch_data\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import random\n",
        "import time\n",
        "import math\n",
        "import warnings\n",
        "\n",
        "from scipy.io import arff\n",
        "from sdv.datasets.local import load_csvs\n",
        "from sdv.metadata import SingleTableMetadata\n",
        "from sdv.single_table import GaussianCopulaSynthesizer\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.over_sampling import SVMSMOTE\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from collections import defaultdict\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_curve, auc, confusion_matrix\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from lightgbm import LGBMClassifier"
      ],
      "metadata": {
        "id": "Lza8MeLYchI2"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "77DtIcZp_AP4"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data preprocessing"
      ],
      "metadata": {
        "id": "KH_s6WhMv_Vg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_data_train(X_train):\n",
        "    # Count missing values before handling missing data\n",
        "    missing_before = np.isnan(X_train).sum()\n",
        "\n",
        "    # Handle missing data\n",
        "    imputer = SimpleImputer(strategy='mean')\n",
        "    X_train = imputer.fit_transform(X_train)\n",
        "\n",
        "    # Count missing values after handling missing data\n",
        "    missing_after = np.isnan(X_train).sum()\n",
        "\n",
        "    # Normalize numeric columns\n",
        "    scaler = StandardScaler()\n",
        "    X_train = scaler.fit_transform(X_train)\n",
        "\n",
        "    return X_train, scaler, imputer\n",
        "\n",
        "def preprocess_data_test(X_test, scaler, imputer):\n",
        "    # Count missing values before handling missing data\n",
        "    missing_before = np.isnan(X_test).sum()\n",
        "\n",
        "    # Handle missing data\n",
        "    X_test = imputer.transform(X_test)\n",
        "\n",
        "    # Count missing values after handling missing data\n",
        "    missing_after = np.isnan(X_test).sum()\n",
        "\n",
        "    # Normalize numeric columns\n",
        "    X_test = scaler.transform(X_test)\n",
        "\n",
        "    return X_test"
      ],
      "metadata": {
        "id": "A9ept3j9vmUy"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Experiments"
      ],
      "metadata": {
        "id": "3Rjlt8zr4vyO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset: Eclipse PDE"
      ],
      "metadata": {
        "id": "p22RSGg043d5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "project = \"Defect_Eclipse_PDE_UI\"\n",
        "fname = \"_\".join(project.split(\"_\")[1:])\n",
        "data_path = f\"data/imbalance_defects_prediction/7_CK_NET_PROC/input/{fname}--CK_NET_PROC.arff\"\n",
        "data = arff.loadarff(data_path)\n",
        "df = pd.DataFrame(data[0])\n",
        "df['isBug'] = df['isBug'].astype('str')\n",
        "d = {'YES': 1, 'NO': 0}  # Remove the byte string prefix 'b'\n",
        "df['isBug'] = df['isBug'].map(d).fillna(df['isBug'])\n",
        "print(df['isBug'])\n",
        "print(\"before drop duplicates\", df.shape[0])\n",
        "df = df.drop_duplicates()\n",
        "df.reset_index(inplace=True, drop=True)\n",
        "print(\"after drop duplicates\", df.shape[0])\n",
        "\n",
        "df.describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 600
        },
        "id": "wyi4iWJFwIhU",
        "outputId": "b2b20574-3774-436f-c0df-ffa2953d543a"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0       1\n",
            "1       0\n",
            "2       0\n",
            "3       1\n",
            "4       0\n",
            "       ..\n",
            "1492    0\n",
            "1493    0\n",
            "1494    0\n",
            "1495    0\n",
            "1496    0\n",
            "Name: isBug, Length: 1497, dtype: int64\n",
            "before drop duplicates 1497\n",
            "after drop duplicates 1497\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "               wmc          dit          rfc          noc          cbo  \\\n",
              "count  1497.000000  1497.000000  1497.000000  1497.000000  1497.000000   \n",
              "mean     23.748831     2.280561    47.502338     0.595858    10.208417   \n",
              "std      31.414402     1.565026    63.113652     2.434228    14.831382   \n",
              "min       0.000000     1.000000     0.000000     0.000000     0.000000   \n",
              "25%       6.000000     1.000000    11.000000     0.000000     3.000000   \n",
              "50%      13.000000     2.000000    25.000000     0.000000     7.000000   \n",
              "75%      28.000000     3.000000    57.000000     0.000000    13.000000   \n",
              "max     286.000000     9.000000   599.000000    46.000000   362.000000   \n",
              "\n",
              "              lcom          loc  revision_num   author_num  linesadd_sum  ...  \\\n",
              "count  1497.000000  1497.000000   1497.000000  1497.000000   1497.000000  ...   \n",
              "mean     82.175685    98.164329     13.512358     3.971276    211.704075  ...   \n",
              "std     210.815685   128.634872     18.884925     2.177879    384.246071  ...   \n",
              "min       0.000000     0.000000      0.000000     0.000000      0.000000  ...   \n",
              "25%       6.000000    24.000000      4.000000     2.000000     21.000000  ...   \n",
              "50%      21.000000    52.000000      8.000000     3.000000     78.000000  ...   \n",
              "75%      66.000000   116.000000     16.000000     6.000000    221.000000  ...   \n",
              "max    3321.000000  1326.000000    410.000000    10.000000   3644.000000  ...   \n",
              "\n",
              "         InFreeClo    OutValClo     InValClo  OutRecipClo   InRecipClo  \\\n",
              "count  1497.000000  1497.000000  1497.000000  1497.000000  1497.000000   \n",
              "mean      0.001132     0.257043     0.257043     0.054766     0.054766   \n",
              "std       0.000635     0.194478     0.293171     0.040618     0.067921   \n",
              "min       0.000668     0.000000     0.000000     0.000668     0.000668   \n",
              "25%       0.000669     0.016684     0.001337     0.007902     0.002004   \n",
              "50%       0.000678     0.436592     0.015344     0.062817     0.006151   \n",
              "75%       0.001361     0.438191     0.509428     0.088306     0.097933   \n",
              "max       0.003036     0.498919     0.780527     0.151869     0.443954   \n",
              "\n",
              "        OutdwReach    IndwReach  nOutdwReach   nIndwReach        isBug  \n",
              "count  1497.000000  1497.000000  1497.000000  1497.000000  1497.000000  \n",
              "mean     82.188384    82.188371     0.054902     0.054902     0.139613  \n",
              "std      60.948191   101.878414     0.040714     0.068055     0.346700  \n",
              "min       1.000000     1.000000     0.000668     0.000668     0.000000  \n",
              "25%      11.833334     3.000000     0.007905     0.002004     0.000000  \n",
              "50%      94.413429     9.219047     0.063068     0.006158     0.000000  \n",
              "75%     132.544647   147.026443     0.088540     0.098214     0.000000  \n",
              "max     227.633392   664.717651     0.152060     0.444033     1.000000  \n",
              "\n",
              "[8 rows x 82 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7fdd9a6c-b96c-4b71-907a-a90a24c2cff3\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>wmc</th>\n",
              "      <th>dit</th>\n",
              "      <th>rfc</th>\n",
              "      <th>noc</th>\n",
              "      <th>cbo</th>\n",
              "      <th>lcom</th>\n",
              "      <th>loc</th>\n",
              "      <th>revision_num</th>\n",
              "      <th>author_num</th>\n",
              "      <th>linesadd_sum</th>\n",
              "      <th>...</th>\n",
              "      <th>InFreeClo</th>\n",
              "      <th>OutValClo</th>\n",
              "      <th>InValClo</th>\n",
              "      <th>OutRecipClo</th>\n",
              "      <th>InRecipClo</th>\n",
              "      <th>OutdwReach</th>\n",
              "      <th>IndwReach</th>\n",
              "      <th>nOutdwReach</th>\n",
              "      <th>nIndwReach</th>\n",
              "      <th>isBug</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>1497.000000</td>\n",
              "      <td>1497.000000</td>\n",
              "      <td>1497.000000</td>\n",
              "      <td>1497.000000</td>\n",
              "      <td>1497.000000</td>\n",
              "      <td>1497.000000</td>\n",
              "      <td>1497.000000</td>\n",
              "      <td>1497.000000</td>\n",
              "      <td>1497.000000</td>\n",
              "      <td>1497.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>1497.000000</td>\n",
              "      <td>1497.000000</td>\n",
              "      <td>1497.000000</td>\n",
              "      <td>1497.000000</td>\n",
              "      <td>1497.000000</td>\n",
              "      <td>1497.000000</td>\n",
              "      <td>1497.000000</td>\n",
              "      <td>1497.000000</td>\n",
              "      <td>1497.000000</td>\n",
              "      <td>1497.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>23.748831</td>\n",
              "      <td>2.280561</td>\n",
              "      <td>47.502338</td>\n",
              "      <td>0.595858</td>\n",
              "      <td>10.208417</td>\n",
              "      <td>82.175685</td>\n",
              "      <td>98.164329</td>\n",
              "      <td>13.512358</td>\n",
              "      <td>3.971276</td>\n",
              "      <td>211.704075</td>\n",
              "      <td>...</td>\n",
              "      <td>0.001132</td>\n",
              "      <td>0.257043</td>\n",
              "      <td>0.257043</td>\n",
              "      <td>0.054766</td>\n",
              "      <td>0.054766</td>\n",
              "      <td>82.188384</td>\n",
              "      <td>82.188371</td>\n",
              "      <td>0.054902</td>\n",
              "      <td>0.054902</td>\n",
              "      <td>0.139613</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>31.414402</td>\n",
              "      <td>1.565026</td>\n",
              "      <td>63.113652</td>\n",
              "      <td>2.434228</td>\n",
              "      <td>14.831382</td>\n",
              "      <td>210.815685</td>\n",
              "      <td>128.634872</td>\n",
              "      <td>18.884925</td>\n",
              "      <td>2.177879</td>\n",
              "      <td>384.246071</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000635</td>\n",
              "      <td>0.194478</td>\n",
              "      <td>0.293171</td>\n",
              "      <td>0.040618</td>\n",
              "      <td>0.067921</td>\n",
              "      <td>60.948191</td>\n",
              "      <td>101.878414</td>\n",
              "      <td>0.040714</td>\n",
              "      <td>0.068055</td>\n",
              "      <td>0.346700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000668</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000668</td>\n",
              "      <td>0.000668</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000668</td>\n",
              "      <td>0.000668</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>6.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>11.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>21.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000669</td>\n",
              "      <td>0.016684</td>\n",
              "      <td>0.001337</td>\n",
              "      <td>0.007902</td>\n",
              "      <td>0.002004</td>\n",
              "      <td>11.833334</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>0.007905</td>\n",
              "      <td>0.002004</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>13.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>25.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>7.000000</td>\n",
              "      <td>21.000000</td>\n",
              "      <td>52.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>78.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000678</td>\n",
              "      <td>0.436592</td>\n",
              "      <td>0.015344</td>\n",
              "      <td>0.062817</td>\n",
              "      <td>0.006151</td>\n",
              "      <td>94.413429</td>\n",
              "      <td>9.219047</td>\n",
              "      <td>0.063068</td>\n",
              "      <td>0.006158</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>28.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>57.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>13.000000</td>\n",
              "      <td>66.000000</td>\n",
              "      <td>116.000000</td>\n",
              "      <td>16.000000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>221.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.001361</td>\n",
              "      <td>0.438191</td>\n",
              "      <td>0.509428</td>\n",
              "      <td>0.088306</td>\n",
              "      <td>0.097933</td>\n",
              "      <td>132.544647</td>\n",
              "      <td>147.026443</td>\n",
              "      <td>0.088540</td>\n",
              "      <td>0.098214</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>286.000000</td>\n",
              "      <td>9.000000</td>\n",
              "      <td>599.000000</td>\n",
              "      <td>46.000000</td>\n",
              "      <td>362.000000</td>\n",
              "      <td>3321.000000</td>\n",
              "      <td>1326.000000</td>\n",
              "      <td>410.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>3644.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.003036</td>\n",
              "      <td>0.498919</td>\n",
              "      <td>0.780527</td>\n",
              "      <td>0.151869</td>\n",
              "      <td>0.443954</td>\n",
              "      <td>227.633392</td>\n",
              "      <td>664.717651</td>\n",
              "      <td>0.152060</td>\n",
              "      <td>0.444033</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8 rows × 82 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7fdd9a6c-b96c-4b71-907a-a90a24c2cff3')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-7fdd9a6c-b96c-4b71-907a-a90a24c2cff3 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-7fdd9a6c-b96c-4b71-907a-a90a24c2cff3');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-413e9492-28f0-47a5-9aa1-7aa356adae9e\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-413e9492-28f0-47a5-9aa1-7aa356adae9e')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-413e9492-28f0-47a5-9aa1-7aa356adae9e button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training and testing using ML models"
      ],
      "metadata": {
        "id": "gGu1Qxej4WAA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generic function to test synthetic data using LR, SVM, DT\n",
        "\n",
        "def evaluate_models(X_train, X_test, y_train, y_test, random_state=42):\n",
        "\n",
        "    # Initialize classifiers\n",
        "    classifiers = {\n",
        "        \"Logistic Regression\": LogisticRegression(random_state=random_state),\n",
        "        \"SVM\": SVC(random_state=random_state),\n",
        "        \"Decision Tree\": DecisionTreeClassifier(random_state=random_state),\n",
        "        \"Gradient Boosting\": GradientBoostingClassifier(random_state=random_state),\n",
        "        \"LightGBM\": LGBMClassifier(random_state = random_state, force_col_wise=True)\n",
        "    }\n",
        "\n",
        "    # Results dictionary to store evaluation metrics\n",
        "    results = {}\n",
        "\n",
        "    # Iterate over classifiers\n",
        "    for name, clf in classifiers.items():\n",
        "        # Fit classifier\n",
        "        clf.fit(X_train, y_train)\n",
        "\n",
        "        # Predictions\n",
        "        y_pred = clf.predict(X_test)\n",
        "\n",
        "        # Evaluation metrics\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "        precision = precision_score(y_test, y_pred)\n",
        "        recall = recall_score(y_test, y_pred)\n",
        "        f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "        # AUC-ROC\n",
        "        if hasattr(clf, \"predict_proba\"):\n",
        "            y_prob = clf.predict_proba(X_test)[:,1]\n",
        "        else:\n",
        "            y_prob = clf.decision_function(X_test)\n",
        "        fpr, tpr, thresholds = roc_curve(y_test, y_prob)\n",
        "        roc_auc = auc(fpr, tpr)\n",
        "\n",
        "        # Confusion matrix\n",
        "        cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "        # Store results\n",
        "        results[name] = {\n",
        "            \"Accuracy\": accuracy,\n",
        "            \"Precision\": precision,\n",
        "            \"Recall\": recall,\n",
        "            \"F1 Score\": f1,\n",
        "            \"ROC AUC\": roc_auc,\n",
        "            \"Confusion Matrix\": cm\n",
        "        }\n",
        "\n",
        "        # Plot AUC-ROC curve\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        plt.plot(fpr, tpr, label=f'{name} (AUC = {roc_auc:.2f})')\n",
        "        plt.plot([0, 1], [0, 1], 'k--')\n",
        "        plt.xlabel('False Positive Rate')\n",
        "        plt.ylabel('True Positive Rate')\n",
        "        plt.title(f'{name} - AUC-ROC Curve')\n",
        "        plt.legend(loc='lower right')\n",
        "        plt.savefig(f'{name}_auc_roc_curve.png', dpi=300)\n",
        "        plt.close()\n",
        "\n",
        "        # Plot confusion matrix\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "        plt.xlabel('Predicted')\n",
        "        plt.ylabel('True')\n",
        "        plt.title(f'{name} - Confusion Matrix')\n",
        "        plt.savefig(f'{name}_confusion_matrix.png', dpi=300)\n",
        "        plt.close()\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "SULk39gP2SUj"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = df.iloc[:, :-1]\n",
        "y = df.iloc[:, -1]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)"
      ],
      "metadata": {
        "id": "2aBR2GZH2bb_"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = evaluate_models(X_train, X_test, y_train, y_test)"
      ],
      "metadata": {
        "id": "adNKyUsSLi6r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c4429d4-5f7f-4f74-be5a-406b10b418b4"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 168, number of negative: 1029\n",
            "[LightGBM] [Info] Total Bins 12562\n",
            "[LightGBM] [Info] Number of data points in the train set: 1197, number of used features: 81\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.140351 -> initscore=-1.812379\n",
            "[LightGBM] [Info] Start training from score -1.812379\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AAAzoxMlLpjk",
        "outputId": "8957f9d7-1ce7-4e6e-87f2-5f340e2f1fe9"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Logistic Regression': {'Accuracy': 0.87, 'Precision': 0.5714285714285714, 'Recall': 0.1951219512195122, 'F1 Score': 0.29090909090909095, 'ROC AUC': 0.6914492890102647, 'Confusion Matrix': array([[253,   6],\n",
            "       [ 33,   8]])}, 'SVM': {'Accuracy': 0.8633333333333333, 'Precision': 0.0, 'Recall': 0.0, 'F1 Score': 0.0, 'ROC AUC': 0.6644693473961766, 'Confusion Matrix': array([[259,   0],\n",
            "       [ 41,   0]])}, 'Decision Tree': {'Accuracy': 0.8466666666666667, 'Precision': 0.43902439024390244, 'Recall': 0.43902439024390244, 'F1 Score': 0.43902439024390244, 'ROC AUC': 0.6751106507204068, 'Confusion Matrix': array([[236,  23],\n",
            "       [ 23,  18]])}, 'Gradient Boosting': {'Accuracy': 0.8766666666666667, 'Precision': 0.625, 'Recall': 0.24390243902439024, 'F1 Score': 0.3508771929824561, 'ROC AUC': 0.7886806667294473, 'Confusion Matrix': array([[253,   6],\n",
            "       [ 31,  10]])}, 'LightGBM': {'Accuracy': 0.8633333333333333, 'Precision': 0.5, 'Recall': 0.17073170731707318, 'F1 Score': 0.2545454545454546, 'ROC AUC': 0.8026179489594123, 'Confusion Matrix': array([[252,   7],\n",
            "       [ 34,   7]])}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RRP Over-sampling"
      ],
      "metadata": {
        "id": "mvmiWNYLXdDJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cluster(candidates, enough, res):\n",
        "    if len(candidates) < enough:\n",
        "        res.append(candidates)\n",
        "        return res\n",
        "\n",
        "    east, west, east_items, west_items = split(candidates)\n",
        "    res = cluster(east_items, enough, res)\n",
        "    res = cluster(west_items, enough, res)\n",
        "\n",
        "    return res\n",
        "\n",
        "\n",
        "def split(candidates):\n",
        "    pivot = random.choice(candidates)\n",
        "    east_pivot = find_farest(pivot, candidates)\n",
        "    west_pivot = find_farest(east_pivot, candidates)\n",
        "    c = cal_distance(east_pivot, west_pivot)\n",
        "\n",
        "    if c == 0:\n",
        "        east_items = candidates[:len(candidates)//2]\n",
        "        west_items = candidates[len(candidates)//2:]\n",
        "        return east_pivot, west_pivot, east_items, west_items\n",
        "\n",
        "    all_distance = []\n",
        "    for candidate in candidates:\n",
        "        a = cal_distance(candidate, west_pivot)\n",
        "        b = cal_distance(candidate, east_pivot)\n",
        "        d = (a ** 2 + c ** 2 - b ** 2) / (2 * c)\n",
        "        all_distance.append((d, candidate))\n",
        "\n",
        "    all_distance.sort(key=lambda x: x[0])\n",
        "    sorted_candidates = [item[1] for item in all_distance]\n",
        "    east_items = sorted_candidates[:len(sorted_candidates)//2]\n",
        "    west_items = sorted_candidates[len(sorted_candidates)//2:]\n",
        "\n",
        "    return east_pivot, west_pivot, east_items, west_items\n",
        "\n",
        "\n",
        "def find_farest(pivot, candidates):\n",
        "    max_d = 0\n",
        "    most_point = pivot\n",
        "\n",
        "    for candidate in candidates:\n",
        "        cur_d = cal_distance(pivot, candidate)\n",
        "        if  cur_d > max_d:\n",
        "            max_d = cur_d\n",
        "            most_point = candidate\n",
        "\n",
        "    return most_point\n",
        "\n",
        "\n",
        "def cal_distance(p1, p2):\n",
        "    return math.sqrt(sum([(v1 - v2) ** 2 for v1, v2 in zip(p1[:-1], p2[:-1])]))\n",
        "\n",
        "\n",
        "def process_mixed_cluster(cluster):\n",
        "    \"\"\"\n",
        "    in DE operation, use current-to-best to mutate the candidates\n",
        "    v_i = x_i + F * (x_b - x_i) + F_min * (x_r1 - x_r2)\n",
        "    \"\"\"\n",
        "    DE_params = {\"F\": 0.8, \"Fmin\": 0.1, \"CR\": 1.0}\n",
        "    pos_point = [(idx, item) for idx, item in enumerate(cluster) if item[-1] == 1]\n",
        "    neg_point = [(idx, item) for idx, item in enumerate(cluster) if item[-1] == 0]\n",
        "\n",
        "    candidate_l = []\n",
        "    if len(pos_point) == 1:\n",
        "        # only 1 pos point in cluster, then mutate all neg points toward to the pos point\n",
        "        xb = pos_point[0][1]\n",
        "        R = random.choice(range(len(xb)-1))\n",
        "\n",
        "        for _, xi in neg_point:\n",
        "            new_candidate = []\n",
        "            for i in range(len(xi)-1):\n",
        "                ri = np.random.uniform(low=0.0, high=1.0, size=1)[0]\n",
        "\n",
        "                if ri < DE_params[\"CR\"] or i == R:\n",
        "                    new_candidate.append(xi[i] + DE_params[\"F\"] * (xb[i] - xi[i]))\n",
        "                else:\n",
        "                    new_candidate.append(xi[i])\n",
        "\n",
        "            new_candidate.append(1)\n",
        "            candidate_l.append(np.array(new_candidate))\n",
        "    else:\n",
        "        # more than 2 pos points in cluster, then randomly pick 3 points, first 1 is current point, and\n",
        "        # another two are support points\n",
        "        for idx1, xb in pos_point:\n",
        "            R = random.choice(range(len(xb)-1))\n",
        "            for idx2, xi in enumerate(cluster):\n",
        "                if not idx1 == idx2:\n",
        "                    available_points = []\n",
        "                    for idx3, p in enumerate(cluster):\n",
        "                        if not idx3 == idx2 and not idx3 == idx1:\n",
        "                            available_points.append(p)\n",
        "                    [xr1, xr2] = random.sample(available_points, 2)\n",
        "\n",
        "                    new_candidate = []\n",
        "                    # if xr1 and xr2 all negative class, then just use xi\n",
        "                    if xr1[-1] == 0 and xr2[-1] == 0:\n",
        "                        for i in range(len(xi)-1):\n",
        "                            ri = np.random.uniform(low=0.0, high=1.0, size=1)[0]\n",
        "\n",
        "                            if ri < DE_params[\"CR\"] or i == R:\n",
        "                                new_candidate.append(xi[i] + DE_params[\"F\"] * (xb[i] - xi[i]))\n",
        "                            else:\n",
        "                                new_candidate.append(xi[i])\n",
        "                    else:\n",
        "                        for i in range(len(xi)-1):\n",
        "                            ri = np.random.uniform(low=0.0, high=1.0, size=1)[0]\n",
        "\n",
        "                            if ri < DE_params[\"CR\"] or i == R:\n",
        "                                if xr1[-1] == 1:\n",
        "                                    new_candidate.append(xi[i] + DE_params[\"F\"] * (xb[i] - xi[i]) + DE_params[\"Fmin\"] * (xr1[i] - xr2[i]))\n",
        "                                else:\n",
        "                                    new_candidate.append(xi[i] + DE_params[\"F\"] * (xb[i] - xi[i]) + DE_params[\"Fmin\"] * (xr2[i] - xr1[i]))\n",
        "                            else:\n",
        "                                new_candidate.append(xi[i])\n",
        "\n",
        "                    new_candidate.append(1)\n",
        "                    candidate_l.append(np.array(new_candidate))\n",
        "\n",
        "    return candidate_l\n",
        "\n",
        "\n",
        "def process_positive_cluster(cluster):\n",
        "    \"\"\"\n",
        "    in DE operation, use best to mutate the candidates\n",
        "    v_i = x_b + F * (x_r1 - x_r2)\n",
        "    \"\"\"\n",
        "    DE_params = {\"F\": 0.8, \"CR\": 1.0}\n",
        "    pos_point = [item for item in cluster if item[-1] == 1]\n",
        "    candidate_l = []\n",
        "\n",
        "    for idx1 in range(len(pos_point)-2):\n",
        "        for idx2 in range(idx1+1, len(pos_point)-1):\n",
        "            for idx3 in range(idx2+1, len(pos_point)):\n",
        "                [xb, xr1, xr2] = random.sample([idx1, idx2, idx3], 3)\n",
        "                xb, xr1, xr2 = pos_point[xb], pos_point[xr1], pos_point[xr2]\n",
        "\n",
        "                R = random.choice(range(len(xb)-1))\n",
        "                new_candidate = []\n",
        "\n",
        "                for i in range(len(xb)-1):\n",
        "                    ri = np.random.uniform(low=0.0, high=1.0, size=1)[0]\n",
        "\n",
        "                    if ri < DE_params[\"CR\"] or i == R:\n",
        "                        new_candidate.append(xb[i] + DE_params[\"F\"] * (xr1[i] - xr2[i]))\n",
        "                    else:\n",
        "                        new_candidate.append(xb[i])\n",
        "\n",
        "                new_candidate.append(1)\n",
        "                candidate_l.append(np.array(new_candidate))\n",
        "\n",
        "    return candidate_l\n",
        "\n",
        "\n",
        "def process_mixed_cluster_extra(cluster):\n",
        "    \"\"\"\n",
        "    in DE operation, use current-to-best-extra to mutate the candidates\n",
        "    v_i = x_b + F * (x_r1 - x_r2) + F_ex * (x_r3 - x_r4)\n",
        "    \"\"\"\n",
        "    DE_params = {\"F\": 0.8, \"CR\": 1.0, \"F_xc\": 0.1}\n",
        "    pos_point = [item for item in cluster if item[-1] == 1]\n",
        "\n",
        "    candidate_l = []\n",
        "    for xb in pos_point:\n",
        "        R = random.choice(range(len(xb)-1))\n",
        "\n",
        "        for xi in cluster:\n",
        "            if not np.array_equal(xb, xi):\n",
        "                available_points = []\n",
        "                for p in cluster:\n",
        "                    if not np.array_equal(p, xi) and not np.array_equal(p, xb):\n",
        "                        available_points.append(p)\n",
        "\n",
        "                for _ in range(20):\n",
        "                    [xr1, xr2, xr3, xr4] = random.sample(available_points, 4)\n",
        "\n",
        "                    new_candidate = []\n",
        "\n",
        "                    for i in range(len(xi)-1):\n",
        "                        ri = np.random.uniform(low=0.0, high=1.0, size=1)[0]\n",
        "\n",
        "                        if ri < DE_params[\"CR\"] or i == R:\n",
        "                            new_candidate.append(xi[i] + DE_params[\"F\"] * (xb[i] - xi[i]) + DE_params[\"F_xc\"] * (xr1[i] - xr2[i]) + DE_params[\"F_xc\"] * (xr3[i] - xr4[i]))\n",
        "                        else:\n",
        "                            new_candidate.append(xi[i])\n",
        "\n",
        "                    new_candidate.append(1)\n",
        "                    candidate_l.append(new_candidate)\n",
        "\n",
        "    return candidate_l\n",
        "\n",
        "\n",
        "def RandomProjectionOversampling(X_train, y_train, threshold):\n",
        "    train_df = pd.concat([X_train, y_train], axis=1)\n",
        "    train_df.reset_index(inplace=True, drop=True)\n",
        "    tar = y_train.name\n",
        "    X_train[tar] = y_train\n",
        "    X_train.reset_index(inplace=True, drop=True)\n",
        "    col_names = X_train.columns\n",
        "\n",
        "    n_data_to_generate = X_train[tar].value_counts()[0] - X_train[tar].value_counts()[1]\n",
        "    X_train = X_train.to_numpy()\n",
        "    start_time = time.time()\n",
        "    res = cluster(X_train, threshold, [])\n",
        "\n",
        "    new_data_negative_cluster = []\n",
        "    new_data_positive_cluster = []\n",
        "    for c in res:\n",
        "        if sum([item[-1] for item in c]) > len(c)//2:\n",
        "            cur_new_data = process_positive_cluster(c)\n",
        "            new_data_positive_cluster += cur_new_data\n",
        "        else:\n",
        "            cur_new_data = process_mixed_cluster(c)\n",
        "            new_data_negative_cluster += cur_new_data\n",
        "\n",
        "    rt = time.time() - start_time\n",
        "    if len(new_data_negative_cluster) >= n_data_to_generate - len(new_data_positive_cluster):\n",
        "        new_data = new_data_positive_cluster + random.sample(new_data_negative_cluster,\n",
        "                                                            n_data_to_generate - len(new_data_positive_cluster))\n",
        "    else:\n",
        "        extra_data = []\n",
        "        for c in res:\n",
        "            cur_extra_data = process_mixed_cluster_extra(c)\n",
        "            extra_data += cur_extra_data\n",
        "\n",
        "        rest_data_to_generate = n_data_to_generate - len(new_data_positive_cluster) - len(new_data_negative_cluster)\n",
        "        new_data = new_data_negative_cluster + new_data_positive_cluster + random.sample(extra_data, rest_data_to_generate)\n",
        "\n",
        "    new_data_df = pd.DataFrame(np.array(new_data), columns=col_names)\n",
        "\n",
        "    return rt, new_data_df, train_df"
      ],
      "metadata": {
        "id": "rAirL-jFXbVO"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SDV - Oversampling"
      ],
      "metadata": {
        "id": "_NaC7Ymj90QV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def do_sdv(X_train, y_train):\n",
        "  train_df = pd.concat([X_train, y_train], axis=1)\n",
        "  class_counts = y_train.value_counts()\n",
        "\n",
        "  # Find minority class label\n",
        "  minority_class_label = class_counts.idxmin()\n",
        "\n",
        "  # Filter rows with minority class label\n",
        "  minority_df = train_df[train_df.iloc[:, -1] == minority_class_label]\n",
        "\n",
        "  # Calculate counts of majority and minority classes\n",
        "  majority_count = class_counts.max()\n",
        "  minority_count = class_counts.min()\n",
        "\n",
        "  metadata_data = SingleTableMetadata()\n",
        "  metadata_data.detect_from_dataframe(minority_df)\n",
        "  # Generate synthetic data using GaussianCopulaSynthesizer\n",
        "  synthesizer_breast_data = GaussianCopulaSynthesizer(metadata_data)\n",
        "  synthesizer_breast_data.fit(minority_df)\n",
        "\n",
        "  # Print sample synthetic data\n",
        "  synthesizer_breast_data.reset_sampling()\n",
        "  sd1 = synthesizer_breast_data.sample(num_rows=majority_count-minority_count)\n",
        "  return sd1, train_df\n",
        "\n",
        "# Function to add synthetic data to the main DataFrame based on percentage\n",
        "def add_synthetic_data(main_df, synthetic_df, percentage, seed=42):\n",
        "    # Calculate number of rows to sample\n",
        "    num_rows = int(len(synthetic_df) * percentage)\n",
        "    np.random.seed(seed)\n",
        "    # Sample the specified percentage of synthetic data\n",
        "    sampled_synthetic_data = synthetic_df.sample(n=num_rows, replace=False, random_state=seed)\n",
        "    # print(sampled_synthetic_data)\n",
        "\n",
        "    # Concatenate sampled synthetic data with main DataFrame\n",
        "    combined_df = pd.concat([main_df, sampled_synthetic_data], ignore_index=True)\n",
        "    # print(combined_df)\n",
        "    return combined_df"
      ],
      "metadata": {
        "id": "o9yiYbcN93gn"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Random Over-Sampling"
      ],
      "metadata": {
        "id": "A3BmbGRgJ5Ns"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_minority_data(X, y):\n",
        "    labels, counts = np.unique(y, return_counts=True)\n",
        "    min_label = min(zip(counts, labels))[1]\n",
        "    indices_with_min_label = np.where(y == min_label)[0]\n",
        "    X_min, y_min = X[indices_with_min_label], y[indices_with_min_label]\n",
        "\n",
        "    # Other class samples\n",
        "    indices_without_min_label = np.where(y != min_label)[0]\n",
        "    X_remaining, y_remaining = X[indices_without_min_label], y[indices_without_min_label]\n",
        "\n",
        "    return X_min, y_min, X_remaining, y_remaining, min_label\n",
        "\n",
        "def random_oversampling(X_train, y_train, oversampling_ratios, seed=42):\n",
        "\n",
        "  oversampled_X_train_ratios = dict()\n",
        "  oversampled_y_train_ratios = dict()\n",
        "  X_minority, y_minority, X_remaining, y_remaining, min_label = find_minority_data(X_train, y_train)\n",
        "  ideal_samps = len(X_remaining) - len(X_minority)\n",
        "\n",
        "  oversampling_samps = [int(ideal_samps * (oversampling_ratio)) for oversampling_ratio in oversampling_ratios]\n",
        "  for oversampling_samp, oversampling_ratio in zip(oversampling_samps, oversampling_ratios):\n",
        "\n",
        "    sampling_strategy = {min_label: len(X_minority) + oversampling_samp}\n",
        "    X_train_upsampled, y_train_upsampled = RandomOverSampler(sampling_strategy=sampling_strategy, random_state = seed).fit_resample(X_train, y_train)\n",
        "\n",
        "    oversampled_X_train_ratios[oversampling_ratio] = X_train_upsampled\n",
        "    oversampled_y_train_ratios[oversampling_ratio] = y_train_upsampled\n",
        "\n",
        "  return list(oversampled_X_train_ratios.values()), list(oversampled_y_train_ratios.values())\n"
      ],
      "metadata": {
        "id": "bF2pvEGDJ704"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SMOTE Over-Sampling"
      ],
      "metadata": {
        "id": "TGGkRAzG15Zy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_minority_data(X, y):\n",
        "    labels, counts = np.unique(y, return_counts=True)\n",
        "    min_label = min(zip(counts, labels))[1]\n",
        "    indices_with_min_label = np.where(y == min_label)[0]\n",
        "    X_min, y_min = X[indices_with_min_label], y[indices_with_min_label]\n",
        "\n",
        "    # Other class samples\n",
        "    indices_without_min_label = np.where(y != min_label)[0]\n",
        "    X_remaining, y_remaining = X[indices_without_min_label], y[indices_without_min_label]\n",
        "\n",
        "    return X_min, y_min, X_remaining, y_remaining, min_label\n",
        "\n",
        "def smote_oversampling(X_train, y_train, oversampling_ratios, seed=42):\n",
        "\n",
        "  oversampled_X_train_ratios = dict()\n",
        "  oversampled_y_train_ratios = dict()\n",
        "  X_minority, y_minority, X_remaining, y_remaining, min_label = find_minority_data(X_train, y_train)\n",
        "  ideal_samps = len(X_remaining) - len(X_minority)\n",
        "\n",
        "  oversampling_samps = [int(ideal_samps * (oversampling_ratio)) for oversampling_ratio in oversampling_ratios]\n",
        "  for oversampling_samp, oversampling_ratio in zip(oversampling_samps, oversampling_ratios):\n",
        "\n",
        "    sampling_strategy = {min_label: len(X_minority) + oversampling_samp}\n",
        "    X_train_upsampled, y_train_upsampled = SMOTE(sampling_strategy=sampling_strategy, random_state = seed).fit_resample(X_train, y_train)\n",
        "\n",
        "    oversampled_X_train_ratios[oversampling_ratio] = X_train_upsampled\n",
        "    oversampled_y_train_ratios[oversampling_ratio] = y_train_upsampled\n",
        "\n",
        "  return list(oversampled_X_train_ratios.values()), list(oversampled_y_train_ratios.values())\n"
      ],
      "metadata": {
        "id": "cF6biXPn125o"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SVM-SMOTE Over-Sampling"
      ],
      "metadata": {
        "id": "QutsbBwdMujq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_minority_data(X, y):\n",
        "    labels, counts = np.unique(y, return_counts=True)\n",
        "    min_label = min(zip(counts, labels))[1]\n",
        "    indices_with_min_label = np.where(y == min_label)[0]\n",
        "    X_min, y_min = X[indices_with_min_label], y[indices_with_min_label]\n",
        "\n",
        "    # Other class samples\n",
        "    indices_without_min_label = np.where(y != min_label)[0]\n",
        "    X_remaining, y_remaining = X[indices_without_min_label], y[indices_without_min_label]\n",
        "\n",
        "    return X_min, y_min, X_remaining, y_remaining, min_label\n",
        "\n",
        "def svm_smote_oversampling(X_train, y_train, oversampling_ratios, seed=42):\n",
        "\n",
        "  oversampled_X_train_ratios = dict()\n",
        "  oversampled_y_train_ratios = dict()\n",
        "  X_minority, y_minority, X_remaining, y_remaining, min_label = find_minority_data(X_train, y_train)\n",
        "  ideal_samps = len(X_remaining) - len(X_minority)\n",
        "\n",
        "  oversampling_samps = [int(ideal_samps * (oversampling_ratio)) for oversampling_ratio in oversampling_ratios]\n",
        "  for oversampling_samp, oversampling_ratio in zip(oversampling_samps, oversampling_ratios):\n",
        "\n",
        "    sampling_strategy = {min_label: len(X_minority) + oversampling_samp}\n",
        "    X_train_upsampled, y_train_upsampled = SVMSMOTE(sampling_strategy=sampling_strategy, random_state = seed).fit_resample(X_train, y_train)\n",
        "\n",
        "    oversampled_X_train_ratios[oversampling_ratio] = X_train_upsampled\n",
        "    oversampled_y_train_ratios[oversampling_ratio] = y_train_upsampled\n",
        "\n",
        "  return list(oversampled_X_train_ratios.values()), list(oversampled_y_train_ratios.values())\n"
      ],
      "metadata": {
        "id": "XwF74E6XMzG4"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Intelligent Pruning"
      ],
      "metadata": {
        "id": "x7qohyP7L0DZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_majority_data(X, y):\n",
        "    labels, counts = np.unique(y, return_counts=True)\n",
        "    max_label = max(zip(counts, labels))[1]\n",
        "    indices_with_max_label = np.where(y == max_label)[0]\n",
        "    X_maj, y_maj = X[indices_with_max_label], y[indices_with_max_label]\n",
        "\n",
        "    # Exclude majority class samples\n",
        "    indices_without_max_label = np.where(y != max_label)[0]\n",
        "    X_remaining, y_remaining = X[indices_without_max_label], y[indices_without_max_label]\n",
        "\n",
        "    return X_maj, y_maj, X_remaining, y_remaining, min(counts)\n",
        "\n",
        "def do_clustering(X, y, labels):\n",
        "  clustered_X = defaultdict(list)\n",
        "  clustered_y = defaultdict(list)\n",
        "\n",
        "  for i, label in enumerate(labels):\n",
        "      clustered_X[label].append(X[i])\n",
        "      clustered_y[label].append(y[i])\n",
        "\n",
        "  # Sort clustered_X and clustered_y in descending order based on the length of values in each dictionary\n",
        "  sorted_clustered_X = dict(sorted(clustered_X.items(), key=lambda x: -len(x[1])))\n",
        "  sorted_clustered_y = dict(sorted(clustered_y.items(), key=lambda x: -len(x[1])))\n",
        "\n",
        "  return sorted_clustered_X, sorted_clustered_y\n",
        "\n",
        "\n",
        "def intelligent_prune_data(pruning_samps, pruning_ratios, clustered_X, clustered_y, per_cluster_pruning_ratio=0.7, seed=42):\n",
        "  random.seed(seed)\n",
        "  pruning_ratios_X_maj, pruning_ratios_y_maj = defaultdict(list), defaultdict(list)\n",
        "  for pruning_samp, pruning_ratio in zip(pruning_samps, pruning_ratios):\n",
        "    samps = 0\n",
        "    # print(\"For Pruning samps: \", pruning_samp)\n",
        "    prune_samps = pruning_samp\n",
        "    # print(prune_samps)\n",
        "    clustered_X_new = defaultdict(list)\n",
        "    clustered_y_new = defaultdict(list)\n",
        "    # Iterate over the sorted dictionaries\n",
        "    for label, values_X in clustered_X.items():\n",
        "        # Calculate the number of samples to prune\n",
        "        num_samples_to_prune = int(prune_samps * per_cluster_pruning_ratio)\n",
        "        if(num_samples_to_prune > len(values_X)):\n",
        "          num_samples_to_prune = len(values_X)//2\n",
        "          prune_samps -= num_samples_to_prune\n",
        "        else:\n",
        "          prune_samps -= num_samples_to_prune\n",
        "\n",
        "        # Randomly choose samples to prune\n",
        "        indices_to_prune = random.sample(range(len(values_X)), num_samples_to_prune)\n",
        "\n",
        "        # Prune the samples from clustered_X and clustered_y\n",
        "        clustered_X_new[label] = [values_X[i] for i in range(len(values_X)) if i not in indices_to_prune]\n",
        "        clustered_y_new[label] = [clustered_y[label][i] for i in range(len(clustered_y[label])) if i not in indices_to_prune]\n",
        "\n",
        "    iter = 0\n",
        "    while(prune_samps > 0):\n",
        "        if(iter>=100):\n",
        "          break\n",
        "        for label, values_X in clustered_X_new.items():\n",
        "          if(prune_samps <=0 or len(values_X) <= 0):\n",
        "            break\n",
        "          # print(len(values_X))\n",
        "          index_to_prune = random.sample(range(len(values_X)), 1)\n",
        "          clustered_X_new[label] = [values_X[i] for i in range(len(values_X)) if i not in index_to_prune]\n",
        "          clustered_y_new[label] = [clustered_y_new[label][i] for i in range(len(clustered_y_new[label])) if i not in index_to_prune]\n",
        "\n",
        "          prune_samps -= 1\n",
        "        iter += 1\n",
        "\n",
        "    for label in clustered_X_new:\n",
        "        pruning_ratios_X_maj[pruning_ratio].extend(clustered_X_new[label])\n",
        "        pruning_ratios_y_maj[pruning_ratio].extend(clustered_y_new[label])\n",
        "\n",
        "  return pruning_ratios_X_maj, pruning_ratios_y_maj\n",
        "\n",
        "def combine_data(pruning_ratios, pruning_ratios_X_maj, pruning_ratios_y_maj, X_remaining, y_remaining):\n",
        "\n",
        "  pruning_ratios_X, pruning_ratios_y = defaultdict(list), defaultdict(list)\n",
        "  for pruning_ratio in pruning_ratios:\n",
        "    pruning_ratios_X[pruning_ratio].extend(pruning_ratios_X_maj[pruning_ratio])\n",
        "    pruning_ratios_X[pruning_ratio].extend(X_remaining)\n",
        "\n",
        "    pruning_ratios_y[pruning_ratio].extend(pruning_ratios_y_maj[pruning_ratio])\n",
        "    pruning_ratios_y[pruning_ratio].extend(y_remaining)\n",
        "\n",
        "  return pruning_ratios_X, pruning_ratios_y\n",
        "\n",
        "def do_intelligent_pruning(X, y, ratio, per_cluster_pruning_ratio=0.7, seed=42):\n",
        "\n",
        "  X_maj, y_maj, X_remaining, y_remaining, min_class_samples = find_majority_data(X, y)\n",
        "  kmeans = KMeans(n_clusters=3, random_state = 42)\n",
        "  kmeans.fit(X_maj)\n",
        "  labels = kmeans.labels_\n",
        "  clustered_X, clustered_y = do_clustering(X_maj, y_maj, labels)\n",
        "\n",
        "  pruning_best = len(X_maj)-min_class_samples\n",
        "  pruning_samps = [int(pruning_best * ratio)]\n",
        "  pruning_ratios = [ratio]\n",
        "\n",
        "  pruning_ratios_X_maj, pruning_ratios_y_maj = intelligent_prune_data(pruning_samps, pruning_ratios, clustered_X, clustered_y, \\\n",
        "                                                                      per_cluster_pruning_ratio=per_cluster_pruning_ratio, seed=seed)\n",
        "\n",
        "  pruning_ratios_X, pruning_ratios_y = combine_data(pruning_ratios, pruning_ratios_X_maj, pruning_ratios_y_maj, X_remaining, y_remaining)\n",
        "\n",
        "  return list(pruning_ratios_X.values()), list(pruning_ratios_y.values())"
      ],
      "metadata": {
        "id": "w9_Pj009Lxp9"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Random Pruning"
      ],
      "metadata": {
        "id": "yky--qnU6rg6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "inputs:\n",
        "X: np.array\n",
        "y: np.array\n",
        "percentage: from 0.0 upto 1.0, enter int value\n",
        "\"\"\"\n",
        "def random_prune_data(X, y, ratio, seed = 42):\n",
        "  # preprocessed_X, scaler, imputer = preprocess_data_train(X)\n",
        "  # preprocessed_X_test = preprocess_data_test(X_test, scaler, imputer)\n",
        "\n",
        "  # X_train, y_train = preprocessed_X_train.to_numpy(), y_train.to_numpy()\n",
        "  # X_test, y_test = preprocessed_X_test.to_numpy(), y_test.to_numpy()\n",
        "  np.random.seed(seed)\n",
        "  labels_count = {}\n",
        "  labels = np.unique(y)\n",
        "  for label in labels:\n",
        "    labels_count[label] = np.count_nonzero(y == label)\n",
        "  max_label = min_label = labels[0]\n",
        "  for label in labels_count:\n",
        "    if labels_count[label] > labels_count[max_label]:\n",
        "      max_label = label\n",
        "    if labels_count[label] < labels_count[min_label]:\n",
        "      min_label = label\n",
        "\n",
        "  # print(\"Max\", max_label, labels_count[max_label])\n",
        "  # print(\"Min\", min_label, labels_count[min_label])\n",
        "\n",
        "  prune_counts = {}\n",
        "  prune_indexes = {}\n",
        "  for label in labels_count:\n",
        "    prune_counts[label] = labels_count[label] - labels_count[min_label]\n",
        "    prune_indexes[label] = np.where(y == label)[0]\n",
        "\n",
        "  prune_amount = int(ratio * sum(map(lambda x: x[1], prune_counts.items())))\n",
        "  prune_it = {}\n",
        "\n",
        "  while prune_amount > 0:\n",
        "    for label in labels:\n",
        "      if (len(prune_indexes[label]) - labels_count[min_label]) > 0 and prune_amount > 0:\n",
        "        random_index = np.random.choice(len(prune_indexes[label]))\n",
        "        random_item = prune_indexes[label][random_index]\n",
        "        prune_indexes[label] = np.delete(prune_indexes[label], random_index)\n",
        "        if prune_it.get(label, None) is None:\n",
        "          prune_it[label] = np.array([])\n",
        "        prune_it[label] = np.append(prune_it[label], [random_item])\n",
        "        prune_amount -= 1\n",
        "\n",
        "\n",
        "\n",
        "  formatted_indexes = np.array([])\n",
        "  for label in prune_indexes:\n",
        "    formatted_indexes = np.append(formatted_indexes, prune_indexes[label])\n",
        "  formatted_indexes = np.sort(formatted_indexes)\n",
        "  new_arr = np.array([np.int64(i) for i in formatted_indexes])\n",
        "\n",
        "  return X[new_arr], y[new_arr]"
      ],
      "metadata": {
        "id": "BtB6jqhZtRD0"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_smo(X, y, smo_K, random_state, budget=10):\n",
        "\n",
        "    # Get indices for each class\n",
        "    np.random.seed(random_state)\n",
        "    class_indices = [np.where(y == i)[0] for i in np.unique(y)]\n",
        "    # Shuffle class_indices\n",
        "    for indices in class_indices:\n",
        "        np.random.shuffle(indices)\n",
        "\n",
        "    # Select K samples for each class\n",
        "    selected_indices = [indices[:smo_K] for indices in class_indices]\n",
        "\n",
        "    # Flatten the list of indices\n",
        "    selected_indices_flat = np.concatenate(selected_indices)\n",
        "\n",
        "    # Get the selected data and target values\n",
        "    selected_data = X.loc[selected_indices_flat]\n",
        "    selected_target = y.loc[selected_indices_flat]\n",
        "\n",
        "    selected_data_df = pd.DataFrame(selected_data, columns=X.columns)\n",
        "    selected_target_df = pd.Series(selected_target, name=y.name)\n",
        "\n",
        "    # Store the remaining samples as well\n",
        "    remaining_indices_flat = np.setdiff1d(np.arange(len(X)), selected_indices_flat)\n",
        "    remaining_data = X.loc[remaining_indices_flat]\n",
        "    remaining_target =  y.loc[remaining_indices_flat]\n",
        "\n",
        "    clf = GaussianNB()\n",
        "    clf.fit(selected_data, selected_target)\n",
        "\n",
        "    for i in range(budget):\n",
        "        out, thresh, out_label = 0, 1E-30, 0\n",
        "        for index, row in remaining_data.iterrows():\n",
        "            target_value = remaining_target.loc[index]\n",
        "            row_2d = row.values.reshape(1, -1)\n",
        "            p1, p2 = clf.predict_proba(row_2d)[0][0], clf.predict_proba(row_2d)[0][1]\n",
        "            score = abs(p1+p2)/(abs(p1-p2)+1E-300)\n",
        "            if(score > thresh):\n",
        "                out = index\n",
        "                thresh = score\n",
        "                if(p1<p2):\n",
        "                    out_label = 1\n",
        "                else:\n",
        "                    out_label = 0\n",
        "\n",
        "        X_to_move = (remaining_data.loc[out]).values.reshape(1, -1)\n",
        "        y_to_move = [out_label]\n",
        "\n",
        "        # Append the row to remaining_data\n",
        "        X_to_move_df = pd.DataFrame(X_to_move, columns=remaining_data.columns)\n",
        "        selected_data_df = pd.concat([selected_data_df, X_to_move_df], ignore_index=True)\n",
        "\n",
        "        # Append the value to remaining_target\n",
        "        y_to_move_series = pd.Series(y_to_move, name=remaining_target.name)\n",
        "        selected_target_df = pd.concat([selected_target_df, y_to_move_series], ignore_index=True)\n",
        "\n",
        "        clf.partial_fit(X_to_move, y_to_move)\n",
        "\n",
        "        remaining_data = remaining_data.drop(out).reset_index(drop=True)\n",
        "        remaining_target = remaining_target.drop(out).reset_index(drop=True)\n",
        "\n",
        "\n",
        "    remaining_data_df = pd.DataFrame(remaining_data, columns=X.columns)\n",
        "\n",
        "    remaining_target_df = pd.Series(clf.predict(remaining_data_df), name=remaining_target.name)\n",
        "    matches = (remaining_target_df == remaining_target).sum()\n",
        "    X_new = pd.concat([selected_data_df, remaining_data_df])\n",
        "    y_new = pd.concat([selected_target_df, remaining_target_df])\n",
        "\n",
        "    return X_new, y_new, matches, len(remaining_target_df)"
      ],
      "metadata": {
        "id": "Xg4RT5ec-zDn"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ratios = [ratio for ratio in np.arange(0, 1.1, 0.2)]"
      ],
      "metadata": {
        "id": "z0A-h4iX3J8p"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learners = {'Logistic Regression' : 'LR', 'SVM': 'SVM', 'Decision Tree': 'DT', 'Gradient Boosting': 'GB', 'LightGBM': 'lightGBM'}\n",
        "\n",
        "X = df.iloc[:, :-1]\n",
        "y = df.iloc[:, -1]\n",
        "\n",
        "states = [82, 15, 4, 95, 36, 32, 29, 18, 14, 87]\n",
        "ratios = [0.2, 0.4, 0.6, 0.8, 1.0]\n",
        "smo_K = 5\n",
        "smo = False"
      ],
      "metadata": {
        "id": "Z1PE64zGed7p"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_data(X, y, random_state):\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = random_state)\n",
        "  return X_train, X_test, y_train, y_test"
      ],
      "metadata": {
        "id": "kVPCRpJXp1k0"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "smo_X_train, smo_y_train = dict(), dict()\n",
        "for rand_state in states:\n",
        "  X_train, X_test, y_train, y_test = split_data(X, y,rand_state)\n",
        "  X_train.reset_index(drop=True, inplace=True)\n",
        "  X_test.reset_index(drop=True, inplace=True)\n",
        "  y_train.reset_index(drop=True, inplace=True)\n",
        "  y_test.reset_index(drop=True, inplace=True)\n",
        "  X_train, y_train, matches, total = apply_smo(X_train, y_train, smo_K, rand_state)\n",
        "\n",
        "  smo_X_train[rand_state] = X_train\n",
        "  smo_y_train[rand_state] = y_train\n",
        "  print(str(matches) + \" Matches out of \" + str(total) + \" dark samples\")"
      ],
      "metadata": {
        "id": "Hgs_CT4f-JyG",
        "outputId": "54871f16-c103-45df-9aaa-c89cf813ebd3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "778 Matches out of 1177 dark samples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Calling Intelligent Pruning"
      ],
      "metadata": {
        "id": "UY6hAjLw9fy3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cols = ['algo', 'ratio', 'seed', 'learner', 'acc', 'prec', 'rec', 'f1', 'auc_roc']\n",
        "df = pd.DataFrame([], columns = cols)\n",
        "algorithm = 'intelligent_pruning'\n",
        "file_name = f'{algorithm}_results'\n",
        "per_cluster_pruning_ratios = [0.5, 0.7, 0.9, 1]\n",
        "\n",
        "for rand_state in states:\n",
        "  if(smo):\n",
        "\n",
        "    X_train, y_train = smo_X_train[rand_state], smo_y_train[rand_state]\n",
        "  for per_cluster_pruning_ratio in per_cluster_pruning_ratios:\n",
        "    for ratio in ratios:\n",
        "\n",
        "      X_train_copy, y_train_copy = X_train.copy(), y_train.copy()\n",
        "\n",
        "      intelligent_pruned_X_train, intelligent_pruned_y_train = do_intelligent_pruning(X_train_copy.to_numpy(), y_train_copy.to_numpy(), ratio, per_cluster_pruning_ratio=per_cluster_pruning_ratio)\n",
        "\n",
        "      preprocessed_intelligent_pruned_X_train, scaler, imputer = preprocess_data_train((np.array(intelligent_pruned_X_train))[0])\n",
        "      preprocessed_X_test = preprocess_data_test(X_test, scaler, imputer)\n",
        "\n",
        "      intelligent_pruned_X_train, intelligent_pruned_y_train = preprocessed_intelligent_pruned_X_train, (np.array(intelligent_pruned_y_train))[0]\n",
        "      intelligent_pruned_X_test, intelligent_pruned_y_test = preprocessed_X_test, y_test.to_numpy()\n",
        "      results = evaluate_models(intelligent_pruned_X_train, intelligent_pruned_X_test, intelligent_pruned_y_train, intelligent_pruned_y_test, rand_state)\n",
        "      for key, item in results.items():\n",
        "        row = {'algo' : [algorithm], 'ratio': [ratio], 'per_cluster_pruning_ratio': [per_cluster_pruning_ratio],'seed': [rand_state], 'learner': [learners[key]], 'acc': [item['Accuracy']], 'prec': [item['Precision']], 'rec': [item['Recall']], 'f1': [item['F1 Score']], 'auc_roc': [item['ROC AUC']]}\n",
        "        temp = pd.DataFrame(row)\n",
        "        df = pd.concat([df, temp], ignore_index=True)\n",
        "df.to_csv(file_name, index=False)\n"
      ],
      "metadata": {
        "id": "wRV_KrPy9is0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Calling Random Pruning"
      ],
      "metadata": {
        "id": "uGrS3yXb-RYZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cols = ['algo', 'ratio', 'seed', 'learner', 'acc', 'prec', 'rec', 'f1', 'auc_roc']\n",
        "df = pd.DataFrame([], columns = cols)\n",
        "algorithm = 'random_pruning'\n",
        "file_name = f'{algorithm}_results'\n",
        "\n",
        "for rand_state in states:\n",
        "  if(smo):\n",
        "\n",
        "    X_train, y_train = smo_X_train[rand_state], smo_y_train[rand_state]\n",
        "  for ratio in ratios:\n",
        "\n",
        "    random_pruned_X_train, random_pruned_y_train = random_prune_data(X_train.to_numpy(), y_train.to_numpy(), ratio, seed=rand_state)\n",
        "    preprocessed_random_pruned_X_train, scaler, imputer = preprocess_data_train(random_pruned_X_train)\n",
        "    preprocessed_X_test = preprocess_data_test(X_test, scaler, imputer)\n",
        "\n",
        "    random_pruned_X_train, random_pruned_y_train = preprocessed_random_pruned_X_train, random_pruned_y_train\n",
        "    random_pruned_X_test, random_pruned_y_test = preprocessed_X_test, y_test.to_numpy()\n",
        "\n",
        "    print(f\"Train data pruned randomly at {ratio * 100}%\")\n",
        "    results = evaluate_models(random_pruned_X_train, random_pruned_X_test, random_pruned_y_train, random_pruned_y_test, rand_state)\n",
        "    for key, item in results.items():\n",
        "      row = {'algo' : [algorithm], 'ratio': [ratio], 'seed': [rand_state], 'learner': [learners[key]], 'acc': [item['Accuracy']], 'prec': [item['Precision']], 'rec': [item['Recall']], 'f1': [item['F1 Score']], 'auc_roc': [item['ROC AUC']]}\n",
        "      temp = pd.DataFrame(row)\n",
        "      df = pd.concat([df, temp], ignore_index=True)\n",
        "df.to_csv(file_name, index=False)\n"
      ],
      "metadata": {
        "id": "zTcW8MhqhP5s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Calling SDV-Oversampling"
      ],
      "metadata": {
        "id": "f_VUTRHBBVDY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cols = ['algo', 'ratio', 'seed', 'learner', 'acc', 'prec', 'rec', 'f1', 'auc_roc']\n",
        "df = pd.DataFrame([], columns = cols)\n",
        "algorithm = 'gaussian_copula'\n",
        "file_name = f'{algorithm}_results'\n",
        "\n",
        "for rand_state in states:\n",
        "  if(smo):\n",
        "\n",
        "    X_train, y_train = smo_X_train[rand_state], smo_y_train[rand_state]\n",
        "  for ratio in ratios:\n",
        "\n",
        "    sd1, train_df = do_sdv(X_train, y_train)\n",
        "    results_syn_sdv = dict()\n",
        "\n",
        "    # Add synthetic data at different percentages to the main DataFrame\n",
        "    combined_df = add_synthetic_data(train_df, sd1, ratio, seed=rand_state)\n",
        "    y_train_sdv = combined_df.iloc[:,-1]\n",
        "    X_train_sdv = combined_df.iloc[:,:-1]\n",
        "\n",
        "    preprocessed_X_train_sdv, scaler, imputer = preprocess_data_train(X_train_sdv)\n",
        "    preprocessed_X_test_sdv = preprocess_data_test(X_test, scaler, imputer)\n",
        "\n",
        "    X_train_sdv, y_train_sdv = preprocessed_X_train_sdv, y_train_sdv.to_numpy()\n",
        "    X_test_sdv, y_test_sdv = preprocessed_X_test_sdv, y_test.to_numpy()\n",
        "\n",
        "    print(f\"Train data combined with {ratio * 100}% synthetic data of minority class\")\n",
        "    results = evaluate_models(X_train_sdv, X_test_sdv, y_train_sdv, y_test_sdv, rand_state)\n",
        "    for key, item in results.items():\n",
        "          row = {'algo' : [algorithm], 'ratio': [ratio], 'seed': [rand_state], 'learner': [learners[key]], 'acc': [item['Accuracy']], 'prec': [item['Precision']], 'rec': [item['Recall']], 'f1': [item['F1 Score']], 'auc_roc': [item['ROC AUC']]}\n",
        "          temp = pd.DataFrame(row)\n",
        "          df = pd.concat([df, temp], ignore_index=True)\n",
        "df.to_csv(file_name, index=False)"
      ],
      "metadata": {
        "id": "g4V6WxpY9kbd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Calling SMOTE-Oversampling"
      ],
      "metadata": {
        "id": "l78jR_BuCw1o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cols = ['algo', 'ratio', 'seed', 'learner', 'acc', 'prec', 'rec', 'f1', 'auc_roc']\n",
        "df = pd.DataFrame([], columns = cols)\n",
        "algorithm = 'smote'\n",
        "file_name = f'{algorithm}_results'\n",
        "\n",
        "for rand_state in states:\n",
        "  if(smo):\n",
        "\n",
        "    X_train, y_train = smo_X_train[rand_state], smo_y_train[rand_state]\n",
        "  for ratio in ratios:\n",
        "\n",
        "    X_train_smote, y_train_smote = smote_oversampling(X_train.to_numpy(), y_train.to_numpy(), [ratio])\n",
        "    preprocessed_X_train_smote, scaler, imputer = preprocess_data_train((np.array(X_train_smote))[0])\n",
        "    preprocessed_X_test_smote = preprocess_data_test(X_test, scaler, imputer)\n",
        "\n",
        "    X_train_smote, y_train_smote = preprocessed_X_train_smote, (np.array(y_train_smote))[0]\n",
        "    X_test_smote, y_test_smote = preprocessed_X_test_smote, y_test.to_numpy()\n",
        "\n",
        "    print(f\"Train data combined with {ratio * 100}% synthetic data of minority class:\")\n",
        "    results = evaluate_models(X_train_smote, X_test_smote, y_train_smote, y_test_smote, rand_state)\n",
        "\n",
        "    for key, item in results.items():\n",
        "          row = {'algo' : [algorithm], 'ratio': [ratio], 'seed': [rand_state], 'learner': [learners[key]], 'acc': [item['Accuracy']], 'prec': [item['Precision']], 'rec': [item['Recall']], 'f1': [item['F1 Score']], 'auc_roc': [item['ROC AUC']]}\n",
        "          temp = pd.DataFrame(row)\n",
        "          df = pd.concat([df, temp], ignore_index=True)\n",
        "df.to_csv(file_name, index=False)"
      ],
      "metadata": {
        "id": "X7S78dnHC0bw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Calling Random-Oversampling"
      ],
      "metadata": {
        "id": "KKwGBPjpKrJt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cols = ['algo', 'ratio', 'seed', 'learner', 'acc', 'prec', 'rec', 'f1', 'auc_roc']\n",
        "df = pd.DataFrame([], columns = cols)\n",
        "algorithm = 'random_oversampling'\n",
        "file_name = f'{algorithm}_results'\n",
        "\n",
        "for rand_state in states:\n",
        "  if(smo):\n",
        "\n",
        "    X_train, y_train = smo_X_train[rand_state], smo_y_train[rand_state]\n",
        "  for ratio in ratios:\n",
        "\n",
        "    X_train_random, y_train_random = random_oversampling(X_train.to_numpy(), y_train.to_numpy(), [ratio])\n",
        "\n",
        "    preprocessed_X_train_random, scaler, imputer = preprocess_data_train((np.array(X_train_random)[0]))\n",
        "    preprocessed_X_test_random = preprocess_data_test(X_test, scaler, imputer)\n",
        "\n",
        "    X_train_random, y_train_random = preprocessed_X_train_random, (np.array(y_train_random))[0]\n",
        "    X_test_random, y_test_random = preprocessed_X_test_random, y_test.to_numpy()\n",
        "\n",
        "    print(f\"Train data combined with {ratio * 100}% synthetic data of minority class:\")\n",
        "    results = evaluate_models(X_train_random, X_test_random, y_train_random, y_test_random, rand_state)\n",
        "\n",
        "    for key, item in results.items():\n",
        "          row = {'algo' : [algorithm], 'ratio': [ratio], 'seed': [rand_state], 'learner': [learners[key]], 'acc': [item['Accuracy']], 'prec': [item['Precision']], 'rec': [item['Recall']], 'f1': [item['F1 Score']], 'auc_roc': [item['ROC AUC']]}\n",
        "          temp = pd.DataFrame(row)\n",
        "          df = pd.concat([df, temp], ignore_index=True)\n",
        "df.to_csv(file_name, index=False)"
      ],
      "metadata": {
        "id": "LC2kVR1tKtFn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Calling SVM-SMOTE Over-Sampling"
      ],
      "metadata": {
        "id": "28Pi8n0HM4vr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cols = ['algo', 'ratio', 'seed', 'learner', 'acc', 'prec', 'rec', 'f1', 'auc_roc']\n",
        "df = pd.DataFrame([], columns = cols)\n",
        "algorithm = 'svm_smote'\n",
        "file_name = f'{algorithm}_results'\n",
        "\n",
        "for rand_state in states:\n",
        "  if(smo):\n",
        "\n",
        "    X_train, y_train = smo_X_train[rand_state], smo_y_train[rand_state]\n",
        "  for ratio in ratios:\n",
        "\n",
        "    X_train_svm_smote, y_train_svm_smote = svm_smote_oversampling(X_train.to_numpy(), y_train.to_numpy(), [ratio])\n",
        "\n",
        "    preprocessed_X_train_svm_smote, scaler, imputer = preprocess_data_train((np.array(X_train_svm_smote))[0])\n",
        "    preprocessed_X_test_svm_smote = preprocess_data_test(X_test, scaler, imputer)\n",
        "\n",
        "    X_train_svm_smote, y_train_svm_smote = preprocessed_X_train_svm_smote, (np.array(y_train_svm_smote))[0]\n",
        "    X_test_svm_smote, y_test_svm_smote = preprocessed_X_test_svm_smote, y_test.to_numpy()\n",
        "\n",
        "    print(f\"Train data combined with {ratio * 100}% synthetic data of minority class:\")\n",
        "    results = evaluate_models(X_train_svm_smote, X_test_svm_smote, y_train_svm_smote, y_test_svm_smote, rand_state)\n",
        "\n",
        "    for key, item in results.items():\n",
        "          row = {'algo' : [algorithm], 'ratio': [ratio], 'seed': [rand_state], 'learner': [learners[key]], 'acc': [item['Accuracy']], 'prec': [item['Precision']], 'rec': [item['Recall']], 'f1': [item['F1 Score']], 'auc_roc': [item['ROC AUC']]}\n",
        "          temp = pd.DataFrame(row)\n",
        "          df = pd.concat([df, temp], ignore_index=True)\n",
        "df.to_csv(file_name, index=False)"
      ],
      "metadata": {
        "id": "8vkEOubrM81F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Calling for RRP Over-Sampling"
      ],
      "metadata": {
        "id": "BcBCMILaXkxu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cols = ['algo', 'ratio', 'seed', 'learner', 'acc', 'prec', 'rec', 'f1', 'auc_roc']\n",
        "df = pd.DataFrame([], columns = cols)\n",
        "algorithm = 'RRP'\n",
        "file_name = f'{algorithm}_results'\n",
        "\n",
        "for rand_state in states:\n",
        "  if(smo):\n",
        "\n",
        "    X_train, y_train = smo_X_train[rand_state], smo_y_train[rand_state]\n",
        "  for ratio in ratios:\n",
        "\n",
        "    X_train_copy, y_train_copy = X_train.copy(), y_train.copy()\n",
        "\n",
        "    rt, new_data_df, train_df = RandomProjectionOversampling(X_train=X_train_copy,\n",
        "                                                                        y_train=y_train_copy,\n",
        "                                                                        threshold=20)\n",
        "\n",
        "    # Add synthetic data at different percentages to the main DataFrame\n",
        "    combined_df = add_synthetic_data(train_df, new_data_df, ratio, seed=rand_state)\n",
        "    y_train_rrp = combined_df.iloc[:,-1]\n",
        "    X_train_rrp = combined_df.iloc[:,:-1]\n",
        "\n",
        "    preprocessed_X_train_rrp, scaler, imputer = preprocess_data_train(X_train_rrp)\n",
        "    preprocessed_X_test_rrp = preprocess_data_test(X_test, scaler, imputer)\n",
        "\n",
        "    X_train_rrp, y_train_rrp = preprocessed_X_train_rrp, y_train_rrp.to_numpy()\n",
        "    X_test_rrp, y_test_rrp = preprocessed_X_test_rrp, y_test.to_numpy()\n",
        "\n",
        "    print(f\"Train data combined with {ratio * 100}% synthetic data of minority class\")\n",
        "    results = evaluate_models(X_train_rrp, X_test_rrp, y_train_rrp, y_test_rrp, rand_state)\n",
        "    for key, item in results.items():\n",
        "          row = {'algo' : [algorithm], 'ratio': [ratio], 'seed': [rand_state], 'learner': [learners[key]], 'acc': [item['Accuracy']], 'prec': [item['Precision']], 'rec': [item['Recall']], 'f1': [item['F1 Score']], 'auc_roc': [item['ROC AUC']]}\n",
        "          temp = pd.DataFrame(row)\n",
        "          df = pd.concat([df, temp], ignore_index=True)\n",
        "df.to_csv(file_name, index=False)"
      ],
      "metadata": {
        "id": "NxkjjBRlXncl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# No-Sampling"
      ],
      "metadata": {
        "id": "yem3Ldws8Qdw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cols = ['algo', 'ratio', 'seed', 'learner', 'acc', 'prec', 'rec', 'f1', 'auc_roc']\n",
        "df = pd.DataFrame([], columns = cols)\n",
        "algorithm = 'No_Sampling'\n",
        "file_name = f'{algorithm}_results'\n",
        "\n",
        "ratios = [0]\n",
        "for rand_state in states:\n",
        "  if(smo):\n",
        "\n",
        "    X_train, y_train = smo_X_train[rand_state], smo_y_train[rand_state]\n",
        "  for ratio in ratios:\n",
        "\n",
        "    X_train_no_sampling, y_train_no_sampling = X_train.copy(), y_train.copy()\n",
        "\n",
        "    preprocessed_X_train_no_sampling, scaler, imputer = preprocess_data_train(X_train_no_sampling)\n",
        "    preprocessed_X_test_no_sampling = preprocess_data_test(X_test, scaler, imputer)\n",
        "\n",
        "    X_train_no_sampling, y_train_no_sampling = preprocessed_X_train_no_sampling, y_train_no_sampling.to_numpy()\n",
        "    X_test_no_sampling, y_test_no_sampling = preprocessed_X_test_no_sampling, y_test.to_numpy()\n",
        "\n",
        "    print(f\"Train data combined with {ratio * 100}% synthetic data of minority class\")\n",
        "    results = evaluate_models(X_train_no_sampling, X_test_no_sampling, y_train_no_sampling, y_test_no_sampling, rand_state)\n",
        "    for key, item in results.items():\n",
        "          row = {'algo' : [algorithm], 'ratio': [ratio], 'seed': [rand_state], 'learner': [learners[key]], 'acc': [item['Accuracy']], 'prec': [item['Precision']], 'rec': [item['Recall']], 'f1': [item['F1 Score']], 'auc_roc': [item['ROC AUC']]}\n",
        "          temp = pd.DataFrame(row)\n",
        "          df = pd.concat([df, temp], ignore_index=True)\n",
        "df.to_csv(file_name, index=False)"
      ],
      "metadata": {
        "id": "9E0qgTbD8Sj-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "isauWyq-CyrZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}