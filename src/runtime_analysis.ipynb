{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOGPRkkvbtJ13x8hxZvNdVP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adipai/data-decent/blob/main/src/runtime_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZaFYMFWQ7tUN",
        "outputId": "d98b0b7d-60da-4330-b2ce-250f366f30b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pmlb in /usr/local/lib/python3.10/dist-packages (1.0.1.post3)\n",
            "Requirement already satisfied: pandas>=1.0.5 in /usr/local/lib/python3.10/dist-packages (from pmlb) (2.0.3)\n",
            "Requirement already satisfied: requests>=2.24.0 in /usr/local/lib/python3.10/dist-packages (from pmlb) (2.31.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from pmlb) (6.0.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.5->pmlb) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.5->pmlb) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.5->pmlb) (2024.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.5->pmlb) (1.25.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.24.0->pmlb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.24.0->pmlb) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.24.0->pmlb) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.24.0->pmlb) (2024.2.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0.5->pmlb) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install pmlb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip data.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EtxDXGj478oQ",
        "outputId": "b0cf3cbe-cca5-4769-9fad-3c7f3ed9c27a"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  data.zip\n",
            "replace data/README.md? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sdv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4C4SNrTzQJv1",
        "outputId": "368e0d69-44f9-47e5-afcb-9051d2676697"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sdv\n",
            "  Downloading sdv-1.12.0-py3-none-any.whl (132 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.0/133.0 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting boto3>=1.28 (from sdv)\n",
            "  Downloading boto3-1.34.86-py3-none-any.whl (139 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.3/139.3 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting botocore>=1.31 (from sdv)\n",
            "  Downloading botocore-1.34.86-py3-none-any.whl (12.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cloudpickle>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from sdv) (2.2.1)\n",
            "Requirement already satisfied: graphviz>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from sdv) (0.20.3)\n",
            "Requirement already satisfied: tqdm>=4.29 in /usr/local/lib/python3.10/dist-packages (from sdv) (4.66.2)\n",
            "Collecting copulas>=0.11.0 (from sdv)\n",
            "  Downloading copulas-0.11.0-py3-none-any.whl (51 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.9/51.9 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ctgan>=0.10.0 (from sdv)\n",
            "  Downloading ctgan-0.10.0-py3-none-any.whl (24 kB)\n",
            "Collecting deepecho>=0.6.0 (from sdv)\n",
            "  Downloading deepecho-0.6.0-py3-none-any.whl (27 kB)\n",
            "Collecting rdt>=1.11.0 (from sdv)\n",
            "  Downloading rdt-1.11.1-py3-none-any.whl (63 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.6/63.6 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sdmetrics>=0.14.0 (from sdv)\n",
            "  Downloading sdmetrics-0.14.0-py3-none-any.whl (169 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m169.9/169.9 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas>=1.3.4 in /usr/local/lib/python3.10/dist-packages (from sdv) (2.0.3)\n",
            "Requirement already satisfied: numpy<2,>=1.23.3 in /usr/local/lib/python3.10/dist-packages (from sdv) (1.25.2)\n",
            "Collecting jmespath<2.0.0,>=0.7.1 (from boto3>=1.28->sdv)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Collecting s3transfer<0.11.0,>=0.10.0 (from boto3>=1.28->sdv)\n",
            "  Downloading s3transfer-0.10.1-py3-none-any.whl (82 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.2/82.2 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from botocore>=1.31->sdv) (2.8.2)\n",
            "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /usr/local/lib/python3.10/dist-packages (from botocore>=1.31->sdv) (2.0.7)\n",
            "Requirement already satisfied: plotly>=5.10.0 in /usr/local/lib/python3.10/dist-packages (from copulas>=0.11.0->sdv) (5.15.0)\n",
            "Requirement already satisfied: scipy>=1.9.2 in /usr/local/lib/python3.10/dist-packages (from copulas>=0.11.0->sdv) (1.11.4)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from ctgan>=0.10.0->sdv) (2.2.1+cu121)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.3.4->sdv) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.3.4->sdv) (2024.1)\n",
            "Collecting Faker>=17 (from rdt>=1.11.0->sdv)\n",
            "  Downloading Faker-24.11.0-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m58.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scikit-learn>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from rdt>=1.11.0->sdv) (1.2.2)\n",
            "Collecting plotly>=5.10.0 (from copulas>=0.11.0->sdv)\n",
            "  Downloading plotly-5.21.0-py3-none-any.whl (15.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.7/15.7 MB\u001b[0m \u001b[31m53.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly>=5.10.0->copulas>=0.11.0->sdv) (8.2.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from plotly>=5.10.0->copulas>=0.11.0->sdv) (24.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore>=1.31->sdv) (1.16.0)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.1.0->rdt>=1.11.0->sdv) (1.4.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.1.0->rdt>=1.11.0->sdv) (3.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->ctgan>=0.10.0->sdv) (3.13.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->ctgan>=0.10.0->sdv) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->ctgan>=0.10.0->sdv) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->ctgan>=0.10.0->sdv) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->ctgan>=0.10.0->sdv) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->ctgan>=0.10.0->sdv) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.11.0->ctgan>=0.10.0->sdv)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.11.0->ctgan>=0.10.0->sdv)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.11.0->ctgan>=0.10.0->sdv)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.11.0->ctgan>=0.10.0->sdv)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.11.0->ctgan>=0.10.0->sdv)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.11.0->ctgan>=0.10.0->sdv)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.11.0->ctgan>=0.10.0->sdv)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.11.0->ctgan>=0.10.0->sdv)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.11.0->ctgan>=0.10.0->sdv)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch>=1.11.0->ctgan>=0.10.0->sdv)\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.11.0->ctgan>=0.10.0->sdv)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->ctgan>=0.10.0->sdv) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11.0->ctgan>=0.10.0->sdv)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->ctgan>=0.10.0->sdv) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->ctgan>=0.10.0->sdv) (1.3.0)\n",
            "Installing collected packages: plotly, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, jmespath, nvidia-cusparse-cu12, nvidia-cudnn-cu12, Faker, botocore, s3transfer, rdt, nvidia-cusolver-cu12, copulas, sdmetrics, boto3, deepecho, ctgan, sdv\n",
            "  Attempting uninstall: plotly\n",
            "    Found existing installation: plotly 5.15.0\n",
            "    Uninstalling plotly-5.15.0:\n",
            "      Successfully uninstalled plotly-5.15.0\n",
            "Successfully installed Faker-24.11.0 boto3-1.34.86 botocore-1.34.86 copulas-0.11.0 ctgan-0.10.0 deepecho-0.6.0 jmespath-1.0.1 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 plotly-5.21.0 rdt-1.11.1 s3transfer-0.10.1 sdmetrics-0.14.0 sdv-1.12.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# All imports here\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from pmlb import fetch_data\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import random\n",
        "import time\n",
        "import math\n",
        "import warnings\n",
        "\n",
        "from scipy.io import arff\n",
        "from sdv.datasets.local import load_csvs\n",
        "from sdv.metadata import SingleTableMetadata\n",
        "from sdv.single_table import GaussianCopulaSynthesizer\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.over_sampling import SVMSMOTE\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from collections import defaultdict\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_curve, auc, confusion_matrix"
      ],
      "metadata": {
        "id": "L8dIJ2Oc76JS"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "ci1SKUWrMtnr"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_data_train(X_train):\n",
        "    # Count missing values before handling missing data\n",
        "    missing_before = np.isnan(X_train).sum()\n",
        "\n",
        "    # Handle missing data\n",
        "    imputer = SimpleImputer(strategy='mean')\n",
        "    X_train = imputer.fit_transform(X_train)\n",
        "\n",
        "    # Count missing values after handling missing data\n",
        "    missing_after = np.isnan(X_train).sum()\n",
        "\n",
        "    # Normalize numeric columns\n",
        "    scaler = StandardScaler()\n",
        "    X_train = scaler.fit_transform(X_train)\n",
        "\n",
        "    return X_train, scaler, imputer\n",
        "\n",
        "def preprocess_data_test(X_test, scaler, imputer):\n",
        "    # Count missing values before handling missing data\n",
        "    missing_before = np.isnan(X_test).sum()\n",
        "\n",
        "    # Handle missing data\n",
        "    X_test = imputer.transform(X_test)\n",
        "\n",
        "    # Count missing values after handling missing data\n",
        "    missing_after = np.isnan(X_test).sum()\n",
        "\n",
        "    # Normalize numeric columns\n",
        "    X_test = scaler.transform(X_test)\n",
        "\n",
        "    return X_test"
      ],
      "metadata": {
        "id": "0tLn4VYYMvC9"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RRP Oversampling"
      ],
      "metadata": {
        "id": "0WLD_-UdMz2e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cluster(candidates, enough, res):\n",
        "    if len(candidates) < enough:\n",
        "        res.append(candidates)\n",
        "        return res\n",
        "\n",
        "    east, west, east_items, west_items = split(candidates)\n",
        "    res = cluster(east_items, enough, res)\n",
        "    res = cluster(west_items, enough, res)\n",
        "\n",
        "    return res\n",
        "\n",
        "\n",
        "def split(candidates):\n",
        "    pivot = random.choice(candidates)\n",
        "    east_pivot = find_farest(pivot, candidates)\n",
        "    west_pivot = find_farest(east_pivot, candidates)\n",
        "    c = cal_distance(east_pivot, west_pivot)\n",
        "\n",
        "    if c == 0:\n",
        "        east_items = candidates[:len(candidates)//2]\n",
        "        west_items = candidates[len(candidates)//2:]\n",
        "        return east_pivot, west_pivot, east_items, west_items\n",
        "\n",
        "    all_distance = []\n",
        "    for candidate in candidates:\n",
        "        a = cal_distance(candidate, west_pivot)\n",
        "        b = cal_distance(candidate, east_pivot)\n",
        "        d = (a ** 2 + c ** 2 - b ** 2) / (2 * c)\n",
        "        all_distance.append((d, candidate))\n",
        "\n",
        "    all_distance.sort(key=lambda x: x[0])\n",
        "    sorted_candidates = [item[1] for item in all_distance]\n",
        "    east_items = sorted_candidates[:len(sorted_candidates)//2]\n",
        "    west_items = sorted_candidates[len(sorted_candidates)//2:]\n",
        "\n",
        "    return east_pivot, west_pivot, east_items, west_items\n",
        "\n",
        "\n",
        "def find_farest(pivot, candidates):\n",
        "    max_d = 0\n",
        "    most_point = pivot\n",
        "\n",
        "    for candidate in candidates:\n",
        "        cur_d = cal_distance(pivot, candidate)\n",
        "        if  cur_d > max_d:\n",
        "            max_d = cur_d\n",
        "            most_point = candidate\n",
        "\n",
        "    return most_point\n",
        "\n",
        "\n",
        "def cal_distance(p1, p2):\n",
        "    return math.sqrt(sum([(v1 - v2) ** 2 for v1, v2 in zip(p1[:-1], p2[:-1])]))\n",
        "\n",
        "\n",
        "def process_mixed_cluster(cluster):\n",
        "    \"\"\"\n",
        "    in DE operation, use current-to-best to mutate the candidates\n",
        "    v_i = x_i + F * (x_b - x_i) + F_min * (x_r1 - x_r2)\n",
        "    \"\"\"\n",
        "    DE_params = {\"F\": 0.8, \"Fmin\": 0.1, \"CR\": 1.0}\n",
        "    pos_point = [(idx, item) for idx, item in enumerate(cluster) if item[-1] == 1]\n",
        "    neg_point = [(idx, item) for idx, item in enumerate(cluster) if item[-1] == 0]\n",
        "\n",
        "    candidate_l = []\n",
        "    if len(pos_point) == 1:\n",
        "        # only 1 pos point in cluster, then mutate all neg points toward to the pos point\n",
        "        xb = pos_point[0][1]\n",
        "        R = random.choice(range(len(xb)-1))\n",
        "\n",
        "        for _, xi in neg_point:\n",
        "            new_candidate = []\n",
        "            for i in range(len(xi)-1):\n",
        "                ri = np.random.uniform(low=0.0, high=1.0, size=1)[0]\n",
        "\n",
        "                if ri < DE_params[\"CR\"] or i == R:\n",
        "                    new_candidate.append(xi[i] + DE_params[\"F\"] * (xb[i] - xi[i]))\n",
        "                else:\n",
        "                    new_candidate.append(xi[i])\n",
        "\n",
        "            new_candidate.append(1)\n",
        "            candidate_l.append(np.array(new_candidate))\n",
        "    else:\n",
        "        # more than 2 pos points in cluster, then randomly pick 3 points, first 1 is current point, and\n",
        "        # another two are support points\n",
        "        for idx1, xb in pos_point:\n",
        "            R = random.choice(range(len(xb)-1))\n",
        "            for idx2, xi in enumerate(cluster):\n",
        "                if not idx1 == idx2:\n",
        "                    available_points = []\n",
        "                    for idx3, p in enumerate(cluster):\n",
        "                        if not idx3 == idx2 and not idx3 == idx1:\n",
        "                            available_points.append(p)\n",
        "                    [xr1, xr2] = random.sample(available_points, 2)\n",
        "\n",
        "                    new_candidate = []\n",
        "                    # if xr1 and xr2 all negative class, then just use xi\n",
        "                    if xr1[-1] == 0 and xr2[-1] == 0:\n",
        "                        for i in range(len(xi)-1):\n",
        "                            ri = np.random.uniform(low=0.0, high=1.0, size=1)[0]\n",
        "\n",
        "                            if ri < DE_params[\"CR\"] or i == R:\n",
        "                                new_candidate.append(xi[i] + DE_params[\"F\"] * (xb[i] - xi[i]))\n",
        "                            else:\n",
        "                                new_candidate.append(xi[i])\n",
        "                    else:\n",
        "                        for i in range(len(xi)-1):\n",
        "                            ri = np.random.uniform(low=0.0, high=1.0, size=1)[0]\n",
        "\n",
        "                            if ri < DE_params[\"CR\"] or i == R:\n",
        "                                if xr1[-1] == 1:\n",
        "                                    new_candidate.append(xi[i] + DE_params[\"F\"] * (xb[i] - xi[i]) + DE_params[\"Fmin\"] * (xr1[i] - xr2[i]))\n",
        "                                else:\n",
        "                                    new_candidate.append(xi[i] + DE_params[\"F\"] * (xb[i] - xi[i]) + DE_params[\"Fmin\"] * (xr2[i] - xr1[i]))\n",
        "                            else:\n",
        "                                new_candidate.append(xi[i])\n",
        "\n",
        "                    new_candidate.append(1)\n",
        "                    candidate_l.append(np.array(new_candidate))\n",
        "\n",
        "    return candidate_l\n",
        "\n",
        "\n",
        "def process_positive_cluster(cluster):\n",
        "    \"\"\"\n",
        "    in DE operation, use best to mutate the candidates\n",
        "    v_i = x_b + F * (x_r1 - x_r2)\n",
        "    \"\"\"\n",
        "    DE_params = {\"F\": 0.8, \"CR\": 1.0}\n",
        "    pos_point = [item for item in cluster if item[-1] == 1]\n",
        "    candidate_l = []\n",
        "\n",
        "    for idx1 in range(len(pos_point)-2):\n",
        "        for idx2 in range(idx1+1, len(pos_point)-1):\n",
        "            for idx3 in range(idx2+1, len(pos_point)):\n",
        "                [xb, xr1, xr2] = random.sample([idx1, idx2, idx3], 3)\n",
        "                xb, xr1, xr2 = pos_point[xb], pos_point[xr1], pos_point[xr2]\n",
        "\n",
        "                R = random.choice(range(len(xb)-1))\n",
        "                new_candidate = []\n",
        "\n",
        "                for i in range(len(xb)-1):\n",
        "                    ri = np.random.uniform(low=0.0, high=1.0, size=1)[0]\n",
        "\n",
        "                    if ri < DE_params[\"CR\"] or i == R:\n",
        "                        new_candidate.append(xb[i] + DE_params[\"F\"] * (xr1[i] - xr2[i]))\n",
        "                    else:\n",
        "                        new_candidate.append(xb[i])\n",
        "\n",
        "                new_candidate.append(1)\n",
        "                candidate_l.append(np.array(new_candidate))\n",
        "\n",
        "    return candidate_l\n",
        "\n",
        "\n",
        "def process_mixed_cluster_extra(cluster):\n",
        "    \"\"\"\n",
        "    in DE operation, use current-to-best-extra to mutate the candidates\n",
        "    v_i = x_b + F * (x_r1 - x_r2) + F_ex * (x_r3 - x_r4)\n",
        "    \"\"\"\n",
        "    DE_params = {\"F\": 0.8, \"CR\": 1.0, \"F_xc\": 0.1}\n",
        "    pos_point = [item for item in cluster if item[-1] == 1]\n",
        "\n",
        "    candidate_l = []\n",
        "    for xb in pos_point:\n",
        "        R = random.choice(range(len(xb)-1))\n",
        "\n",
        "        for xi in cluster:\n",
        "            if not np.array_equal(xb, xi):\n",
        "                available_points = []\n",
        "                for p in cluster:\n",
        "                    if not np.array_equal(p, xi) and not np.array_equal(p, xb):\n",
        "                        available_points.append(p)\n",
        "\n",
        "                for _ in range(20):\n",
        "                    [xr1, xr2, xr3, xr4] = random.sample(available_points, 4)\n",
        "\n",
        "                    new_candidate = []\n",
        "\n",
        "                    for i in range(len(xi)-1):\n",
        "                        ri = np.random.uniform(low=0.0, high=1.0, size=1)[0]\n",
        "\n",
        "                        if ri < DE_params[\"CR\"] or i == R:\n",
        "                            new_candidate.append(xi[i] + DE_params[\"F\"] * (xb[i] - xi[i]) + DE_params[\"F_xc\"] * (xr1[i] - xr2[i]) + DE_params[\"F_xc\"] * (xr3[i] - xr4[i]))\n",
        "                        else:\n",
        "                            new_candidate.append(xi[i])\n",
        "\n",
        "                    new_candidate.append(1)\n",
        "                    candidate_l.append(new_candidate)\n",
        "\n",
        "    return candidate_l\n",
        "\n",
        "\n",
        "def RandomProjectionOversampling(X_train, y_train, threshold):\n",
        "    train_df = pd.concat([X_train, y_train], axis=1)\n",
        "    train_df.reset_index(inplace=True, drop=True)\n",
        "    tar = y_train.name\n",
        "    X_train[tar] = y_train\n",
        "    X_train.reset_index(inplace=True, drop=True)\n",
        "    col_names = X_train.columns\n",
        "\n",
        "    n_data_to_generate = X_train[tar].value_counts()[0] - X_train[tar].value_counts()[1]\n",
        "    X_train = X_train.to_numpy()\n",
        "    start_time = time.time()\n",
        "    res = cluster(X_train, threshold, [])\n",
        "\n",
        "    new_data_negative_cluster = []\n",
        "    new_data_positive_cluster = []\n",
        "    for c in res:\n",
        "        if sum([item[-1] for item in c]) > len(c)//2:\n",
        "            cur_new_data = process_positive_cluster(c)\n",
        "            new_data_positive_cluster += cur_new_data\n",
        "        else:\n",
        "            cur_new_data = process_mixed_cluster(c)\n",
        "            new_data_negative_cluster += cur_new_data\n",
        "\n",
        "    rt = time.time() - start_time\n",
        "    print(len(new_data_negative_cluster), n_data_to_generate - len(new_data_positive_cluster), n_data_to_generate, len(new_data_positive_cluster))\n",
        "    if len(new_data_negative_cluster) >= n_data_to_generate - len(new_data_positive_cluster):\n",
        "\n",
        "        if(n_data_to_generate > len(new_data_positive_cluster)):\n",
        "            new_data = new_data_positive_cluster + random.sample(new_data_negative_cluster,\n",
        "                                                            n_data_to_generate - len(new_data_positive_cluster))\n",
        "        else:\n",
        "            new_data = new_data_positive_cluster\n",
        "    else:\n",
        "        extra_data = []\n",
        "        for c in res:\n",
        "            cur_extra_data = process_mixed_cluster_extra(c)\n",
        "            extra_data += cur_extra_data\n",
        "\n",
        "        rest_data_to_generate = n_data_to_generate - len(new_data_positive_cluster) - len(new_data_negative_cluster)\n",
        "        new_data = new_data_negative_cluster + new_data_positive_cluster + random.sample(extra_data, rest_data_to_generate)\n",
        "\n",
        "    new_data_df = pd.DataFrame(np.array(new_data), columns=col_names)\n",
        "\n",
        "    return rt, new_data_df, train_df"
      ],
      "metadata": {
        "id": "GzQMnMj_M1lg"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SDV Oversampling"
      ],
      "metadata": {
        "id": "V_XoU1Z7M3aM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def do_sdv(X_train, y_train):\n",
        "  train_df = pd.concat([X_train, y_train], axis=1)\n",
        "  class_counts = y_train.value_counts()\n",
        "\n",
        "  # Find minority class label\n",
        "  minority_class_label = class_counts.idxmin()\n",
        "\n",
        "  # Filter rows with minority class label\n",
        "  minority_df = train_df[train_df.iloc[:, -1] == minority_class_label]\n",
        "\n",
        "  # Calculate counts of majority and minority classes\n",
        "  majority_count = class_counts.max()\n",
        "  minority_count = class_counts.min()\n",
        "\n",
        "  metadata_data = SingleTableMetadata()\n",
        "  metadata_data.detect_from_dataframe(minority_df)\n",
        "  # Generate synthetic data using GaussianCopulaSynthesizer\n",
        "  synthesizer_breast_data = GaussianCopulaSynthesizer(metadata_data)\n",
        "  synthesizer_breast_data.fit(minority_df)\n",
        "\n",
        "  # Print sample synthetic data\n",
        "  synthesizer_breast_data.reset_sampling()\n",
        "  sd1 = synthesizer_breast_data.sample(num_rows=majority_count-minority_count)\n",
        "  return sd1, train_df\n",
        "\n",
        "# Function to add synthetic data to the main DataFrame based on percentage\n",
        "def add_synthetic_data(main_df, synthetic_df, percentage, seed=42):\n",
        "    # Calculate number of rows to sample\n",
        "    num_rows = int(len(synthetic_df) * percentage)\n",
        "    np.random.seed(seed)\n",
        "    # Sample the specified percentage of synthetic data\n",
        "    sampled_synthetic_data = synthetic_df.sample(n=num_rows, replace=False, random_state=seed)\n",
        "    # print(sampled_synthetic_data)\n",
        "\n",
        "    # Concatenate sampled synthetic data with main DataFrame\n",
        "    combined_df = pd.concat([main_df, sampled_synthetic_data], ignore_index=True)\n",
        "    # print(combined_df)\n",
        "    return combined_df"
      ],
      "metadata": {
        "id": "L-fr1ARLM5Qc"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random Oversampling"
      ],
      "metadata": {
        "id": "gQIAk07bM7dO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_minority_data(X, y):\n",
        "    labels, counts = np.unique(y, return_counts=True)\n",
        "    min_label = min(zip(counts, labels))[1]\n",
        "    indices_with_min_label = np.where(y == min_label)[0]\n",
        "    X_min, y_min = X[indices_with_min_label], y[indices_with_min_label]\n",
        "\n",
        "    # Other class samples\n",
        "    indices_without_min_label = np.where(y != min_label)[0]\n",
        "    X_remaining, y_remaining = X[indices_without_min_label], y[indices_without_min_label]\n",
        "\n",
        "    return X_min, y_min, X_remaining, y_remaining, min_label\n",
        "\n",
        "def random_oversampling(X_train, y_train, oversampling_ratios, seed=42):\n",
        "\n",
        "  oversampled_X_train_ratios = dict()\n",
        "  oversampled_y_train_ratios = dict()\n",
        "  X_minority, y_minority, X_remaining, y_remaining, min_label = find_minority_data(X_train, y_train)\n",
        "  ideal_samps = len(X_remaining) - len(X_minority)\n",
        "\n",
        "  oversampling_samps = [int(ideal_samps * (oversampling_ratio)) for oversampling_ratio in oversampling_ratios]\n",
        "  for oversampling_samp, oversampling_ratio in zip(oversampling_samps, oversampling_ratios):\n",
        "\n",
        "    sampling_strategy = {min_label: len(X_minority) + oversampling_samp}\n",
        "    X_train_upsampled, y_train_upsampled = RandomOverSampler(sampling_strategy=sampling_strategy, random_state = seed).fit_resample(X_train, y_train)\n",
        "\n",
        "    oversampled_X_train_ratios[oversampling_ratio] = X_train_upsampled\n",
        "    oversampled_y_train_ratios[oversampling_ratio] = y_train_upsampled\n",
        "\n",
        "  return list(oversampled_X_train_ratios.values()), list(oversampled_y_train_ratios.values())\n"
      ],
      "metadata": {
        "id": "JU1fewUpM6vK"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SMOTE Oversampling"
      ],
      "metadata": {
        "id": "DQYVRa9jM_KG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_minority_data(X, y):\n",
        "    labels, counts = np.unique(y, return_counts=True)\n",
        "    min_label = min(zip(counts, labels))[1]\n",
        "    indices_with_min_label = np.where(y == min_label)[0]\n",
        "    X_min, y_min = X[indices_with_min_label], y[indices_with_min_label]\n",
        "\n",
        "    # Other class samples\n",
        "    indices_without_min_label = np.where(y != min_label)[0]\n",
        "    X_remaining, y_remaining = X[indices_without_min_label], y[indices_without_min_label]\n",
        "\n",
        "    return X_min, y_min, X_remaining, y_remaining, min_label\n",
        "\n",
        "def smote_oversampling(X_train, y_train, oversampling_ratios, seed=42):\n",
        "\n",
        "  oversampled_X_train_ratios = dict()\n",
        "  oversampled_y_train_ratios = dict()\n",
        "  X_minority, y_minority, X_remaining, y_remaining, min_label = find_minority_data(X_train, y_train)\n",
        "  ideal_samps = len(X_remaining) - len(X_minority)\n",
        "\n",
        "  oversampling_samps = [int(ideal_samps * (oversampling_ratio)) for oversampling_ratio in oversampling_ratios]\n",
        "  for oversampling_samp, oversampling_ratio in zip(oversampling_samps, oversampling_ratios):\n",
        "\n",
        "    sampling_strategy = {min_label: len(X_minority) + oversampling_samp}\n",
        "    X_train_upsampled, y_train_upsampled = SMOTE(sampling_strategy=sampling_strategy, random_state = seed).fit_resample(X_train, y_train)\n",
        "\n",
        "    oversampled_X_train_ratios[oversampling_ratio] = X_train_upsampled\n",
        "    oversampled_y_train_ratios[oversampling_ratio] = y_train_upsampled\n",
        "\n",
        "  return list(oversampled_X_train_ratios.values()), list(oversampled_y_train_ratios.values())\n"
      ],
      "metadata": {
        "id": "EAtM8kXyM-jL"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SVM SMOTE Oversampling"
      ],
      "metadata": {
        "id": "ftNi8ft8NCjx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_minority_data(X, y):\n",
        "    labels, counts = np.unique(y, return_counts=True)\n",
        "    min_label = min(zip(counts, labels))[1]\n",
        "    indices_with_min_label = np.where(y == min_label)[0]\n",
        "    X_min, y_min = X[indices_with_min_label], y[indices_with_min_label]\n",
        "\n",
        "    # Other class samples\n",
        "    indices_without_min_label = np.where(y != min_label)[0]\n",
        "    X_remaining, y_remaining = X[indices_without_min_label], y[indices_without_min_label]\n",
        "\n",
        "    return X_min, y_min, X_remaining, y_remaining, min_label\n",
        "\n",
        "def svm_smote_oversampling(X_train, y_train, oversampling_ratios, seed=42):\n",
        "\n",
        "  oversampled_X_train_ratios = dict()\n",
        "  oversampled_y_train_ratios = dict()\n",
        "  X_minority, y_minority, X_remaining, y_remaining, min_label = find_minority_data(X_train, y_train)\n",
        "  ideal_samps = len(X_remaining) - len(X_minority)\n",
        "\n",
        "  oversampling_samps = [int(ideal_samps * (oversampling_ratio)) for oversampling_ratio in oversampling_ratios]\n",
        "  for oversampling_samp, oversampling_ratio in zip(oversampling_samps, oversampling_ratios):\n",
        "\n",
        "    sampling_strategy = {min_label: len(X_minority) + oversampling_samp}\n",
        "    X_train_upsampled, y_train_upsampled = SVMSMOTE(sampling_strategy=sampling_strategy, random_state = seed).fit_resample(X_train, y_train)\n",
        "\n",
        "    oversampled_X_train_ratios[oversampling_ratio] = X_train_upsampled\n",
        "    oversampled_y_train_ratios[oversampling_ratio] = y_train_upsampled\n",
        "\n",
        "  return list(oversampled_X_train_ratios.values()), list(oversampled_y_train_ratios.values())\n"
      ],
      "metadata": {
        "id": "MYQ8zcDaNEov"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Intelligent Pruning"
      ],
      "metadata": {
        "id": "aLXQ29gaNGr7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_majority_data(X, y):\n",
        "    labels, counts = np.unique(y, return_counts=True)\n",
        "    max_label = max(zip(counts, labels))[1]\n",
        "    indices_with_max_label = np.where(y == max_label)[0]\n",
        "    X_maj, y_maj = X[indices_with_max_label], y[indices_with_max_label]\n",
        "\n",
        "    # Exclude majority class samples\n",
        "    indices_without_max_label = np.where(y != max_label)[0]\n",
        "    X_remaining, y_remaining = X[indices_without_max_label], y[indices_without_max_label]\n",
        "\n",
        "    return X_maj, y_maj, X_remaining, y_remaining, min(counts)\n",
        "\n",
        "def do_clustering(X, y, labels):\n",
        "  clustered_X = defaultdict(list)\n",
        "  clustered_y = defaultdict(list)\n",
        "\n",
        "  for i, label in enumerate(labels):\n",
        "      clustered_X[label].append(X[i])\n",
        "      clustered_y[label].append(y[i])\n",
        "\n",
        "  # Sort clustered_X and clustered_y in descending order based on the length of values in each dictionary\n",
        "  sorted_clustered_X = dict(sorted(clustered_X.items(), key=lambda x: -len(x[1])))\n",
        "  sorted_clustered_y = dict(sorted(clustered_y.items(), key=lambda x: -len(x[1])))\n",
        "\n",
        "  return sorted_clustered_X, sorted_clustered_y\n",
        "\n",
        "\n",
        "def intelligent_prune_data(pruning_samps, pruning_ratios, clustered_X, clustered_y, per_cluster_pruning_ratio=0.7, seed=42):\n",
        "  random.seed(seed)\n",
        "  pruning_ratios_X_maj, pruning_ratios_y_maj = defaultdict(list), defaultdict(list)\n",
        "  for pruning_samp, pruning_ratio in zip(pruning_samps, pruning_ratios):\n",
        "    samps = 0\n",
        "    # print(\"For Pruning samps: \", pruning_samp)\n",
        "    prune_samps = pruning_samp\n",
        "    # print(prune_samps)\n",
        "    clustered_X_new = defaultdict(list)\n",
        "    clustered_y_new = defaultdict(list)\n",
        "    # Iterate over the sorted dictionaries\n",
        "    for label, values_X in clustered_X.items():\n",
        "        # Calculate the number of samples to prune\n",
        "        num_samples_to_prune = int(prune_samps * per_cluster_pruning_ratio)\n",
        "        if(num_samples_to_prune > len(values_X)):\n",
        "          num_samples_to_prune = len(values_X)//2\n",
        "          prune_samps -= num_samples_to_prune\n",
        "        else:\n",
        "          prune_samps -= num_samples_to_prune\n",
        "\n",
        "        # Randomly choose samples to prune\n",
        "        indices_to_prune = random.sample(range(len(values_X)), num_samples_to_prune)\n",
        "\n",
        "        # Prune the samples from clustered_X and clustered_y\n",
        "        clustered_X_new[label] = [values_X[i] for i in range(len(values_X)) if i not in indices_to_prune]\n",
        "        clustered_y_new[label] = [clustered_y[label][i] for i in range(len(clustered_y[label])) if i not in indices_to_prune]\n",
        "\n",
        "    iter = 0\n",
        "    while(prune_samps > 0):\n",
        "        if(iter>=100):\n",
        "          break\n",
        "        for label, values_X in clustered_X_new.items():\n",
        "          if(prune_samps <=0 or len(values_X) <= 0):\n",
        "            break\n",
        "          # print(len(values_X))\n",
        "          index_to_prune = random.sample(range(len(values_X)), 1)\n",
        "          clustered_X_new[label] = [values_X[i] for i in range(len(values_X)) if i not in index_to_prune]\n",
        "          clustered_y_new[label] = [clustered_y_new[label][i] for i in range(len(clustered_y_new[label])) if i not in index_to_prune]\n",
        "\n",
        "          prune_samps -= 1\n",
        "        iter += 1\n",
        "\n",
        "    for label in clustered_X_new:\n",
        "        pruning_ratios_X_maj[pruning_ratio].extend(clustered_X_new[label])\n",
        "        pruning_ratios_y_maj[pruning_ratio].extend(clustered_y_new[label])\n",
        "\n",
        "  return pruning_ratios_X_maj, pruning_ratios_y_maj\n",
        "\n",
        "def combine_data(pruning_ratios, pruning_ratios_X_maj, pruning_ratios_y_maj, X_remaining, y_remaining):\n",
        "\n",
        "  pruning_ratios_X, pruning_ratios_y = defaultdict(list), defaultdict(list)\n",
        "  for pruning_ratio in pruning_ratios:\n",
        "    pruning_ratios_X[pruning_ratio].extend(pruning_ratios_X_maj[pruning_ratio])\n",
        "    pruning_ratios_X[pruning_ratio].extend(X_remaining)\n",
        "\n",
        "    pruning_ratios_y[pruning_ratio].extend(pruning_ratios_y_maj[pruning_ratio])\n",
        "    pruning_ratios_y[pruning_ratio].extend(y_remaining)\n",
        "\n",
        "  return pruning_ratios_X, pruning_ratios_y\n",
        "\n",
        "def do_intelligent_pruning(X, y, ratio, per_cluster_pruning_ratio=0.7, seed=42):\n",
        "\n",
        "  X_maj, y_maj, X_remaining, y_remaining, min_class_samples = find_majority_data(X, y)\n",
        "  kmeans = KMeans(n_clusters=3, random_state = 42)\n",
        "  kmeans.fit(X_maj)\n",
        "  labels = kmeans.labels_\n",
        "  clustered_X, clustered_y = do_clustering(X_maj, y_maj, labels)\n",
        "\n",
        "  pruning_best = len(X_maj)-min_class_samples\n",
        "  pruning_samps = [int(pruning_best * ratio)]\n",
        "  pruning_ratios = [ratio]\n",
        "\n",
        "  pruning_ratios_X_maj, pruning_ratios_y_maj = intelligent_prune_data(pruning_samps, pruning_ratios, clustered_X, clustered_y, \\\n",
        "                                                                      per_cluster_pruning_ratio=per_cluster_pruning_ratio, seed=seed)\n",
        "\n",
        "  pruning_ratios_X, pruning_ratios_y = combine_data(pruning_ratios, pruning_ratios_X_maj, pruning_ratios_y_maj, X_remaining, y_remaining)\n",
        "\n",
        "  return list(pruning_ratios_X.values()), list(pruning_ratios_y.values())"
      ],
      "metadata": {
        "id": "pYyPPKIENIcT"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random Pruning"
      ],
      "metadata": {
        "id": "aKl7Qe_eNLkv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "inputs:\n",
        "X: np.array\n",
        "y: np.array\n",
        "percentage: from 0.0 upto 1.0, enter int value\n",
        "\"\"\"\n",
        "def random_prune_data(X, y, ratio, seed = 42):\n",
        "  # preprocessed_X, scaler, imputer = preprocess_data_train(X)\n",
        "  # preprocessed_X_test = preprocess_data_test(X_test, scaler, imputer)\n",
        "\n",
        "  # X_train, y_train = preprocessed_X_train.to_numpy(), y_train.to_numpy()\n",
        "  # X_test, y_test = preprocessed_X_test.to_numpy(), y_test.to_numpy()\n",
        "  np.random.seed(seed)\n",
        "  labels_count = {}\n",
        "  labels = np.unique(y)\n",
        "  for label in labels:\n",
        "    labels_count[label] = np.count_nonzero(y == label)\n",
        "  max_label = min_label = labels[0]\n",
        "  for label in labels_count:\n",
        "    if labels_count[label] > labels_count[max_label]:\n",
        "      max_label = label\n",
        "    if labels_count[label] < labels_count[min_label]:\n",
        "      min_label = label\n",
        "\n",
        "  # print(\"Max\", max_label, labels_count[max_label])\n",
        "  # print(\"Min\", min_label, labels_count[min_label])\n",
        "\n",
        "  prune_counts = {}\n",
        "  prune_indexes = {}\n",
        "  for label in labels_count:\n",
        "    prune_counts[label] = labels_count[label] - labels_count[min_label]\n",
        "    prune_indexes[label] = np.where(y == label)[0]\n",
        "\n",
        "  prune_amount = int(ratio * sum(map(lambda x: x[1], prune_counts.items())))\n",
        "  prune_it = {}\n",
        "\n",
        "  while prune_amount > 0:\n",
        "    for label in labels:\n",
        "      if (len(prune_indexes[label]) - labels_count[min_label]) > 0 and prune_amount > 0:\n",
        "        random_index = np.random.choice(len(prune_indexes[label]))\n",
        "        random_item = prune_indexes[label][random_index]\n",
        "        prune_indexes[label] = np.delete(prune_indexes[label], random_index)\n",
        "        if prune_it.get(label, None) is None:\n",
        "          prune_it[label] = np.array([])\n",
        "        prune_it[label] = np.append(prune_it[label], [random_item])\n",
        "        prune_amount -= 1\n",
        "\n",
        "\n",
        "\n",
        "  formatted_indexes = np.array([])\n",
        "  for label in prune_indexes:\n",
        "    formatted_indexes = np.append(formatted_indexes, prune_indexes[label])\n",
        "  formatted_indexes = np.sort(formatted_indexes)\n",
        "  new_arr = np.array([np.int64(i) for i in formatted_indexes])\n",
        "\n",
        "  return X[new_arr], y[new_arr]"
      ],
      "metadata": {
        "id": "QupxdCN6NMyJ"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ratios = [ratio for ratio in np.arange(0, 1.1, 0.2)]"
      ],
      "metadata": {
        "id": "BZmHBaPjNQb1"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learners = {'Logistic Regression' : 'LR', 'SVM': 'SVM', 'Decision Tree': 'DT' }\n",
        "\n",
        "states = [82, 15, 4, 95, 36, 32, 29, 18, 14, 87]\n",
        "ratios = [1.0]\n",
        "smo_K = 5\n",
        "smo = False"
      ],
      "metadata": {
        "id": "p9Z-PQshNSAK"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_data(X, y, random_state):\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = random_state)\n",
        "  return X_train, X_test, y_train, y_test"
      ],
      "metadata": {
        "id": "ow6aZPG5NchB"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = fetch_data('breast_cancer')\n",
        "print(\"#Rows: \", len(df))\n",
        "print(\"#Columns: \", len(df.columns))\n",
        "print(\"Class-Distribution: \", int((np.unique(df.iloc[:,-1], return_counts=True)[1][0]/len(df))*100), 100-int((np.unique(df.iloc[:,-1], return_counts=True)[1][0]/len(df))*100))\n",
        "X = df.iloc[:, :-1]\n",
        "y = df.iloc[:, -1]\n",
        "avg_time_breast_cancer = dict()\n",
        "\n",
        "# intelligent pruning\n",
        "per_cluster_pruning_ratios = [0.5]\n",
        "intelligent_pruning_time = 0\n",
        "for rand_state in states:\n",
        "  X_train, X_test, y_train, y_test = split_data(X, y, rand_state)\n",
        "  for per_cluster_pruning_ratio in per_cluster_pruning_ratios:\n",
        "    for ratio in ratios:\n",
        "      X_train_copy, y_train_copy = X_train.copy(), y_train.copy()\n",
        "\n",
        "      start_time = time.time()  # Start timing\n",
        "      intelligent_pruned_X_train, intelligent_pruned_y_train = do_intelligent_pruning(X_train_copy.to_numpy(), y_train_copy.to_numpy(), ratio, per_cluster_pruning_ratio=per_cluster_pruning_ratio)\n",
        "      end_time = time.time()  # End timing\n",
        "      execution_time = end_time - start_time\n",
        "      intelligent_pruning_time += execution_time\n",
        "intelligent_pruning_time /= len(states)\n",
        "\n",
        "# random pruning\n",
        "random_pruning_time = 0\n",
        "for rand_state in states:\n",
        "  X_train, X_test, y_train, y_test = split_data(X, y, rand_state)\n",
        "  X_train, X_test, y_train, y_test = split_data(X, y, rand_state)\n",
        "  for ratio in ratios:\n",
        "    X_train_copy, y_train_copy = X_train.copy(), y_train.copy()\n",
        "    start_time = time.time()  # Start timing\n",
        "    random_pruned_X_train, random_pruned_y_train = random_prune_data(X_train_copy.to_numpy(), y_train_copy.to_numpy(), ratio, seed=rand_state)\n",
        "    end_time = time.time()  # End timing\n",
        "    execution_time = end_time - start_time\n",
        "    random_pruning_time += execution_time\n",
        "random_pruning_time /= len(states)\n",
        "\n",
        "# gaussian copula\n",
        "gc_time = 0\n",
        "for rand_state in states:\n",
        "  X_train, X_test, y_train, y_test = split_data(X, y, rand_state)\n",
        "  for ratio in ratios:\n",
        "    X_train_copy, y_train_copy = X_train.copy(), y_train.copy()\n",
        "    start_time = time.time()  # Start timing\n",
        "    sd1, train_df = do_sdv(X_train_copy, y_train_copy)\n",
        "    end_time = time.time()  # End timing\n",
        "    execution_time = end_time - start_time\n",
        "    gc_time += execution_time\n",
        "gc_time /= len(states)\n",
        "\n",
        "# smote\n",
        "smote_time = 0\n",
        "for rand_state in states:\n",
        "  X_train, X_test, y_train, y_test = split_data(X, y, rand_state)\n",
        "  for ratio in ratios:\n",
        "    X_train_copy, y_train_copy = X_train.copy(), y_train.copy()\n",
        "    start_time = time.time()  # Start timing\n",
        "    X_train_smote, y_train_smote = smote_oversampling(X_train_copy.to_numpy(), y_train_copy.to_numpy(), [ratio])\n",
        "    end_time = time.time()  # End timing\n",
        "    execution_time = end_time - start_time\n",
        "    smote_time += execution_time\n",
        "smote_time /= len(states)\n",
        "\n",
        "# random_oversampling\n",
        "random_over_time = 0\n",
        "for rand_state in states:\n",
        "  X_train, X_test, y_train, y_test = split_data(X, y, rand_state)\n",
        "  for ratio in ratios:\n",
        "    X_train_copy, y_train_copy = X_train.copy(), y_train.copy()\n",
        "    start_time = time.time()  # Start timing\n",
        "    X_train_random, y_train_random = random_oversampling(X_train_copy.to_numpy(), y_train_copy.to_numpy(), [ratio])\n",
        "    end_time = time.time()  # End timing\n",
        "    execution_time = end_time - start_time\n",
        "    random_over_time += execution_time\n",
        "random_over_time /= len(states)\n",
        "\n",
        "# svm smote\n",
        "svm_smote_time = 0\n",
        "for rand_state in states:\n",
        "  X_train, X_test, y_train, y_test = split_data(X, y, rand_state)\n",
        "  for ratio in ratios:\n",
        "    X_train_copy, y_train_copy = X_train.copy(), y_train.copy()\n",
        "    start_time = time.time()  # Start timing\n",
        "    X_train_random, y_train_random = svm_smote_oversampling(X_train_copy.to_numpy(), y_train_copy.to_numpy(), [ratio])\n",
        "    end_time = time.time()  # End timing\n",
        "    execution_time = end_time - start_time\n",
        "    svm_smote_time += execution_time\n",
        "svm_smote_time /= len(states)\n",
        "\n",
        "# RRP\n",
        "rrp_time = 0\n",
        "for rand_state in states:\n",
        "  X_train, X_test, y_train, y_test = split_data(X, y, rand_state)\n",
        "  for ratio in ratios:\n",
        "    X_train_copy, y_train_copy = X_train.copy(), y_train.copy()\n",
        "    start_time = time.time()  # Start timing\n",
        "    rt, new_data_df, train_df = RandomProjectionOversampling(X_train=X_train_copy,\n",
        "                                                                        y_train=y_train_copy,\n",
        "                                                                        threshold=10)\n",
        "    end_time = time.time()  # End timing\n",
        "    execution_time = end_time - start_time\n",
        "    rrp_time += execution_time\n",
        "rrp_time /= len(states)\n",
        "\n",
        "avg_time_breast_cancer['intelligent_pruning'] = intelligent_pruning_time\n",
        "avg_time_breast_cancer['random_pruning'] = random_pruning_time\n",
        "avg_time_breast_cancer['sdv-g'] = gc_time\n",
        "avg_time_breast_cancer['smote'] = smote_time\n",
        "avg_time_breast_cancer['random_oversampling'] = random_over_time\n",
        "avg_time_breast_cancer['svm_smote'] = svm_smote_time\n",
        "avg_time_breast_cancer['rrp'] = rrp_time\n",
        "\n",
        "avg_time_breast_cancer\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mvypt6Ph8jv6",
        "outputId": "6789535e-009a-4514-cb6c-ed460da6e2ff"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#Rows:  286\n",
            "#Columns:  10\n",
            "Class-Distribution:  70 30\n",
            "323 60 94 34\n",
            "290 46 88 42\n",
            "292 66 92 26\n",
            "289 42 86 44\n",
            "288 50 88 38\n",
            "262 48 90 42\n",
            "226 28 90 62\n",
            "287 36 82 46\n",
            "267 58 96 38\n",
            "277 58 94 36\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'intelligent_pruning': 0.14870667457580566,\n",
              " 'random_pruning': 0.009167814254760742,\n",
              " 'sdv-g': 0.9510603189468384,\n",
              " 'smote': 0.002190566062927246,\n",
              " 'random_oversampling': 0.0009759902954101562,\n",
              " 'svm_smote': 0.008511209487915039,\n",
              " 'rrp': 0.06048896312713623}"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = fetch_data('churn')\n",
        "print(\"#Rows: \", len(df))\n",
        "print(\"#Columns: \", len(df.columns))\n",
        "print(\"Class-Distribution: \", int((np.unique(df.iloc[:,-1], return_counts=True)[1][0]/len(df))*100), 100-int((np.unique(df.iloc[:,-1], return_counts=True)[1][0]/len(df))*100))\n",
        "avg_time_churn = dict()\n",
        "\n",
        "# intelligent pruning\n",
        "per_cluster_pruning_ratios = [0.5]\n",
        "intelligent_pruning_time = 0\n",
        "for rand_state in states:\n",
        "  X_train, X_test, y_train, y_test = split_data(X, y, rand_state)\n",
        "  for per_cluster_pruning_ratio in per_cluster_pruning_ratios:\n",
        "    for ratio in ratios:\n",
        "      X_train_copy, y_train_copy = X_train.copy(), y_train.copy()\n",
        "\n",
        "      start_time = time.time()  # Start timing\n",
        "      intelligent_pruned_X_train, intelligent_pruned_y_train = do_intelligent_pruning(X_train_copy.to_numpy(), y_train_copy.to_numpy(), ratio, per_cluster_pruning_ratio=per_cluster_pruning_ratio)\n",
        "      end_time = time.time()  # End timing\n",
        "      execution_time = end_time - start_time\n",
        "      intelligent_pruning_time += execution_time\n",
        "intelligent_pruning_time /= len(states)\n",
        "\n",
        "# random pruning\n",
        "random_pruning_time = 0\n",
        "for rand_state in states:\n",
        "  X_train, X_test, y_train, y_test = split_data(X, y, rand_state)\n",
        "  X_train, X_test, y_train, y_test = split_data(X, y, rand_state)\n",
        "  for ratio in ratios:\n",
        "    X_train_copy, y_train_copy = X_train.copy(), y_train.copy()\n",
        "    start_time = time.time()  # Start timing\n",
        "    random_pruned_X_train, random_pruned_y_train = random_prune_data(X_train_copy.to_numpy(), y_train_copy.to_numpy(), ratio, seed=rand_state)\n",
        "    end_time = time.time()  # End timing\n",
        "    execution_time = end_time - start_time\n",
        "    random_pruning_time += execution_time\n",
        "random_pruning_time /= len(states)\n",
        "\n",
        "# gaussian copula\n",
        "gc_time = 0\n",
        "for rand_state in states:\n",
        "  X_train, X_test, y_train, y_test = split_data(X, y, rand_state)\n",
        "  for ratio in ratios:\n",
        "    X_train_copy, y_train_copy = X_train.copy(), y_train.copy()\n",
        "    start_time = time.time()  # Start timing\n",
        "    sd1, train_df = do_sdv(X_train_copy, y_train_copy)\n",
        "    end_time = time.time()  # End timing\n",
        "    execution_time = end_time - start_time\n",
        "    gc_time += execution_time\n",
        "gc_time /= len(states)\n",
        "\n",
        "# smote\n",
        "smote_time = 0\n",
        "for rand_state in states:\n",
        "  X_train, X_test, y_train, y_test = split_data(X, y, rand_state)\n",
        "  for ratio in ratios:\n",
        "    X_train_copy, y_train_copy = X_train.copy(), y_train.copy()\n",
        "    start_time = time.time()  # Start timing\n",
        "    X_train_smote, y_train_smote = smote_oversampling(X_train_copy.to_numpy(), y_train_copy.to_numpy(), [ratio])\n",
        "    end_time = time.time()  # End timing\n",
        "    execution_time = end_time - start_time\n",
        "    smote_time += execution_time\n",
        "smote_time /= len(states)\n",
        "\n",
        "# random_oversampling\n",
        "random_over_time = 0\n",
        "for rand_state in states:\n",
        "  X_train, X_test, y_train, y_test = split_data(X, y, rand_state)\n",
        "  for ratio in ratios:\n",
        "    X_train_copy, y_train_copy = X_train.copy(), y_train.copy()\n",
        "    start_time = time.time()  # Start timing\n",
        "    X_train_random, y_train_random = random_oversampling(X_train_copy.to_numpy(), y_train_copy.to_numpy(), [ratio])\n",
        "    end_time = time.time()  # End timing\n",
        "    execution_time = end_time - start_time\n",
        "    random_over_time += execution_time\n",
        "random_over_time /= len(states)\n",
        "\n",
        "# svm smote\n",
        "svm_smote_time = 0\n",
        "for rand_state in states:\n",
        "  X_train, X_test, y_train, y_test = split_data(X, y, rand_state)\n",
        "  for ratio in ratios:\n",
        "    X_train_copy, y_train_copy = X_train.copy(), y_train.copy()\n",
        "    start_time = time.time()  # Start timing\n",
        "    X_train_random, y_train_random = svm_smote_oversampling(X_train_copy.to_numpy(), y_train_copy.to_numpy(), [ratio])\n",
        "    end_time = time.time()  # End timing\n",
        "    execution_time = end_time - start_time\n",
        "    svm_smote_time += execution_time\n",
        "svm_smote_time /= len(states)\n",
        "\n",
        "# RRP\n",
        "rrp_time = 0\n",
        "for rand_state in states:\n",
        "  X_train, X_test, y_train, y_test = split_data(X, y, rand_state)\n",
        "  for ratio in ratios:\n",
        "    X_train_copy, y_train_copy = X_train.copy(), y_train.copy()\n",
        "    start_time = time.time()  # Start timing\n",
        "    rt, new_data_df, train_df = RandomProjectionOversampling(X_train=X_train_copy,\n",
        "                                                                        y_train=y_train_copy,\n",
        "                                                                        threshold=10)\n",
        "    end_time = time.time()  # End timing\n",
        "    execution_time = end_time - start_time\n",
        "    rrp_time += execution_time\n",
        "rrp_time /= len(states)\n",
        "\n",
        "avg_time_churn['intelligent_pruning'] = intelligent_pruning_time\n",
        "avg_time_churn['random_pruning'] = random_pruning_time\n",
        "avg_time_churn['sdv-g'] = gc_time\n",
        "avg_time_churn['smote'] = smote_time\n",
        "avg_time_churn['random_oversampling'] = random_over_time\n",
        "avg_time_churn['svm_smote'] = svm_smote_time\n",
        "avg_time_churn['rrp'] = rrp_time\n",
        "\n",
        "avg_time_churn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oAHBTyFv9r30",
        "outputId": "cdfdc0dc-cfc8-48b8-e292-fab58f0eddb8"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#Rows:  5000\n",
            "#Columns:  21\n",
            "Class-Distribution:  85 15\n",
            "323 60 94 34\n",
            "290 46 88 42\n",
            "292 66 92 26\n",
            "289 42 86 44\n",
            "288 50 88 38\n",
            "262 48 90 42\n",
            "226 28 90 62\n",
            "287 36 82 46\n",
            "267 58 96 38\n",
            "277 58 94 36\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'intelligent_pruning': 0.20820820331573486,\n",
              " 'random_pruning': 0.012825798988342286,\n",
              " 'sdv-g': 1.1037529945373534,\n",
              " 'smote': 0.0030627727508544924,\n",
              " 'random_oversampling': 0.0017938852310180665,\n",
              " 'svm_smote': 0.014114451408386231,\n",
              " 'rrp': 0.09835276603698731}"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "project = \"Defect_Eclipse_JDT_Core\"\n",
        "fname = \"_\".join(project.split(\"_\")[1:])\n",
        "data_path = f\"data/imbalance_defects_prediction/7_CK_NET_PROC/input/{fname}--CK_NET_PROC.arff\"\n",
        "data = arff.loadarff(data_path)\n",
        "df = pd.DataFrame(data[0])\n",
        "df['isBug'] = df['isBug'].astype('str')\n",
        "d = {'YES': 1, 'NO': 0}  # Remove the byte string prefix 'b'\n",
        "df['isBug'] = df['isBug'].map(d).fillna(df['isBug'])\n",
        "print(df['isBug'])\n",
        "print(\"before drop duplicates\", df.shape[0])\n",
        "df = df.drop_duplicates()\n",
        "df.reset_index(inplace=True, drop=True)\n",
        "print(\"after drop duplicates\", df.shape[0])\n",
        "print(\"#Rows: \", len(df))\n",
        "print(\"#Columns: \", len(df.columns))\n",
        "print(\"Class-Distribution: \", int((np.unique(df.iloc[:,-1], return_counts=True)[1][0]/len(df))*100), 100-int((np.unique(df.iloc[:,-1], return_counts=True)[1][0]/len(df))*100))\n",
        "\n",
        "avg_time_eclipse_JDT = dict()\n",
        "\n",
        "# intelligent pruning\n",
        "per_cluster_pruning_ratios = [0.5]\n",
        "intelligent_pruning_time = 0\n",
        "for rand_state in states:\n",
        "  X_train, X_test, y_train, y_test = split_data(X, y, rand_state)\n",
        "  for per_cluster_pruning_ratio in per_cluster_pruning_ratios:\n",
        "    for ratio in ratios:\n",
        "      X_train_copy, y_train_copy = X_train.copy(), y_train.copy()\n",
        "\n",
        "      start_time = time.time()  # Start timing\n",
        "      intelligent_pruned_X_train, intelligent_pruned_y_train = do_intelligent_pruning(X_train_copy.to_numpy(), y_train_copy.to_numpy(), ratio, per_cluster_pruning_ratio=per_cluster_pruning_ratio)\n",
        "      end_time = time.time()  # End timing\n",
        "      execution_time = end_time - start_time\n",
        "      intelligent_pruning_time += execution_time\n",
        "intelligent_pruning_time /= len(states)\n",
        "\n",
        "# random pruning\n",
        "random_pruning_time = 0\n",
        "for rand_state in states:\n",
        "  X_train, X_test, y_train, y_test = split_data(X, y, rand_state)\n",
        "  X_train, X_test, y_train, y_test = split_data(X, y, rand_state)\n",
        "  for ratio in ratios:\n",
        "    X_train_copy, y_train_copy = X_train.copy(), y_train.copy()\n",
        "    start_time = time.time()  # Start timing\n",
        "    random_pruned_X_train, random_pruned_y_train = random_prune_data(X_train_copy.to_numpy(), y_train_copy.to_numpy(), ratio, seed=rand_state)\n",
        "    end_time = time.time()  # End timing\n",
        "    execution_time = end_time - start_time\n",
        "    random_pruning_time += execution_time\n",
        "random_pruning_time /= len(states)\n",
        "\n",
        "# gaussian copula\n",
        "gc_time = 0\n",
        "for rand_state in states:\n",
        "  X_train, X_test, y_train, y_test = split_data(X, y, rand_state)\n",
        "  for ratio in ratios:\n",
        "    X_train_copy, y_train_copy = X_train.copy(), y_train.copy()\n",
        "    start_time = time.time()  # Start timing\n",
        "    sd1, train_df = do_sdv(X_train_copy, y_train_copy)\n",
        "    end_time = time.time()  # End timing\n",
        "    execution_time = end_time - start_time\n",
        "    gc_time += execution_time\n",
        "gc_time /= len(states)\n",
        "\n",
        "# smote\n",
        "smote_time = 0\n",
        "for rand_state in states:\n",
        "  X_train, X_test, y_train, y_test = split_data(X, y, rand_state)\n",
        "  for ratio in ratios:\n",
        "    X_train_copy, y_train_copy = X_train.copy(), y_train.copy()\n",
        "    start_time = time.time()  # Start timing\n",
        "    X_train_smote, y_train_smote = smote_oversampling(X_train_copy.to_numpy(), y_train_copy.to_numpy(), [ratio])\n",
        "    end_time = time.time()  # End timing\n",
        "    execution_time = end_time - start_time\n",
        "    smote_time += execution_time\n",
        "smote_time /= len(states)\n",
        "\n",
        "# random_oversampling\n",
        "random_over_time = 0\n",
        "for rand_state in states:\n",
        "  X_train, X_test, y_train, y_test = split_data(X, y, rand_state)\n",
        "  for ratio in ratios:\n",
        "    X_train_copy, y_train_copy = X_train.copy(), y_train.copy()\n",
        "    start_time = time.time()  # Start timing\n",
        "    X_train_random, y_train_random = random_oversampling(X_train_copy.to_numpy(), y_train_copy.to_numpy(), [ratio])\n",
        "    end_time = time.time()  # End timing\n",
        "    execution_time = end_time - start_time\n",
        "    random_over_time += execution_time\n",
        "random_over_time /= len(states)\n",
        "\n",
        "# svm smote\n",
        "svm_smote_time = 0\n",
        "for rand_state in states:\n",
        "  X_train, X_test, y_train, y_test = split_data(X, y, rand_state)\n",
        "  for ratio in ratios:\n",
        "    X_train_copy, y_train_copy = X_train.copy(), y_train.copy()\n",
        "    start_time = time.time()  # Start timing\n",
        "    X_train_random, y_train_random = svm_smote_oversampling(X_train_copy.to_numpy(), y_train_copy.to_numpy(), [ratio])\n",
        "    end_time = time.time()  # End timing\n",
        "    execution_time = end_time - start_time\n",
        "    svm_smote_time += execution_time\n",
        "svm_smote_time /= len(states)\n",
        "\n",
        "# RRP\n",
        "rrp_time = 0\n",
        "for rand_state in states:\n",
        "  X_train, X_test, y_train, y_test = split_data(X, y, rand_state)\n",
        "  for ratio in ratios:\n",
        "    X_train_copy, y_train_copy = X_train.copy(), y_train.copy()\n",
        "    start_time = time.time()  # Start timing\n",
        "    rt, new_data_df, train_df = RandomProjectionOversampling(X_train=X_train_copy,\n",
        "                                                                        y_train=y_train_copy,\n",
        "                                                                        threshold=10)\n",
        "    end_time = time.time()  # End timing\n",
        "    execution_time = end_time - start_time\n",
        "    rrp_time += execution_time\n",
        "rrp_time /= len(states)\n",
        "\n",
        "avg_time_eclipse_JDT['intelligent_pruning'] = intelligent_pruning_time\n",
        "avg_time_eclipse_JDT['random_pruning'] = random_pruning_time\n",
        "avg_time_eclipse_JDT['sdv-g'] = gc_time\n",
        "avg_time_eclipse_JDT['smote'] = smote_time\n",
        "avg_time_eclipse_JDT['random_oversampling'] = random_over_time\n",
        "avg_time_eclipse_JDT['svm_smote'] = svm_smote_time\n",
        "avg_time_eclipse_JDT['rrp'] = rrp_time\n",
        "\n",
        "avg_time_eclipse_JDT"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JwaQ7DzN9RtF",
        "outputId": "2f059759-6324-48fc-8125-7c093b366c4b"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0      0\n",
            "1      0\n",
            "2      0\n",
            "3      0\n",
            "4      0\n",
            "      ..\n",
            "992    0\n",
            "993    0\n",
            "994    0\n",
            "995    0\n",
            "996    0\n",
            "Name: isBug, Length: 997, dtype: int64\n",
            "before drop duplicates 997\n",
            "after drop duplicates 997\n",
            "#Rows:  997\n",
            "#Columns:  82\n",
            "Class-Distribution:  79 21\n",
            "323 60 94 34\n",
            "290 46 88 42\n",
            "292 66 92 26\n",
            "289 42 86 44\n",
            "288 50 88 38\n",
            "262 48 90 42\n",
            "226 28 90 62\n",
            "287 36 82 46\n",
            "267 58 96 38\n",
            "277 58 94 36\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'intelligent_pruning': 0.03628861904144287,\n",
              " 'random_pruning': 0.004814291000366211,\n",
              " 'sdv-g': 0.7802107334136963,\n",
              " 'smote': 0.0020032167434692384,\n",
              " 'random_oversampling': 0.0013318538665771484,\n",
              " 'svm_smote': 0.00854194164276123,\n",
              " 'rrp': 0.05523369312286377}"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "project = \"Defect_Eclipse_PDE_UI\"\n",
        "fname = \"_\".join(project.split(\"_\")[1:])\n",
        "data_path = f\"data/imbalance_defects_prediction/7_CK_NET_PROC/input/{fname}--CK_NET_PROC.arff\"\n",
        "data = arff.loadarff(data_path)\n",
        "df = pd.DataFrame(data[0])\n",
        "df['isBug'] = df['isBug'].astype('str')\n",
        "d = {'YES': 1, 'NO': 0}  # Remove the byte string prefix 'b'\n",
        "df['isBug'] = df['isBug'].map(d).fillna(df['isBug'])\n",
        "print(df['isBug'])\n",
        "print(\"before drop duplicates\", df.shape[0])\n",
        "df = df.drop_duplicates()\n",
        "df.reset_index(inplace=True, drop=True)\n",
        "print(\"after drop duplicates\", df.shape[0])\n",
        "\n",
        "print(\"#Rows: \", len(df))\n",
        "print(\"#Columns: \", len(df.columns))\n",
        "print(\"Class-Distribution: \", int((np.unique(df.iloc[:,-1], return_counts=True)[1][0]/len(df))*100), 100-int((np.unique(df.iloc[:,-1], return_counts=True)[1][0]/len(df))*100))\n",
        "\n",
        "avg_time_eclipse_PDE = dict()\n",
        "\n",
        "# intelligent pruning\n",
        "per_cluster_pruning_ratios = [0.5]\n",
        "intelligent_pruning_time = 0\n",
        "for rand_state in states:\n",
        "  X_train, X_test, y_train, y_test = split_data(X, y, rand_state)\n",
        "  for per_cluster_pruning_ratio in per_cluster_pruning_ratios:\n",
        "    for ratio in ratios:\n",
        "      X_train_copy, y_train_copy = X_train.copy(), y_train.copy()\n",
        "\n",
        "      start_time = time.time()  # Start timing\n",
        "      intelligent_pruned_X_train, intelligent_pruned_y_train = do_intelligent_pruning(X_train_copy.to_numpy(), y_train_copy.to_numpy(), ratio, per_cluster_pruning_ratio=per_cluster_pruning_ratio)\n",
        "      end_time = time.time()  # End timing\n",
        "      execution_time = end_time - start_time\n",
        "      intelligent_pruning_time += execution_time\n",
        "intelligent_pruning_time /= len(states)\n",
        "\n",
        "# random pruning\n",
        "random_pruning_time = 0\n",
        "for rand_state in states:\n",
        "  X_train, X_test, y_train, y_test = split_data(X, y, rand_state)\n",
        "  X_train, X_test, y_train, y_test = split_data(X, y, rand_state)\n",
        "  for ratio in ratios:\n",
        "    X_train_copy, y_train_copy = X_train.copy(), y_train.copy()\n",
        "    start_time = time.time()  # Start timing\n",
        "    random_pruned_X_train, random_pruned_y_train = random_prune_data(X_train_copy.to_numpy(), y_train_copy.to_numpy(), ratio, seed=rand_state)\n",
        "    end_time = time.time()  # End timing\n",
        "    execution_time = end_time - start_time\n",
        "    random_pruning_time += execution_time\n",
        "random_pruning_time /= len(states)\n",
        "\n",
        "# gaussian copula\n",
        "gc_time = 0\n",
        "for rand_state in states:\n",
        "  X_train, X_test, y_train, y_test = split_data(X, y, rand_state)\n",
        "  for ratio in ratios:\n",
        "    X_train_copy, y_train_copy = X_train.copy(), y_train.copy()\n",
        "    start_time = time.time()  # Start timing\n",
        "    sd1, train_df = do_sdv(X_train_copy, y_train_copy)\n",
        "    end_time = time.time()  # End timing\n",
        "    execution_time = end_time - start_time\n",
        "    gc_time += execution_time\n",
        "gc_time /= len(states)\n",
        "\n",
        "# smote\n",
        "smote_time = 0\n",
        "for rand_state in states:\n",
        "  X_train, X_test, y_train, y_test = split_data(X, y, rand_state)\n",
        "  for ratio in ratios:\n",
        "    X_train_copy, y_train_copy = X_train.copy(), y_train.copy()\n",
        "    start_time = time.time()  # Start timing\n",
        "    X_train_smote, y_train_smote = smote_oversampling(X_train_copy.to_numpy(), y_train_copy.to_numpy(), [ratio])\n",
        "    end_time = time.time()  # End timing\n",
        "    execution_time = end_time - start_time\n",
        "    smote_time += execution_time\n",
        "smote_time /= len(states)\n",
        "\n",
        "# random_oversampling\n",
        "random_over_time = 0\n",
        "for rand_state in states:\n",
        "  X_train, X_test, y_train, y_test = split_data(X, y, rand_state)\n",
        "  for ratio in ratios:\n",
        "    X_train_copy, y_train_copy = X_train.copy(), y_train.copy()\n",
        "    start_time = time.time()  # Start timing\n",
        "    X_train_random, y_train_random = random_oversampling(X_train_copy.to_numpy(), y_train_copy.to_numpy(), [ratio])\n",
        "    end_time = time.time()  # End timing\n",
        "    execution_time = end_time - start_time\n",
        "    random_over_time += execution_time\n",
        "random_over_time /= len(states)\n",
        "\n",
        "# svm smote\n",
        "svm_smote_time = 0\n",
        "for rand_state in states:\n",
        "  X_train, X_test, y_train, y_test = split_data(X, y, rand_state)\n",
        "  for ratio in ratios:\n",
        "    X_train_copy, y_train_copy = X_train.copy(), y_train.copy()\n",
        "    start_time = time.time()  # Start timing\n",
        "    X_train_random, y_train_random = svm_smote_oversampling(X_train_copy.to_numpy(), y_train_copy.to_numpy(), [ratio])\n",
        "    end_time = time.time()  # End timing\n",
        "    execution_time = end_time - start_time\n",
        "    svm_smote_time += execution_time\n",
        "svm_smote_time /= len(states)\n",
        "\n",
        "# RRP\n",
        "rrp_time = 0\n",
        "for rand_state in states:\n",
        "  X_train, X_test, y_train, y_test = split_data(X, y, rand_state)\n",
        "  for ratio in ratios:\n",
        "    X_train_copy, y_train_copy = X_train.copy(), y_train.copy()\n",
        "    start_time = time.time()  # Start timing\n",
        "    rt, new_data_df, train_df = RandomProjectionOversampling(X_train=X_train_copy,\n",
        "                                                                        y_train=y_train_copy,\n",
        "                                                                        threshold=10)\n",
        "    end_time = time.time()  # End timing\n",
        "    execution_time = end_time - start_time\n",
        "    rrp_time += execution_time\n",
        "rrp_time /= len(states)\n",
        "\n",
        "avg_time_eclipse_PDE['intelligent_pruning'] = intelligent_pruning_time\n",
        "avg_time_eclipse_PDE['random_pruning'] = random_pruning_time\n",
        "avg_time_eclipse_PDE['sdv-g'] = gc_time\n",
        "avg_time_eclipse_PDE['smote'] = smote_time\n",
        "avg_time_eclipse_PDE['random_oversampling'] = random_over_time\n",
        "avg_time_eclipse_PDE['svm_smote'] = svm_smote_time\n",
        "avg_time_eclipse_PDE['rrp'] = rrp_time\n",
        "\n",
        "avg_time_eclipse_PDE"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LE43z00V9SIJ",
        "outputId": "bca75480-8ea0-4acc-8801-e7181564801e"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0       1\n",
            "1       0\n",
            "2       0\n",
            "3       1\n",
            "4       0\n",
            "       ..\n",
            "1492    0\n",
            "1493    0\n",
            "1494    0\n",
            "1495    0\n",
            "1496    0\n",
            "Name: isBug, Length: 1497, dtype: int64\n",
            "before drop duplicates 1497\n",
            "after drop duplicates 1497\n",
            "#Rows:  1497\n",
            "#Columns:  82\n",
            "Class-Distribution:  86 14\n",
            "323 60 94 34\n",
            "290 46 88 42\n",
            "292 66 92 26\n",
            "289 42 86 44\n",
            "288 50 88 38\n",
            "262 48 90 42\n",
            "226 28 90 62\n",
            "287 36 82 46\n",
            "267 58 96 38\n",
            "277 58 94 36\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'intelligent_pruning': 0.014562559127807618,\n",
              " 'random_pruning': 0.0036672353744506836,\n",
              " 'sdv-g': 0.8721404314041138,\n",
              " 'smote': 0.00211482048034668,\n",
              " 'random_oversampling': 0.0009580612182617188,\n",
              " 'svm_smote': 0.008735227584838866,\n",
              " 'rrp': 0.0617199182510376}"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "project = \"Defect_Mylyn\"\n",
        "fname = \"_\".join(project.split(\"_\")[1:])\n",
        "data_path = f\"data/imbalance_defects_prediction/7_CK_NET_PROC/input/{fname}--CK_NET_PROC.arff\"\n",
        "data = arff.loadarff(data_path)\n",
        "df = pd.DataFrame(data[0])\n",
        "df['isBug'] = df['isBug'].astype('str')\n",
        "d = {'b\\'YES\\'': 1, 'b\\'NO\\'': 0}\n",
        "df['isBug'] = df['isBug'].astype(str).map(d).fillna(df['isBug'])\n",
        "df['isBug'] = df['isBug'].map({'YES': 1.0, 'NO': 0.0})\n",
        "print(\"before drop duplicates\", df.shape[0])\n",
        "df = df.drop_duplicates()\n",
        "df.reset_index(inplace=True, drop=True)\n",
        "print(\"after drop duplicates\", df.shape[0])\n",
        "print(\"#Rows: \", len(df))\n",
        "print(\"#Columns: \", len(df.columns))\n",
        "print(\"Class-Distribution: \", int((np.unique(df.iloc[:,-1], return_counts=True)[1][0]/len(df))*100), 100-int((np.unique(df.iloc[:,-1], return_counts=True)[1][0]/len(df))*100))\n",
        "\n",
        "avg_time_mylyn = dict()\n",
        "\n",
        "# intelligent pruning\n",
        "per_cluster_pruning_ratios = [0.5]\n",
        "intelligent_pruning_time = 0\n",
        "for rand_state in states:\n",
        "  X_train, X_test, y_train, y_test = split_data(X, y, rand_state)\n",
        "  for per_cluster_pruning_ratio in per_cluster_pruning_ratios:\n",
        "    for ratio in ratios:\n",
        "      X_train_copy, y_train_copy = X_train.copy(), y_train.copy()\n",
        "\n",
        "      start_time = time.time()  # Start timing\n",
        "      intelligent_pruned_X_train, intelligent_pruned_y_train = do_intelligent_pruning(X_train_copy.to_numpy(), y_train_copy.to_numpy(), ratio, per_cluster_pruning_ratio=per_cluster_pruning_ratio)\n",
        "      end_time = time.time()  # End timing\n",
        "      execution_time = end_time - start_time\n",
        "      intelligent_pruning_time += execution_time\n",
        "intelligent_pruning_time /= len(states)\n",
        "\n",
        "# random pruning\n",
        "random_pruning_time = 0\n",
        "for rand_state in states:\n",
        "  X_train, X_test, y_train, y_test = split_data(X, y, rand_state)\n",
        "  X_train, X_test, y_train, y_test = split_data(X, y, rand_state)\n",
        "  for ratio in ratios:\n",
        "    X_train_copy, y_train_copy = X_train.copy(), y_train.copy()\n",
        "    start_time = time.time()  # Start timing\n",
        "    random_pruned_X_train, random_pruned_y_train = random_prune_data(X_train_copy.to_numpy(), y_train_copy.to_numpy(), ratio, seed=rand_state)\n",
        "    end_time = time.time()  # End timing\n",
        "    execution_time = end_time - start_time\n",
        "    random_pruning_time += execution_time\n",
        "random_pruning_time /= len(states)\n",
        "\n",
        "# gaussian copula\n",
        "gc_time = 0\n",
        "for rand_state in states:\n",
        "  X_train, X_test, y_train, y_test = split_data(X, y, rand_state)\n",
        "  for ratio in ratios:\n",
        "    X_train_copy, y_train_copy = X_train.copy(), y_train.copy()\n",
        "    start_time = time.time()  # Start timing\n",
        "    sd1, train_df = do_sdv(X_train_copy, y_train_copy)\n",
        "    end_time = time.time()  # End timing\n",
        "    execution_time = end_time - start_time\n",
        "    gc_time += execution_time\n",
        "gc_time /= len(states)\n",
        "\n",
        "# smote\n",
        "smote_time = 0\n",
        "for rand_state in states:\n",
        "  X_train, X_test, y_train, y_test = split_data(X, y, rand_state)\n",
        "  for ratio in ratios:\n",
        "    X_train_copy, y_train_copy = X_train.copy(), y_train.copy()\n",
        "    start_time = time.time()  # Start timing\n",
        "    X_train_smote, y_train_smote = smote_oversampling(X_train_copy.to_numpy(), y_train_copy.to_numpy(), [ratio])\n",
        "    end_time = time.time()  # End timing\n",
        "    execution_time = end_time - start_time\n",
        "    smote_time += execution_time\n",
        "smote_time /= len(states)\n",
        "\n",
        "# random_oversampling\n",
        "random_over_time = 0\n",
        "for rand_state in states:\n",
        "  X_train, X_test, y_train, y_test = split_data(X, y, rand_state)\n",
        "  for ratio in ratios:\n",
        "    X_train_copy, y_train_copy = X_train.copy(), y_train.copy()\n",
        "    start_time = time.time()  # Start timing\n",
        "    X_train_random, y_train_random = random_oversampling(X_train_copy.to_numpy(), y_train_copy.to_numpy(), [ratio])\n",
        "    end_time = time.time()  # End timing\n",
        "    execution_time = end_time - start_time\n",
        "    random_over_time += execution_time\n",
        "random_over_time /= len(states)\n",
        "\n",
        "# svm smote\n",
        "svm_smote_time = 0\n",
        "for rand_state in states:\n",
        "  X_train, X_test, y_train, y_test = split_data(X, y, rand_state)\n",
        "  for ratio in ratios:\n",
        "    X_train_copy, y_train_copy = X_train.copy(), y_train.copy()\n",
        "    start_time = time.time()  # Start timing\n",
        "    X_train_random, y_train_random = svm_smote_oversampling(X_train_copy.to_numpy(), y_train_copy.to_numpy(), [ratio])\n",
        "    end_time = time.time()  # End timing\n",
        "    execution_time = end_time - start_time\n",
        "    svm_smote_time += execution_time\n",
        "svm_smote_time /= len(states)\n",
        "\n",
        "# RRP\n",
        "rrp_time = 0\n",
        "for rand_state in states:\n",
        "  X_train, X_test, y_train, y_test = split_data(X, y, rand_state)\n",
        "  for ratio in ratios:\n",
        "    X_train_copy, y_train_copy = X_train.copy(), y_train.copy()\n",
        "    start_time = time.time()  # Start timing\n",
        "    rt, new_data_df, train_df = RandomProjectionOversampling(X_train=X_train_copy,\n",
        "                                                                        y_train=y_train_copy,\n",
        "                                                                        threshold=10)\n",
        "    end_time = time.time()  # End timing\n",
        "    execution_time = end_time - start_time\n",
        "    rrp_time += execution_time\n",
        "rrp_time /= len(states)\n",
        "\n",
        "avg_time_mylyn['intelligent_pruning'] = intelligent_pruning_time\n",
        "avg_time_mylyn['random_pruning'] = random_pruning_time\n",
        "avg_time_mylyn['sdv-g'] = gc_time\n",
        "avg_time_mylyn['smote'] = smote_time\n",
        "avg_time_mylyn['random_oversampling'] = random_over_time\n",
        "avg_time_mylyn['svm_smote'] = svm_smote_time\n",
        "avg_time_mylyn['rrp'] = rrp_time\n",
        "\n",
        "avg_time_mylyn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "85AcEL3o9cIW",
        "outputId": "6c12d442-b1f0-4e5c-c73a-cc45167e6791"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "before drop duplicates 1862\n",
            "after drop duplicates 1861\n",
            "#Rows:  1861\n",
            "#Columns:  82\n",
            "Class-Distribution:  86 14\n",
            "323 60 94 34\n",
            "290 46 88 42\n",
            "292 66 92 26\n",
            "289 42 86 44\n",
            "288 50 88 38\n",
            "262 48 90 42\n",
            "226 28 90 62\n",
            "287 36 82 46\n",
            "267 58 96 38\n",
            "277 58 94 36\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'intelligent_pruning': 0.015069150924682617,\n",
              " 'random_pruning': 0.00323946475982666,\n",
              " 'sdv-g': 0.8848967552185059,\n",
              " 'smote': 0.0020857334136962892,\n",
              " 'random_oversampling': 0.0009673118591308593,\n",
              " 'svm_smote': 0.009211325645446777,\n",
              " 'rrp': 0.058608508110046385}"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = f\"data/Vulnerable_Files/moodle-2_0_0-metrics.arff\"\n",
        "data = arff.loadarff(data_path)\n",
        "df = pd.DataFrame(data[0])\n",
        "df['IsVulnerable'] = df['IsVulnerable'].astype('str')\n",
        "d = {'b\\'yes\\'': 1, 'b\\'no\\'': 0}\n",
        "df['IsVulnerable'] = df['IsVulnerable'].astype(str).map(d).fillna(df['IsVulnerable'])\n",
        "df['IsVulnerable'] = df['IsVulnerable'].map({'yes': 1.0, 'no': 0.0})\n",
        "# df.drop(['loc', 'nOutgoingExternFlsCalled', 'nOutgoingExternCallsUniq'], inplace=True, axis=1)\n",
        "print(\"before drop duplicates\", df.shape[0])\n",
        "df = df.drop_duplicates()\n",
        "df.reset_index(inplace=True, drop=True)\n",
        "print(\"after drop duplicates\", df.shape[0])\n",
        "print(\"#Rows: \", len(df))\n",
        "print(\"#Columns: \", len(df.columns))\n",
        "print(\"Class-Distribution: \", int((np.unique(df.iloc[:,-1], return_counts=True)[1][0]/len(df))*100), 100-int((np.unique(df.iloc[:,-1], return_counts=True)[1][0]/len(df))*100))\n",
        "\n",
        "avg_time_moodle_vuln = dict()\n",
        "\n",
        "# intelligent pruning\n",
        "per_cluster_pruning_ratios = [0.5]\n",
        "intelligent_pruning_time = 0\n",
        "for rand_state in states:\n",
        "  X_train, X_test, y_train, y_test = split_data(X, y, rand_state)\n",
        "  for per_cluster_pruning_ratio in per_cluster_pruning_ratios:\n",
        "    for ratio in ratios:\n",
        "      X_train_copy, y_train_copy = X_train.copy(), y_train.copy()\n",
        "\n",
        "      start_time = time.time()  # Start timing\n",
        "      intelligent_pruned_X_train, intelligent_pruned_y_train = do_intelligent_pruning(X_train_copy.to_numpy(), y_train_copy.to_numpy(), ratio, per_cluster_pruning_ratio=per_cluster_pruning_ratio)\n",
        "      end_time = time.time()  # End timing\n",
        "      execution_time = end_time - start_time\n",
        "      intelligent_pruning_time += execution_time\n",
        "intelligent_pruning_time /= len(states)\n",
        "\n",
        "# random pruning\n",
        "random_pruning_time = 0\n",
        "for rand_state in states:\n",
        "  X_train, X_test, y_train, y_test = split_data(X, y, rand_state)\n",
        "  X_train, X_test, y_train, y_test = split_data(X, y, rand_state)\n",
        "  for ratio in ratios:\n",
        "    X_train_copy, y_train_copy = X_train.copy(), y_train.copy()\n",
        "    start_time = time.time()  # Start timing\n",
        "    random_pruned_X_train, random_pruned_y_train = random_prune_data(X_train_copy.to_numpy(), y_train_copy.to_numpy(), ratio, seed=rand_state)\n",
        "    end_time = time.time()  # End timing\n",
        "    execution_time = end_time - start_time\n",
        "    random_pruning_time += execution_time\n",
        "random_pruning_time /= len(states)\n",
        "\n",
        "# gaussian copula\n",
        "gc_time = 0\n",
        "for rand_state in states:\n",
        "  X_train, X_test, y_train, y_test = split_data(X, y, rand_state)\n",
        "  for ratio in ratios:\n",
        "    X_train_copy, y_train_copy = X_train.copy(), y_train.copy()\n",
        "    start_time = time.time()  # Start timing\n",
        "    sd1, train_df = do_sdv(X_train_copy, y_train_copy)\n",
        "    end_time = time.time()  # End timing\n",
        "    execution_time = end_time - start_time\n",
        "    gc_time += execution_time\n",
        "gc_time /= len(states)\n",
        "\n",
        "# smote\n",
        "smote_time = 0\n",
        "for rand_state in states:\n",
        "  X_train, X_test, y_train, y_test = split_data(X, y, rand_state)\n",
        "  for ratio in ratios:\n",
        "    X_train_copy, y_train_copy = X_train.copy(), y_train.copy()\n",
        "    start_time = time.time()  # Start timing\n",
        "    X_train_smote, y_train_smote = smote_oversampling(X_train_copy.to_numpy(), y_train_copy.to_numpy(), [ratio])\n",
        "    end_time = time.time()  # End timing\n",
        "    execution_time = end_time - start_time\n",
        "    smote_time += execution_time\n",
        "smote_time /= len(states)\n",
        "\n",
        "# random_oversampling\n",
        "random_over_time = 0\n",
        "for rand_state in states:\n",
        "  X_train, X_test, y_train, y_test = split_data(X, y, rand_state)\n",
        "  for ratio in ratios:\n",
        "    X_train_copy, y_train_copy = X_train.copy(), y_train.copy()\n",
        "    start_time = time.time()  # Start timing\n",
        "    X_train_random, y_train_random = random_oversampling(X_train_copy.to_numpy(), y_train_copy.to_numpy(), [ratio])\n",
        "    end_time = time.time()  # End timing\n",
        "    execution_time = end_time - start_time\n",
        "    random_over_time += execution_time\n",
        "random_over_time /= len(states)\n",
        "\n",
        "# svm smote\n",
        "svm_smote_time = 0\n",
        "for rand_state in states:\n",
        "  X_train, X_test, y_train, y_test = split_data(X, y, rand_state)\n",
        "  for ratio in ratios:\n",
        "    X_train_copy, y_train_copy = X_train.copy(), y_train.copy()\n",
        "    start_time = time.time()  # Start timing\n",
        "    X_train_random, y_train_random = svm_smote_oversampling(X_train_copy.to_numpy(), y_train_copy.to_numpy(), [ratio])\n",
        "    end_time = time.time()  # End timing\n",
        "    execution_time = end_time - start_time\n",
        "    svm_smote_time += execution_time\n",
        "svm_smote_time /= len(states)\n",
        "\n",
        "# RRP\n",
        "rrp_time = 0\n",
        "for rand_state in states:\n",
        "  X_train, X_test, y_train, y_test = split_data(X, y, rand_state)\n",
        "  for ratio in ratios:\n",
        "    X_train_copy, y_train_copy = X_train.copy(), y_train.copy()\n",
        "    start_time = time.time()  # Start timing\n",
        "    rt, new_data_df, train_df = RandomProjectionOversampling(X_train=X_train_copy,\n",
        "                                                                        y_train=y_train_copy,\n",
        "                                                                        threshold=10)\n",
        "    end_time = time.time()  # End timing\n",
        "    execution_time = end_time - start_time\n",
        "    rrp_time += execution_time\n",
        "rrp_time /= len(states)\n",
        "\n",
        "avg_time_moodle_vuln['intelligent_pruning'] = intelligent_pruning_time\n",
        "avg_time_moodle_vuln['random_pruning'] = random_pruning_time\n",
        "avg_time_moodle_vuln['sdv-g'] = gc_time\n",
        "avg_time_moodle_vuln['smote'] = smote_time\n",
        "avg_time_moodle_vuln['random_oversampling'] = random_over_time\n",
        "avg_time_moodle_vuln['svm_smote'] = svm_smote_time\n",
        "avg_time_moodle_vuln['rrp'] = rrp_time\n",
        "\n",
        "avg_time_moodle_vuln"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rbpIQG_V9g0a",
        "outputId": "370dd5a4-da70-4674-d968-649d0ac26645"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "before drop duplicates 2942\n",
            "after drop duplicates 2056\n",
            "#Rows:  2056\n",
            "#Columns:  14\n",
            "Class-Distribution:  98 2\n",
            "323 60 94 34\n",
            "290 46 88 42\n",
            "292 66 92 26\n",
            "289 42 86 44\n",
            "288 50 88 38\n",
            "262 48 90 42\n",
            "226 28 90 62\n",
            "287 36 82 46\n",
            "267 58 96 38\n",
            "277 58 94 36\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'intelligent_pruning': 0.014297795295715333,\n",
              " 'random_pruning': 0.003216457366943359,\n",
              " 'sdv-g': 0.8517296791076661,\n",
              " 'smote': 0.0033036470413208008,\n",
              " 'random_oversampling': 0.0019164085388183594,\n",
              " 'svm_smote': 0.014578557014465332,\n",
              " 'rrp': 0.07613322734832764}"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_path = f\"data/Bug_Reports/ambari-train.csv\"\n",
        "test_data_path = f\"data/Bug_Reports/ambari-test.csv\"\n",
        "train_df = pd.read_csv(train_data_path)\n",
        "test_df = pd.read_csv(test_data_path)\n",
        "\n",
        "df = pd.concat([train_df, test_df])\n",
        "print(\"#Rows: \", len(df))\n",
        "print(\"#Columns: \", len(df.columns))\n",
        "print(\"Class-Distribution: \", int((np.unique(df.iloc[:,-1], return_counts=True)[1][0]/len(df))*100), 100-int((np.unique(df.iloc[:,-1], return_counts=True)[1][0]/len(df))*100))\n",
        "\n",
        "avg_time_ambari = dict()\n",
        "\n",
        "# intelligent pruning\n",
        "per_cluster_pruning_ratios = [0.5]\n",
        "intelligent_pruning_time = 0\n",
        "for rand_state in states:\n",
        "  X_train, X_test, y_train, y_test = split_data(X, y, rand_state)\n",
        "  for per_cluster_pruning_ratio in per_cluster_pruning_ratios:\n",
        "    for ratio in ratios:\n",
        "      X_train_copy, y_train_copy = X_train.copy(), y_train.copy()\n",
        "\n",
        "      start_time = time.time()  # Start timing\n",
        "      intelligent_pruned_X_train, intelligent_pruned_y_train = do_intelligent_pruning(X_train_copy.to_numpy(), y_train_copy.to_numpy(), ratio, per_cluster_pruning_ratio=per_cluster_pruning_ratio)\n",
        "      end_time = time.time()  # End timing\n",
        "      execution_time = end_time - start_time\n",
        "      intelligent_pruning_time += execution_time\n",
        "intelligent_pruning_time /= len(states)\n",
        "\n",
        "# random pruning\n",
        "random_pruning_time = 0\n",
        "for rand_state in states:\n",
        "  X_train, X_test, y_train, y_test = split_data(X, y, rand_state)\n",
        "  X_train, X_test, y_train, y_test = split_data(X, y, rand_state)\n",
        "  for ratio in ratios:\n",
        "    X_train_copy, y_train_copy = X_train.copy(), y_train.copy()\n",
        "    start_time = time.time()  # Start timing\n",
        "    random_pruned_X_train, random_pruned_y_train = random_prune_data(X_train_copy.to_numpy(), y_train_copy.to_numpy(), ratio, seed=rand_state)\n",
        "    end_time = time.time()  # End timing\n",
        "    execution_time = end_time - start_time\n",
        "    random_pruning_time += execution_time\n",
        "random_pruning_time /= len(states)\n",
        "\n",
        "# gaussian copula\n",
        "gc_time = 0\n",
        "for rand_state in states:\n",
        "  X_train, X_test, y_train, y_test = split_data(X, y, rand_state)\n",
        "  for ratio in ratios:\n",
        "    X_train_copy, y_train_copy = X_train.copy(), y_train.copy()\n",
        "    start_time = time.time()  # Start timing\n",
        "    sd1, train_df = do_sdv(X_train_copy, y_train_copy)\n",
        "    end_time = time.time()  # End timing\n",
        "    execution_time = end_time - start_time\n",
        "    gc_time += execution_time\n",
        "gc_time /= len(states)\n",
        "\n",
        "# smote\n",
        "smote_time = 0\n",
        "for rand_state in states:\n",
        "  X_train, X_test, y_train, y_test = split_data(X, y, rand_state)\n",
        "  for ratio in ratios:\n",
        "    X_train_copy, y_train_copy = X_train.copy(), y_train.copy()\n",
        "    start_time = time.time()  # Start timing\n",
        "    X_train_smote, y_train_smote = smote_oversampling(X_train_copy.to_numpy(), y_train_copy.to_numpy(), [ratio])\n",
        "    end_time = time.time()  # End timing\n",
        "    execution_time = end_time - start_time\n",
        "    smote_time += execution_time\n",
        "smote_time /= len(states)\n",
        "\n",
        "# random_oversampling\n",
        "random_over_time = 0\n",
        "for rand_state in states:\n",
        "  X_train, X_test, y_train, y_test = split_data(X, y, rand_state)\n",
        "  for ratio in ratios:\n",
        "    X_train_copy, y_train_copy = X_train.copy(), y_train.copy()\n",
        "    start_time = time.time()  # Start timing\n",
        "    X_train_random, y_train_random = random_oversampling(X_train_copy.to_numpy(), y_train_copy.to_numpy(), [ratio])\n",
        "    end_time = time.time()  # End timing\n",
        "    execution_time = end_time - start_time\n",
        "    random_over_time += execution_time\n",
        "random_over_time /= len(states)\n",
        "\n",
        "# svm smote\n",
        "svm_smote_time = 0\n",
        "for rand_state in states:\n",
        "  X_train, X_test, y_train, y_test = split_data(X, y, rand_state)\n",
        "  for ratio in ratios:\n",
        "    X_train_copy, y_train_copy = X_train.copy(), y_train.copy()\n",
        "    start_time = time.time()  # Start timing\n",
        "    X_train_random, y_train_random = svm_smote_oversampling(X_train_copy.to_numpy(), y_train_copy.to_numpy(), [ratio])\n",
        "    end_time = time.time()  # End timing\n",
        "    execution_time = end_time - start_time\n",
        "    svm_smote_time += execution_time\n",
        "svm_smote_time /= len(states)\n",
        "\n",
        "# RRP\n",
        "rrp_time = 0\n",
        "for rand_state in states:\n",
        "  X_train, X_test, y_train, y_test = split_data(X, y, rand_state)\n",
        "  for ratio in ratios:\n",
        "    X_train_copy, y_train_copy = X_train.copy(), y_train.copy()\n",
        "    start_time = time.time()  # Start timing\n",
        "    rt, new_data_df, train_df = RandomProjectionOversampling(X_train=X_train_copy,\n",
        "                                                                        y_train=y_train_copy,\n",
        "                                                                        threshold=10)\n",
        "    end_time = time.time()  # End timing\n",
        "    execution_time = end_time - start_time\n",
        "    rrp_time += execution_time\n",
        "rrp_time /= len(states)\n",
        "\n",
        "avg_time_ambari['intelligent_pruning'] = intelligent_pruning_time\n",
        "avg_time_ambari['random_pruning'] = random_pruning_time\n",
        "avg_time_ambari['sdv-g'] = gc_time\n",
        "avg_time_ambari['smote'] = smote_time\n",
        "avg_time_ambari['random_oversampling'] = random_over_time\n",
        "avg_time_ambari['svm_smote'] = svm_smote_time\n",
        "avg_time_ambari['rrp'] = rrp_time\n",
        "\n",
        "avg_time_ambari"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1RqA3zfo9lVc",
        "outputId": "cc5f4a28-3648-4968-9fdc-fbedf9f79d0d"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#Rows:  1000\n",
            "#Columns:  102\n",
            "Class-Distribution:  97 3\n",
            "323 60 94 34\n",
            "290 46 88 42\n",
            "292 66 92 26\n",
            "289 42 86 44\n",
            "288 50 88 38\n",
            "262 48 90 42\n",
            "226 28 90 62\n",
            "287 36 82 46\n",
            "267 58 96 38\n",
            "277 58 94 36\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'intelligent_pruning': 0.014783573150634766,\n",
              " 'random_pruning': 0.003258347511291504,\n",
              " 'sdv-g': 0.7723204851150512,\n",
              " 'smote': 0.002077460289001465,\n",
              " 'random_oversampling': 0.0009907245635986327,\n",
              " 'svm_smote': 0.008299660682678223,\n",
              " 'rrp': 0.05817878246307373}"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "data_path = f\"data/JavaScript_Vulnerability/JSVulnerabilityDataSet-1.0.csv\"\n",
        "df = pd.read_csv(data_path)\n",
        "drop_columns = [\"name\", \"longname\", \"path\", \"full_repo_path\", \"line\", \"column\", \"endline\", \"endcolumn\"]\n",
        "df = df.drop(drop_columns, axis=1)\n",
        "print(\"before drop duplicates\", df.shape[0])\n",
        "df = df.drop_duplicates()\n",
        "df.reset_index(inplace=True, drop=True)\n",
        "print(\"after drop duplicates\", df.shape[0])\n",
        "print(\"#Rows: \", len(df))\n",
        "print(\"#Columns: \", len(df.columns))\n",
        "print(\"Class-Distribution: \", int((np.unique(df.iloc[:,-1], return_counts=True)[1][0]/len(df))*100), 100-int((np.unique(df.iloc[:,-1], return_counts=True)[1][0]/len(df))*100))\n",
        "\n",
        "avg_time_js_vuln = dict()\n",
        "\n",
        "# intelligent pruning\n",
        "per_cluster_pruning_ratios = [0.5]\n",
        "intelligent_pruning_time = 0\n",
        "for rand_state in states:\n",
        "  X_train, X_test, y_train, y_test = split_data(X, y, rand_state)\n",
        "  for per_cluster_pruning_ratio in per_cluster_pruning_ratios:\n",
        "    for ratio in ratios:\n",
        "      X_train_copy, y_train_copy = X_train.copy(), y_train.copy()\n",
        "\n",
        "      start_time = time.time()  # Start timing\n",
        "      intelligent_pruned_X_train, intelligent_pruned_y_train = do_intelligent_pruning(X_train_copy.to_numpy(), y_train_copy.to_numpy(), ratio, per_cluster_pruning_ratio=per_cluster_pruning_ratio)\n",
        "      end_time = time.time()  # End timing\n",
        "      execution_time = end_time - start_time\n",
        "      intelligent_pruning_time += execution_time\n",
        "intelligent_pruning_time /= len(states)\n",
        "\n",
        "# random pruning\n",
        "random_pruning_time = 0\n",
        "for rand_state in states:\n",
        "  X_train, X_test, y_train, y_test = split_data(X, y, rand_state)\n",
        "  X_train, X_test, y_train, y_test = split_data(X, y, rand_state)\n",
        "  for ratio in ratios:\n",
        "    X_train_copy, y_train_copy = X_train.copy(), y_train.copy()\n",
        "    start_time = time.time()  # Start timing\n",
        "    random_pruned_X_train, random_pruned_y_train = random_prune_data(X_train_copy.to_numpy(), y_train_copy.to_numpy(), ratio, seed=rand_state)\n",
        "    end_time = time.time()  # End timing\n",
        "    execution_time = end_time - start_time\n",
        "    random_pruning_time += execution_time\n",
        "random_pruning_time /= len(states)\n",
        "\n",
        "# gaussian copula\n",
        "gc_time = 0\n",
        "for rand_state in states:\n",
        "  X_train, X_test, y_train, y_test = split_data(X, y, rand_state)\n",
        "  for ratio in ratios:\n",
        "    X_train_copy, y_train_copy = X_train.copy(), y_train.copy()\n",
        "    start_time = time.time()  # Start timing\n",
        "    sd1, train_df = do_sdv(X_train_copy, y_train_copy)\n",
        "    end_time = time.time()  # End timing\n",
        "    execution_time = end_time - start_time\n",
        "    gc_time += execution_time\n",
        "gc_time /= len(states)\n",
        "\n",
        "# smote\n",
        "smote_time = 0\n",
        "for rand_state in states:\n",
        "  X_train, X_test, y_train, y_test = split_data(X, y, rand_state)\n",
        "  for ratio in ratios:\n",
        "    X_train_copy, y_train_copy = X_train.copy(), y_train.copy()\n",
        "    start_time = time.time()  # Start timing\n",
        "    X_train_smote, y_train_smote = smote_oversampling(X_train_copy.to_numpy(), y_train_copy.to_numpy(), [ratio])\n",
        "    end_time = time.time()  # End timing\n",
        "    execution_time = end_time - start_time\n",
        "    smote_time += execution_time\n",
        "smote_time /= len(states)\n",
        "\n",
        "# random_oversampling\n",
        "random_over_time = 0\n",
        "for rand_state in states:\n",
        "  X_train, X_test, y_train, y_test = split_data(X, y, rand_state)\n",
        "  for ratio in ratios:\n",
        "    X_train_copy, y_train_copy = X_train.copy(), y_train.copy()\n",
        "    start_time = time.time()  # Start timing\n",
        "    X_train_random, y_train_random = random_oversampling(X_train_copy.to_numpy(), y_train_copy.to_numpy(), [ratio])\n",
        "    end_time = time.time()  # End timing\n",
        "    execution_time = end_time - start_time\n",
        "    random_over_time += execution_time\n",
        "random_over_time /= len(states)\n",
        "\n",
        "# svm smote\n",
        "svm_smote_time = 0\n",
        "for rand_state in states:\n",
        "  X_train, X_test, y_train, y_test = split_data(X, y, rand_state)\n",
        "  for ratio in ratios:\n",
        "    X_train_copy, y_train_copy = X_train.copy(), y_train.copy()\n",
        "    start_time = time.time()  # Start timing\n",
        "    X_train_random, y_train_random = svm_smote_oversampling(X_train_copy.to_numpy(), y_train_copy.to_numpy(), [ratio])\n",
        "    end_time = time.time()  # End timing\n",
        "    execution_time = end_time - start_time\n",
        "    svm_smote_time += execution_time\n",
        "svm_smote_time /= len(states)\n",
        "\n",
        "# RRP\n",
        "rrp_time = 0\n",
        "for rand_state in states:\n",
        "  X_train, X_test, y_train, y_test = split_data(X, y, rand_state)\n",
        "  for ratio in ratios:\n",
        "    X_train_copy, y_train_copy = X_train.copy(), y_train.copy()\n",
        "    start_time = time.time()  # Start timing\n",
        "    rt, new_data_df, train_df = RandomProjectionOversampling(X_train=X_train_copy,\n",
        "                                                                        y_train=y_train_copy,\n",
        "                                                                        threshold=10)\n",
        "    end_time = time.time()  # End timing\n",
        "    execution_time = end_time - start_time\n",
        "    rrp_time += execution_time\n",
        "rrp_time /= len(states)\n",
        "\n",
        "avg_time_js_vuln['intelligent_pruning'] = intelligent_pruning_time\n",
        "avg_time_js_vuln['random_pruning'] = random_pruning_time\n",
        "avg_time_js_vuln['sdv-g'] = gc_time\n",
        "avg_time_js_vuln['smote'] = smote_time\n",
        "avg_time_js_vuln['random_oversampling'] = random_over_time\n",
        "avg_time_js_vuln['svm_smote'] = svm_smote_time\n",
        "avg_time_js_vuln['rrp'] = rrp_time\n",
        "\n",
        "avg_time_js_vuln"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n61S0tx49wGo",
        "outputId": "c0f6c819-80d7-4163-f963-5a35f6a83fd7"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "before drop duplicates 12125\n",
            "after drop duplicates 6271\n",
            "#Rows:  6271\n",
            "#Columns:  36\n",
            "Class-Distribution:  85 15\n",
            "323 60 94 34\n",
            "290 46 88 42\n",
            "292 66 92 26\n",
            "289 42 86 44\n",
            "288 50 88 38\n",
            "262 48 90 42\n",
            "226 28 90 62\n",
            "287 36 82 46\n",
            "267 58 96 38\n",
            "277 58 94 36\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'intelligent_pruning': 0.014800024032592774,\n",
              " 'random_pruning': 0.003415560722351074,\n",
              " 'sdv-g': 0.8556629419326782,\n",
              " 'smote': 0.0023018598556518556,\n",
              " 'random_oversampling': 0.000997447967529297,\n",
              " 'svm_smote': 0.010094499588012696,\n",
              " 'rrp': 0.05139403343200684}"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    }
  ]
}